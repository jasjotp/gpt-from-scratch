[Music]
it is my pleasure to bring up and a man
that needs really no introduction so I'm
not going to spend any time introducing
him I would go by now where this
individual is and what he's done from an
innovation standpoint uh thought
leadership just really driving
technology artificial intelligence
throughout the world
Jensen I will use one name come up
because last name is not needed with
this gentleman so welcome
Jensen I would just say very proud to
call Jensen a friend a great partner and
someone that I look at as a mentor
looking at how he's leading the AI
Revolution and how he's leading his
organization so we'll just grab Tech
thank
you so Jensen so great great to have you
here uh we have a great group of
customers here partners and I will also
say we have some of our esteemed golfers
here that actually I'm not sure how many
events we've had in the past but I don't
remember a lot of the professional
golfers coming up to me and saying hey
can I get into the event and I was it's
like well certainly you can so uh you're
attracting I I've seen a lot of
customers before this doesn't look like
customers you guys are all too well
dressed to be
customers we would consider them custo
friends friends uh friends and partners
and just great to be here with you so
great to be here you know just so many
things going on right now it's such an
exciting space so first of all I do want
to thank Jensen and I've said it before
for taking the time to come here his
he's he's all over the place been in t
you know over in Asia Taiwan you were
you know India speaking and and you know
again the venues are normally could be
30 40 50 60,000 people that you're
speaking to so to come here and speak
with us in a really intimate setting
means a lot to me and I know it means a
lot to uh our customers partners and
friends here yeah thanks thanks for the
invitation I've been looking forward to
it um maybe you could just take it from
you know a little bit of the past the
present and the future and I know that's
a lot but so much has changed even in
you know you've been driving Nvidia for
30 30 plus years now but so much has
changed you know even in the last three
or four years so maybe you could just
frame up from your view how much has
changed and why in the last several
years to where we are today and where
you think things are going and and and I
know that's a lot uh and we can take
break breaks and I I can give a
perspective but really you know I know
they really want to hear me talk but
they don't so uh I I I would just like
to maybe you can share with them that
past present future where you think so
well it it is an extraordinary time uh
we are Reinventing the computer for the
first time since it was introduced
largely in
1964 the IBM system
360 uh the architecture of of that IBM
system it's the most famous computer the
world's ever ever built it created as a
result the most valuable company in the
world IBM IBM system 360 introduced the
idea of central processing unit
multitasking the separation of software
and Hardware with a layer called
operating system IO subsystems um
architecture compatibility family
compatibility all of the concepts that
govern uh the computer industry today
was really introduced in literally one
press release in 1964 it's
an unbelievable thing and and um uh that
basic that basic system uh ran out of
steam about about a decade ago and and
uh we we that that was the that was the
idea of our company was to invent a new
way of doing Computing uh as as the
general purpose Computing was going to
run out of steam and one of the one of
the benefits of what we invented um
results in when you when you create
amazing Computing technology it
effectively drives down the cost and so
we so if you will many of you might have
heard of the word Moors law U Moors law
uh is an incredible technology force and
it it created the modern technology
industry uh it doubled in performance
every year and a half but the easier way
to think about that is it increased in
performance by 10 times every five years
so every time if it's 10 times every
five years every 10 years 100 times so
Moore's law was improving the
performance of Technology of computing
by about a 100 times every decade uh you
invert that and the reason for that is
because deflationary technology is what
creates an
abundance it was it was the fact that we
took our industry drove the cost of
computing down by a 100 times every 10
years that today you have you have uh
these supercomputers in your pocket
called
iPhones and and um
uh what we did was over the course of
the last 10 years reduce the cost of
computing or increase the performance of
computing by one million
times so uh a million times is a lot
bigger than a 100 as you can imagine and
and the the way to think about that is
is um what would happen if you could
travel
somewhere uh a 100,000 times faster or
10,000 times faster
it would change everything about how you
think about the problem and so if you
were to go lift something and it's all
of a sun 10,000 times lighter than it
used to or you go do something and you
could do it with 10,000 times more scale
than you could how you see the world
completely changed and to the point
where uh we invented this new way of
doing software called machine
learning you could just literally take
uh all of the observed data in the world
you give it to a computer and this
computer is about the size of this room
would study all of the data in the world
and say figured it out this is how the
world
works that is Chad
GPT literally took all of the data in
the world on the internet and over the
course of several months uh looked for
patterns and relationships that
eventually understood uh words and uh
vocabulary and
uh syntax and grammar and and then you
gave it multiple languages at one time
and it figured out how to correlate um
uh uh well I don't I don't know French
but but but dog and Sh I think in French
dog and Shen right and so dog and Shen
are really the same word it figured out
how to cor first of all understand the
languages of both but it also correlated
the two to the point where you could now
translate and and so we created this
this architecture that is now a
universal translator and and now you you
figured out if it could do it different
language but what what if I could do it
for images and you correlate uh this uh
patch of pixels that appears to look
like a dog with the word
dog and when you can do that then when
you're shown an image of a dog you say
dog and when you say the word uh when
you type in the word dog it now
generates an image of a dog and that
that idea of course uh stable diffusion
M Journey uh Sora is now we can do it in
in in video and so then you you say okay
well if we created a system that that
can do that uh what if we taught it a
whole bunch of other types of digital
information like for example chemicals
and proteins and video and 3D and uh
fluid dynamics and and now because you
understand the meaning of all of the
things you can understand the meaning of
a protein and you could say I would like
you to generate a protein and it would
take a sequence of amino acids and would
turn it into the 3D structure of a
protein and when you can understand the
3D structure of a protein you understand
its function what it's for and that just
won a Nobel Prize that is um uh that's
deep mind's Alpha fold and and so I just
gave you some examples of what you can
do but but but the way the big idea here
is is that that we've now uh learned the
language of almost everything we learned
the language of images and videos and of
course language human language and
proteins and and we could also translate
to anything and so for for anybody who's
interested in starting a company uh what
I just described to you uh is a
framework to think about what can you do
now that you couldn't do before and so
uh for example you guys are doing golf
let's say let's say that that
uh you would like um an AI just to
record all everybody's rounds and uh
instead of instead of somebody going
through the videos and trying to figure
out which one of the segments were
interesting and told the story of the
round you just have an AI watch the
video and the AI will watch the video
and say this is interesting uh and uh
from that video it would generate the
captioning uh from the captioning it
could even voice over it generate the
generate words generate speech and so
you would go from uh I said earlier from
amino acid to protein why can't you go
from video to speech and video to speech
would basically be somebody who is a a
commentator uh talking about uh you know
a round of golf and it would only talk
about the important things uh it's
interesting because because if you could
create that
AI uh after you don't as you're watching
that video you might say I well uh you
know
uh what was what was the uh what did
what did this player how did Jim do in
the last hole so you would literally
talk to the AI and the AI would talk
back and say the last hole you know Jim
did this and that and so so from this
framework you could see the Cambrian
explosion of startups and there H that's
hundreds and thousands of startups are
showing up because of this translation
this framework but the the important
idea that I wanted to to end up with is
is this we reinvented the computer
whereas we used
to code software we now do machine
learning and so instead of humans
programmers writing software we now have
computers write the software instead of
developing software we're now using
machine learning to create
AIS and and that is the that is the big
transformation of our of our industry
that we're not making something software
anymore we're now producing
intelligence and the production of
intelligence has created a whole new
layer of industry and this layer of the
industry um what it's going to augment
is you know measured in trillions not in
billions and that's the that's the
remarkable thing about what what we've
built no it's it's a it's fascinating to
see you know all the things going on we
we talked about you know we're just
starting this phase of translating you
know words to images images to words
your different languages you think about
you know on the medical side from cells
to to to to literally cures you know
outcomes that you're going to deliver uh
maybe for the group that that we're
looking at we talked a bit about you
know we're we're we're in such the early
stages also that this has really come to
life with chat
gbt uh and people you know I would look
at it individually everybody's been able
to create your own account right to it
you can see the power of it now we're
we're moving into the power of business
the power of driving efficiencies
opportunities differentiation you know
so maybe you could touch on a little bit
of where do you see it today and where
do you see it going around changing the
way organizations operate and think
about individuals also and businesses we
talked about agents everybody will have
an A you know their own agent maybe
personally uh but also as a business and
then you know avatars and agents that
individuals may have so jents and maybe
kind of share a little bit of your view
of where this is going and how it may be
impacting businesses and individuals as
we move forward well let's make it first
of all take it take a step back and ask
ourselves when we use you know what is
the miracle of chat
GPT um and these large language models
the the miracle is that we all know how
to use it
that's actually the
miracle the miracle is that we can all
interact with it and and what are you
really doing well we used to program
computers now you program AIS and AIS
are easy to program you just tell them
what you want and and if you're not sure
uh you know it takes a swing at it
anyways um but it could also ask you I'm
not exactly sure what you mean and you
need to be more precise and you might
not even know exactly how to ask it to
do what you want it to do and you you
literally say that uh I would like to
ask you to do this I would like to ask
you to write me a Python program that
sorts you know a thousand
numbers and I don't know how to ask you
to do that teach me how to ask you to do
that it'll even come back and ask you
and teach you how to ask it to do it so
that it can do it are you guys and so
this is the miracle and so what I what
what did I in that interaction what just
happened I programmed I programmed an AI
to write a program that the computer can
run okay and so uh this is no different
than you asking a software programmer um
to do that on your behalf now that's
software but it could be almost anything
it could be uh review this document it
could be write me a contract it could be
um uh uh uh
create a uh a vacation plan that you
know it could be anything and so the
first thing that we we should observe is
that we close the gap of Technology
programming a computer is really really
in the hands of call it 10 million
people 10 million people out of eight
billion people in the world knows how to
program computers and yet 100% I just I
can do we can do the thought experiment
I'm certain 100% of you can use chat GPT
to fairly good effect and and so that's
that's the first thing to to observe is
that the technology barrier is now gone
now let's let me go to The Other Extreme
now and let's do another thought
experiment what would what would it what
would working be like
someday um and and uh it would likely be
my experience with working and so my
experience you know I I work in in um uh
one of the most advanced one of the most
capable computer science company the
world's ever know and I'm surrounded by
32,000 people and the way I interact
with them uh you know of course there's
a lot of face to face but most of my
interactions are through Outlook and
Outlook I see all these little little
dots and and some of them are literally
faces and some of them are cartoon faces
and so on so forth in the future many of
those little
dots will just be
AIS and that AI would be somebody would
be good at marketing campaign somebody
would be good at sales Camp campaign
somebody somebody's really good at
customer service somebody's good at
software programming chip design system
engineering some somebody is a really
good mechanical engineer and they're
working with a whole bunch of other
mechanical engineers and so on so forth
and I'm hoping that that I'm surrounded
by these super agents that are so good
at at what what they do uh that I can I
can give it almost any Mission any dream
and I I can come up with big ideas about
what we would like to achieve and what's
the amazing thing is I would assign that
mission today we were assigning to
somebody I would assign that mission to
to an agent to an AI agent that AI agent
would invite a whole bunch of other
friends other agents some of them are
biological some of them are
digital and uh they'll they'll come up
with a plan and they'll come back and
you know pitch the plan to me just like
they do today and uh we'll break it down
on improve it uh you know and and we
work together as a team and then goes
off and does the job and so so if that's
the
future if that's the future then we just
got to work our way back and say how do
we go create that future yeah and it's I
mean we talked a little bit in the back
obviously our our teams work together in
a in a big way uh so many exciting
things going on but kind of going back
we see now and correct me if I'm wrong
if you agree or disagree you know you
think about chat GPT you think about
some of the large language models that
all of us here can go out and you have
you have the ability to go exercise that
use those
ask it really interesting compelling
things to do in information and ideas
and it keeps getting smarter and smarter
you see how fast those models are
changing you know the versions are going
up and the capabilities but then within
the companies as Jensen has mentioned
you know we see this capability that
organizations have to build their own
call it large language models or rag
models which is really a customized uh
AI platform that you're going to build
on an Nvidia platform in your company
that's going to allow you to literally
write apis into these large language
models to deliver these agents at scale
within an organization and that's really
what we're seeing and I'm kind of
curious you know you know Jensen give
your perspective but we're in a really
early stages of that and it's really
starting to come to life and those are
things that we're seeing so kind of your
view maybe a little more color on how
you see that transitioning over the next
couple six months to a couple of years
because uh from my view and you know
from what we even talked about it's like
everybody's going to have an agent and
organizations are going to have
thousands and thousands of agents and
it's not necessarily going to displace
you know employees it's going to enable
and complement in a lot of ways so maybe
just a little more color on well I hope
it displaces a lot of my current
work and and what I mean by that is very
honestly there there's there's there no
agents that that we know no AIS that we
know that could replace 100% of
anybody's job but we know of many agents
that can replace 80% of the things that
we do well nothing would give me more
joy than to have something do 80% of
what I do today so I can go find 80%
more things to go do that's the
definition of productivity do more with
less and so and and of course of course
uh uh we would like that to happen for
all of our employees and and we have ai
all of our companies today to uh write
software help us design chips help us
design systems and and and they all tend
to be inward focused because we could
develop the technology inside and if it
doesn't work that well uh we still have
excellent Engineers to to override
override their their uh uh their
decisions and and their suggestions now
as the technology gets better we're
starting to get to a point where we
could use it for outward facing um work
and outward facing work would be uh
marketing campaigns sales campaigns
customer service uh so on so forth the
the way to think about a good mental
model for for thinking about AI is this
the same exact way you think about
employees the first thing you have to do
is you have to onboard them and and
there's a way of onboarding AIS you you
give it a whole bunch of data that's
specific to you to your company and your
the work that you're trying to do many
examples you you just you show your
employee
this is the way we talk in our company
these are core values uh this is this is
the functionality of your job these are
examples of uh work products that we
would like you to create these is this
is how you this this is a a source of
all the resources that you can get your
approved to go used and then within that
boundary you have to go do that work and
so so you train the AI to do that
there's a methodology for doing that you
evaluate the AI just like we evaluate
employees uh we guard rail and just like
we guard rail employees after we onboard
somebody in the accounting department we
don't say you know please don't don't
you well I don't know what you tell
accountants not to do don't talk to
customers
so oh that's good I'm going to use that
where's our finance folks for example
you know that's a joke for you're you're
an engineer you're an engineer don't
talk to customers uh you know and and
you're in sales don't don't launch
products
and and and so so we guardrail them and
they're ai ai Technologies for Guard
railing things uh and then there's
there's a flywheel for continuous
Improvement and so on so forth and so
these these uh everything that I just
described is a whole bunch of other AIS
and these other AIS are helping us on
board AIS fine tune AIS um uh you know
soone evaluate eyes guard rail AIS you
know improve AIS and so all all of this
is is kind of like a framework for
employees yeah and and my sense is that
that um what what is likely to happen is
that we'll just we'll have two two very
significant workforces inside companies
one that are that are uh you know one
that are biological they're made of meat
and then and then the other ones they're
M electrons you know and and Flesh and
Bones I mean you know and and and we'll
have both and then they'll work together
and they and they'll work with each
other yeah no and it is it's it's
fascinating to to to see that maybe uh
talk a little bit about your vision and
maybe the ecosystem to enable this
within organizations because you talk
about Nvidia you know we talked about in
the back where and I truly believe this
Jensen does not look at uh Nvidia even
know it's now you know you look at top
first second you know bouncing around
you know largest market cap company in
the in the world so just an amazing job
honestly to to I
mean
uh I I mean this and I know it and
Jensen knows I I I don't say this
stroking Jensen on this uh but he will
go down as one of the great innovators
of our lifetime and uh and really a
great leader I mean a great leader that
is maniacal in regards to his focus on
Innovation and drive and really a a good
person from a humility perspective too
very humble and driven and it's kind of
fascinating maybe you can give a little
your perspective on you how you look at
the market because you really don't I
feel like that's where we should
end you know how you look at the market
because you really don't fixate on like
the market cap and the value you you and
you talk about creating markets not like
here's a market and here's competitors
and here's how much I can yet it's I'm
looking at creating additional markets
for NVIDIA but also the ecosystem talked
about you know with Jay and the team and
Craig how you're trying to pull
organizations with you into this AI
journey to to drive positive change for
our humanity and for business uh so
maybe a little bit of how you look at
that of being a market maker and not
necessarily going at a size of a market
that's there today
yeah um Nvidia is a market maker not
share
taker um we we we only do things that
other people don't do um there's nothing
wrong with with market share thinking
there's nothing wrong with benchmarking
there's there all of those are good
skills to to do um I I tend to not not
think about market share and the reason
for that is because because in a lot of
ways
uh you you have to ask yourself for for
what reason are we doing this if there
was somebody else who was already doing
it and and um we've not been around a
very long time we're only we're only 32
33 years old and so there were a whole
lot of technology companies that existed
uh already by the time that that Nvidia
was founded and so the question is is
what can we do that's unique that that
um adds a perspective and and a
contribution that if we didn't exist
wouldn't exist and so that that way of
thinking that way of thinking and the
Computing model we invented uh was
really unlikely to have been done and
and um uh and so the the company was
always the company's good at thinking
you
know what if what if the world was like
this what if we could do that notice
just now I answered a question my my
natural inclination is to answer the
question from the past and then from the
future the present is the only place
that I'm
uncomfortable if you ask me what's going
to happen tomorrow I have no clue if you
ask me what's going to happen in 10
years I have a lot of
confidence and and if you ask me what
happened 10 years ago I I'll tell you I
don't remember but but but the the the
reason the reason why I have a lot of
confidence in that is because you reason
from first principles um uh and the
limits of of
physics and and if you look far if you
look far out enough you don't have to
worry about about the the challenges of
getting there you just have to you just
imagine being there and so I am almost I
am completely certain that human or
robotics is going to be um here and and
there's the first principal reasons for
that because we understand something
about the technology breakthroughs that
we've we've invented I'm now certain
that it will be created um I certain why
it's going to be helpful and so so now
you have these certain Concepts you know
and uh for example I'm I'm certain my
Outlook is going to be the way I
described I am certain that when I'm
writing emails sometimes I'm writing an
email to uh biological employee
sometimes I'm writing an email to an
artificial int employee I'm certain of
that I'm certain that that my email is
going to have a whole bunch of toos and
froms and and many of them are going to
be intermixed and none of them are going
to
know and they're all we're all going to
be exchanging information and however we
exchange information today
and so once you live in that future then
the question is how do you get there you
know do you like that future is that is
that going to be helpful to the industry
is that going to be helpful to the world
is that going to be helpful to the
society and and then you work your way
back and uh I find that always to be
more helpful and that
future usually doesn't include um that
person is doing it so let's take it from
them it usually starts with it hasn't
been done before and so I I always find
that that thought model model is is
helpful to me and and then and then
since it doesn't exist then it's very
likely that the ecosystem of Partners
don't exist then you have to go find
friends to help you do it you know and I
don't have to go do everything myself I
just want to make sure it's done and so
and and I I actually you know like to do
as little as possible
frankly and and so so when you then that
future needs friends and and of course
in many of the areas where we have
Enterprise AI industrial AI uh wwt is
just an incredible friend and and that's
why our partnership is so great you know
I would say just in the spirit of uh you
know this it's
a it's special to have Jensen here and I
don't want to to hog the the the kind of
the mic up here so maybe will it open up
to to some of the audience and I know
Jensen actually enjoys the free flowing
uh question so we've got a shy guy over
there Mr DeWalt first time with a mic in
his hand uh uh I have to I have to tell
you I'm much more comfortable with the
whole bunch of engineers in the room
Well he kind of covers both so all right
hey fir first of all I just say how
inspiring it is to see the two of you up
here what is it 30 plus years both of
you grinding away and building
incredible businesses so congratulations
it's a great moment um I don't know if
you know this Jensen but I've been in
cyber security for 25 years and uh
fighting a b bad guys for a lot of time
at McAfee Mandy and firey talk about
putting fingers in the Dyke right I mean
the bad guys have been winning a lot
until I've seen kind of through your
eyes to Nvidia and see the future of
what AI can really do it's pretty cool
to like start to see the power of
autonomy and what we can do to reverse
kind of what the offense can do versus
what the defense my question is what do
you see I mean how do we solve some of
these threats and risks that we're
seeing every day in the world of cyber
and uh what's invidious part in that so
yeah I appreciate the question first of
all um artificial intelligence is going
to change the the the profile of cyber
security completely and and the reason
for that is this uh as you know the the
issue with cyber security isn't well
there are a lot of issues but one of the
issues with cyber security is the being
overwhelmed by false positives if you're
if you're too stringent on your
detection policy you're just flooded
with
with detections that aren't really uh uh
cyber threats um and the only way to
solve that problem is with humans to go
reason about it to reason about every
single one of the detections and ask
yourself through other sources of
information whether this is actually a
threat to be concerned about or not well
guess what that process you can now
automate with AI and as a result we
could tighten up our cyber security
posture and not be overwhelmed by false
positives because we'll just have a
whole bunch of you know ai ai cyber
security agents go and reason about
every single one of those and just go
you know yeah not an issue uh the the
other thing that is really cool about
cyber security is that that the the
framework that the industry put together
for cyber security really likely is
going to be the framework we use for AI
security um as you know AI is not one
giant model AI is a system of models and
when we finally deploy AI we're going to
have for every AI that we deploy we're
going to have probably hundreds of AIS
watching over it no different than air
traffic control watches over the planes
no different than inside a plane there's
there's three autopilot systems with two
pilots all the planes in the air are
watching over each other Air Traffic
Control watching all of the planes not
to mention the layers and layers of
policies and regulations and
methodologies and the best practices so
on so forth isn't that right and that's
just for not well that not not just for
autopilot and that's just autopilot okay
now imagine what we're going to do when
we deploy autopilot of this type of this
type of agent or you know lawyers and
doctors and accountants and Mark we're
going to have all kinds of other agents
inside the company that watches over all
of the other agents and so that's going
to be one of the one of the agents
inside the company is sort of you know
uh employee com you know AI employee
compliance agents you know and so so I I
think the framework that has been
created for cyber security over the
years is really a good one uh notice in
cyber security whenever one company has
a threat is shared with all of the other
companies we're going to do the same
thing with AI and so so I think I think
a lot of the work that on the one hand
uh your industry is going to help the
creation of the AI industry on the one
hand the creation of AI Industries is
going to come back and help your
industry
it's kind of kind of interesting also I
would say that the that the topic you
brought up uh uh Dave actually sits on
one of the airlin boards so to use the
pilot and the planes as one as uh very
very near and dear to Dave h Hello
Jensen hello Jim um my name's Aiden I
have ai in my name which was handy for
today but um question I have is that's a
blessing or a burden a burden uh
because the don't the only intelligence
I have is artificial but the the
question I have Jensen is when you think
about the future of of sovereign Nations
and you think about strategy and how
that's going to change how countries are
LED and how countries interact with each
other and also access to to for for poor
people to be upgraded in a world of
abundance how do you see that playing
out um and what's a time frame you think
where that can start to really impact um
geopolitics okay yeah I appreciate the
question excellent U first of all we're
going to take the marginal cost of
intelligence down to approximately
zero even a poor person can afford it
there you go that's the first
observation and and um remember remember
there was a time when when um well still
it still is true uh access to to clean
water is is is uh a great challenge for
many many countries and many people a
bottled water really re evolutionize the
access for for clean water uh you know
artificial intelligence is going to
modernize if you will democratize the
access of of intelligence uh it could be
uh Radiologists in a in a small village
tier four tier 7 Village in Africa or
India or you know you you you pick your
favorite place and so so I think I think
that working backwards I think that's
one of the great opportunities uh with
respect to Sovereign AI the observation
is this your your country's
data belongs to its
people sovereignty is no longer just the
land and the air above it sovereignty is
also your people your language your
culture your history that's all part of
your sovereignty and it's now encoded
digitally into ones and zeros but nobody
ever knew what to do with it until now
so now you could turn that Sovereign
data which belongs to you into your
Sovereign Ai and every country no
country wants to say hey take all the
take all the data that belongs to us and
import back to us an AI everybody wants
to process the data themselves refine it
into Ai and so I think this is going to
be a a new form of national
infrastructure which is you know we
build this thing called a this this AI
supercomputer and and we call it AI
infrastructure Computing infrastructure
it's going to be like energy it's going
to be like Communications it'll be like
you know part of the national
infrastructure uh in countries and we're
seeing this Awakening to Sovereign AI
all over the world uh I was just in
Denmark I was in Sweden uh from here I'm
going to go I'm going to go to
um it's surprising but Japan Indonesia
and Thailand uh they're all going to
build their own Sovereign AIS and um I
that that's likely what's going to
happen Jensen thank thanks for being
here uh just a question for you about
the future Workforce uh what advice
would you give to High School junior
seniors as they're trying to create
their
path uh F first of all I would I would
encourage them to do well in school in
all of the areas that that that school
is going to everything that we learned
is still
good it's all good and and and it's it's
not so much that everything we learned
was actually correct as you know
our history books have been uh the
reason why they give you so many re
addition you know you got to ask
yourself you get a tech history textbook
as addition
27 you know I get what happened to the
last
26 and and and who did it contaminate
and so so the fact of the matter is um
none none of that is is a problem just
it's about the learning process but the
one thing that I will say is that
everybody should get an AI tutor I have
three I use chat GPT I use perplexity
and I use Gemini and I use all three of
them uh on a regular basis I I use them
flying down here and so if I have
questions if I have a thought in my head
uh if I'm if I'm trying to explore some
new new area and trying to learn
something new the first thing I do is I
go to one of those AIS and I ask it some
a series of questions about the topic
I'm about to learn and and I I'll I'll
I'll start by saying you know Hey so uh
explain digital biology and computer AED
drug Discovery to me uh in uh as a fifth
grader as a starting point and then I
then I'll go okay well explain to me as
a college student and then and then
after that I've learned it a few times
then I can ask then then the next thing
I ask is what question should I be
asking um uh give me a give me a
framework for how I should be learning
about this and and so I'm asking a whole
bunch of questions about how to learn
and then and then beyond that I I can
learn it and it it remembers where I
left off left off yesterday and and so
so I think every every every student
should get a get a tutor and that tutor
AI tutor will teach them how to prompt
how to engage AIS how to how to ask an
AI to give to give you the information
you need and and so on so forth uh I I
think that this this J journey of
getting tutors into every student this
could very well be the most
revolutionary thing that that came out
of AI okay but everything that we're
learning Jon you know I I would look at
it i' even look at where we are today
and I'd be curious your thoughts on this
and my my advice
that even employees students kids uh I I
think a lot of success is going to be
around individuals that are
curious uh that that are very thoughtful
and know how to ask good questions
inquisitive questions because you think
about how even today and it'll continue
to evolve using AI models you know
perplexity open AI uh it's about
learning with that and being inquisit
enough you know and having that desire
to continuous learning and continuous
asking and probing uh those are
individuals I think within our company I
want them to be pushing the models and
have a curious mind if you don't have a
curious mind you're not going to learn
and I think that's one thing for for
students and for employees is that we're
in a world that if you want to be static
you're going to be in trouble if if
you're going to continue if you have a
desire to to learn and push and and
innovate and challenge yourself and be
curious I think you're going to succeed
so I yeah I completely agree I
completely agree hi Jensen uh scart with
gxo Logistics um we've been talking a
lot about a lot of the great things AI
will bring to the Future to us the other
things that I worry about is on the flip
side of that where you start to think
about the Bad actors and we think even
about cyber security well the Bad actors
will they not be also using AI
technology to be those attack weapons
and be even smarter than they are today
number one and then number two what
about the energy uh aspect of this too
when I try to go into certain data
centers today even light up new servers
and things like that sometimes I have
problems getting to places that have
energy so all of those two elements I'd
love to hear what your thoughts are uh
there there's only one solution to the
bad actor problem uh and and that is
that is we have to go way
faster that there's you know uh for for
Dave in the cyber security industry uh
worrying about it is the wrong answer
you just got to make sure that you're
ahead on Cyber secur Technology and and
it's it's not just as just as it is it
is not like likely we lose we lose our
job to AI it's very likely we'll lose
our jobs to somebody who uses AI it's
not likely that AI does something to us
it's likely that somebody uses AI to do
something to us and so I think your
concern is is is um uh well placed and
the answer is you got to go fast make
sure our AI technology is way better
than theirs and we democratize it
meaning it's available to
everybody okay and so you could you
could pick on one you can pick on
somebody but you can't pick on everybody
and so so I think the the um uh uh your
second question was power energy yeah
yeah uh uh two two answers for that
first of all you want to think about AI
from a longitudinal perspective the goal
is not to train the AI the goal isn't to
go to school the goal is to apply what
you learned now um you could teach an
AI uh how to predict physics like for
example weather prediction we've taught
in AI how to predict weather almost
10,000 times more energy efficiently
than using a supercomputer to do it
every single country every single region
around the world is running
supercomputers right now as we speak to
try to figure out what tomorrow's
weather is and what next week's weather
is because there might be a storm and
somebody just wants to know whether to
wear sweater or not okay and so that
supercomputer is running 24/7 all the
time now we could replace that by
teaching it in AI the laws of physics
well not the laws of physics you teach
an AI the patterns of physics and then
now it could predict it and and people
ask me how could you teach an AI physics
and the answer is actually s
surprisingly simple you know I
I my dogs were just talking about my
dogs my dogs don't understand the laws
of physics they don't understand
Newtonian physics they don't understand
spring Theory but boy they could catch
balls like you can't believe and and the
reason for that is because they learn
they learn physics by observing it and
uh uh we do we we're basically doing the
same thing by giving it so much
information that it observed the
patterns of physics and multiphysics and
it does it even better than principled
solvers okay so so you have to think
about the longitudinal the goal the goal
is not to train the model the goal is to
go discover a new material that could be
better for uh energy capture uh better
for carbon capture and we we train a
model that predicts Wells that are
better at Carbon capture we want to go
create a model that that um does a
better job uh um designing a new
material for
car okay you know make cars more
aerodynamic or make wind farms more uh
more productive make uh Electro voltaic
cells more productive for for example
that's the goal does that make sense and
so you have to you have to put the AI to
school in order to get the benefits out
of the AI and when we deliver the
benefits out of the AI it takes it out
of my goodness right if data centers
consume call it 3% or 4% of the world's
energy today you got 96% to go solve for
that's the benefit of AI and then the
last thing that I'll say is
this I hope that the amount of energy
that is used for artificial
intelligence goes up as a percentage of
human consumption over time and the
reason for that it's currently 4% out of
the world the world's total use I'm
hoping that artificial intelligence is
you know solid 10
15% and the reason for that is remember
what that 96% is doing the 96% of the
world's power is our
Industries it's making things it's
moving things around it's all the things
that we need to to live well what we
would like to do is instead of producing
all of the those other things we like to
produce intelligence and the
manufacturing of intelligence we hope
give you the examples that I was
mentioning earlier the positive benefits
everywhere else and so my my prediction
is that we have invented a new
industry I mentioned
earlier that we now produce this thing
called intelligence the idea that we're
manufacturing intelligence is a concept
that's insanely hard to understand and
so let me give you an analogy so 300
years ago there was a machine that was
created today that machine is called
Nvidia but 300 300 years ago the machine
was created was called a
Dynamo and people would pour water into
this thing and you light it on fire and
what comes out of it is
electricity and so you apply energy to
this thing to this machine and what
comes out of it is is an invisible thing
and the way you pay for the electricity
is dollars per kilowatt hours
okay now we have this new machine this
Nvidia machine that that we build you
apply energy to it and what comes out of
it is tokens that depending on how you
reconstitute these tokens turns into
either words or proteins or videos or
you know so on so forth and you know or
articulation for uh driving driverless
cars or you know whatever it is okay and
so so so these tokens that come out as
intelligence are monetized dollars per
token dollars per million tokens dollars
per kilowatt hour can you see that and
so this energy creation industry is the
beginning of an industrial revolution we
have now we have we're at the beginning
the reason why Jim and are so excited
about this is we're literally at the
beginning of a new Industrial Revolution
and this new industrial revolution has a
new new machinery and this new Machinery
produces something that's never been
produced before just like 300 years
ago and it's forming these incredible
companies and it's forming all these
Incredible use cases just like 300 years
ago when the electricity was discovered
all kinds of new things came out for
example consumer electronics appliances
Appliance the appliance industry didn't
exist that's the reason why GE was
making electricity on the one hand app
Ian is on the other hand Appliance was
the consumption of the energy they were
creating and so in a lot of ways to
together between us and
wwt did you guys hear
that this this AI is easier to train and
so so between us and wwt we have the
agents and so on so forth which are
essentially if you will the consumer the
appliances that consumes the tokens that
are being produced by this Factory and
so you know the one the one I would add
Jensen is is fascinating about that
while I'm listening to you and I'm
thinking I'm like you know sounds like
nonsense right now but in 300 years I'm
just no well that's exact honestly
that's exactly where I was going with
this if you think about how long it took
for the energy to to to literally take
hold and scale and you know it it was in
one little pocket of the world and
literally took forever for that energy
to be prevalent ubiquitous around the
world so if you think about where AI is
literally almost overnight you've
created this this intelligence that was
opened up to the world uh overnight so
you think how how long some of these
Transformations and these these
Industrial Revolution and now the AI
Revolution it's happening the the
difference is you know it was a speed is
incredible yeah it's it's it's it's
almost Unthinkable to compare and
contrast the two and the magnitude of
change it's going to create in such a
per I mean I I just think that just
listening to what you were walking
through and I think about how fast the
speed of Change is Going to Be
unimaginable so the only advice is
engage it as soon as you can if
something is moving super fast you know
if you if you have a train that's moving
super fast don't watch it get on it you
know and so here's yeah here's a ET ship
and it's going super fast get on it go
ahead go ahead yeah yeah yeah sorry here
back here sorry um first of all amazing
thank you so much and I think on behalf
this entire audience and our 40 401ks
thank you a lot as well too give me your
stock price but in all seriousness
you've been talking about the
supercomputer you're talking about how
things move so fast and like today is
arguably the slowest day we'll ever see
in our lifetime given how fast things
are moving but when you think about the
the next generation of the supercomputer
computer if and how do you think about
Quantum Computing as we start to use
that and whether or not that's going to
replace GPU compute or complemented or
assisted yeah Quantum Computing is going
to be really good at something so for
example let me give you an example of a
is everybody everybody you guys know
what a Quantum so for example let me
give you an example of what quantum
computers really good at so suppose we
suppose we have
a like like a wedding party like like
today like tonight well I'm not sure how
you did it but suppose we had a wedding
party
and and the wedding party had you know
let's say 300 people and you have to
figure out uh what are the best ways to
seat 300
people and there are a lot of
complexities as you could imagine some
some of the some of the some of the some
of the people are more important than
the others uh and like for example uh
you know my college friends like they're
in the
back isn't that right and and then and
then you're you're uh your your your
wife's uh you know friends they're in
the front you know and then for for
example and so you got to go through
this combination so so turns out 300
people the number he is he is human and
he is very know very smart but uh he has
to deal with the well I'm trying to
explain a complicated complicated idea
very simply and so you have these 300
people the number of combinations is
more than the number of atoms in the
universe okay and so so you're a
computer trying to figure out all all
the different permutations and all the
different combinations trying to figure
out which one is the most optimal as you
know that comp that Computing problem it
will run
forever or you could ask your
mother-in-law and
and and the reason for that is very
simple because she's going to use
artificial intelligence to to to you
know these people absolutely must sit
together these people absolutely must
sit in the in the front uh you know the
they brought a really expensive gift so
on so forth and then and then you know
your college friends in the back you
know what I'm saying and so so you got
to sort through all that I just gave you
an example of where artificial
intelligence can find a perfectly good
answer that we thought quantum computers
were necessary in the
past and and the the the simple idea is
that quantum computer is really good at
small data large combination
problems Ai and classical Computing
doesn't really care and then number one
number two the two of them will have to
live together someday because quantum
computers are good at certain things and
quite poor at many other things and the
big idea is that AI has kind of punted
quantum computer down the road by about
20 30 years okay but we're we work on
quantum computers uh we work with just
about every quantum computer company in
the world uh because we want it to
happen but it's still going to be a
decade or plus you know far away from me
anyways that's the idea okay one last
question as a 30-year veteran in this IT
industry I absolutely love technology
and really enjoy watching it how makes
the world better but also as a father of
two um I've also watched in a lot of
ways how Technologies work to you know
degrade interpersonal skills Humanity
time with people and you said something
interesting earlier tonight when you
said um you love how AI is given you
time back to go do the things you want
to do do I just thought maybe you could
share a little light into what your
vision is of how AI might be able to
help our children in the future in terms
of the humanity aspect the inter
personal skills and and a more fun way
of life well I I think that that um I
appreciate the question I think I think
Jim and I kind of approached it uh
approached that that topic in in a
slightly different way but we both
approached it uh which is which is um
the the AI is an is an instrument of
knowledge you know whereas your car is
an instrument of moving atoms from point
A to point B the AI is an instrument
it's also an instrument it's it's a
toaster it's just happens to be a a
super smart
toaster and you could talk to the
toaster about anything you want and ask
it to help you do things and today of
course the the toaster the AI that you
know is it's information based but that
same AI is about to become embodied as
well you know your ex your version of
R2-D2 uh and C3 CPO is right around the
corner and and I can't I can't wait I
can't wait um and so I know I know that
my AI my AIS are are um having a
dialogue with me on a regular basis and
helping me learn and helping me do tasks
that that in a lot of ways um you know I
find quite arduous like for example I
have to write a lot of
speeches and and um the context of the
speeches are all always different um uh
but the core information and my tone is
often very is is the same and so you
know I I didn't do this today everything
I just want to let you know everything
here I didn't prepare for but but but in
in many prepared speeches I'll give it
the context I'll give it the context um
and and I'll I'll tell it these are my
themes and then I'll I'll tell my AI to
refer to all of my other writings and
all of my other speeches and I say write
me a six-minute speeech that addresses
these points but highlights the themes
behind the things I talk about and it
comes back within one second literally
one second later I have a six minute
speech and then from that I refine it
into something that well that that is
you know hopefully better than what it
provided and often times it generates
one and I'll say is that the best you
can
do literally I just go Chad gbt is that
the best you can do and it goes I can do
better and I love that I love that and
and and then it gives me another one and
just to just to mess with it you know
since it's the same price anyways I I I
don't think so I think you could do even
better than that and comes back with yet
another one and each one of them
actually improves on the last one and
then I take that one and I modified it
saved me a ton of time writing speeches
is is painful and it's arduous and um
and but it's important and so you know
those are kind of examples of how I use
it um but but I think the most important
thing is I have a Learning Partner um I
have an age I have a little AI That's My
R2-D2 that's remembering the things that
I'm learning it's teaching me things
bring information to me uh enriching me
and making me a a smarter CEO so so I
think for your kids I would highly
recommend that they do that and and um I
would pay for the professional one it's
only $20 a month and and um it's a tutor
literally a tutor for you know a
Christmas present um I would right for
every one of them just give him a
Christmas present here's your Christmas
present a tutor they go
[Applause]
oh I love you Dad you're the best dad in
the world I will say with getting off I
was talking to a CTO of a large Fortune
Fortune 50 bank and the CTO was talking
about his son 5-year-old son that was
literally voice
interacting with a you know large
language model and and going back and
forth about dinosaurs so he was
literally listening or learning about
the history of dinosaurs but then you
know you get the creativity of a kid
that is unlimited was talking about well
what about the dinosaur with I want with
pink hair and they would literally you
know all of a sudden it would kick
something back around so if you think
about in ways that you know kids have
you know there is no guardrail around
their creativity and how they think but
it was fascinating to me that the
individual saying I was blown away by
the interaction that my 5-year-old had
with the large language mod how much he
was learning around the history of
dinosaurs but also he had no limitations
about what he was asking and it was
playing back with him so I I think you
know from a creativity standpoint and
Innovation uh it it really is unlimited
so maybe just in closing out thank you
that was an excellent last question yeah
well I just want to want I want to say
that that uh it's it's a great pleasure
working with you Jim and and your team
and wwt is a fantastic organization uh
you could tell that wwt was built by
hand from the ground up

this is how intelligence is
made a new kind of
factory generator of
tokens the building blocks of
AI tokens have opened a new
frontier the first step into an
extraordinary
world where less possibilities are
born tokens transform images into
scientific
data charting alien
atmospheres and guiding the explorers of
tomorrow they turn raw data into
foresight so next
time we'll be ready
[Music]
tokens decode the laws of
physics to get us there
[Music]
faster and take us
further tokens see disease before it
takes
hold they help us unravel the language
of life
and learn what makes us
tick tokens connect the
dots so we can protect our most noble
[Music]
creatures they turn potential into
[Music]
plenty and help us Harvest our
[Music]
Bounty tokens don't just teach robots
how to move but to bring
[Music]
joy to lend us a
hand and put life within reach
[Music]
together we take the Next Great Leap to
bravely
go where no one has gone
[Music]
before and
here is where it all
[Music]
begins welcome to the stage Nvidia
founder and CEO Jensen Wong
[Music]
welcome to
GTC what an amazing
year we wanted to do this at
Nvidia so through the magic of
artificial
intelligence we're going to bring you to
nvidia's
headquarters I think I'm bringing you to
Nvidia headquarters
what do you
think this
is this is where we work this is where
we work what an amazing year it was and
we have a lot of incredible things to
talk about and I just want you to know
that I'm up here without a net there are
no scripts there's no teleprompter and
I've got a lot of things to cover so
let's get started first of all I want to
thank all of the sponsors all the
amazing people who are part of this
conference just about every single
industry is
represented healthc care is here
Transportation
retail gosh the computer industry
everybody in the computer industry is
here and so it's really really terrific
to see all of you and thank you for
sponsoring it
GTC started with gForce it all started
with GeForce and
today I have here a GeForce
5090 and
5090 unbelievably 25 years
later 25 years after we started working
on GeForce GeForce is sold out all over
the world this is the 90 the Blackwell
generation and comparing it to the 490
look how it's 30% smaller in volume it's
30% better at dissipating energy and
incredible performance hard to even
compare and the reason for that is
because of artificial
intelligence GeForce brought Cuda to the
world Cuda enabled Ai and AI has now
come back to revolutionized computer
Graphics what you're looking at is
realtime computer Graphics 100% path
traced for every pixel that's rendered
artificial intelligence predicts the
other 15
think about this for a second for every
pixel that we mathematically rendered
artificial intelligence inferred the
other 15 and it has to do so with so
much Precision that the image looks
right and it's temporally accurate
meaning that from frame to frame to
frame going forward or backwards because
it's computer Graphics it has to stay
temporarily stable incredible artificial
intelligence has made extraordinary
progress it has only been 10 years now
we've been talking about AI for a little
longer than that but AI really came into
the world's Consciousness about a decade
ago started with perception AI computer
vision speech
recognition then generative AI the last
5 years we've largely focused on
generative AI teaching an AI how to
translate from one modality to another
another modality text to image image to
text text to
video amino acids to
proteins properties the chemicals all
kinds of different ways that we can use
AI to generate generate content
generative AI fundamentally changed how
Computing is done from a retrieval
Computing model we now have a generative
Computing model whereas almost
everything that we did in the past was
about creating content in advance
storing multiple versions of it and
fetching whatever version we think is
appropriate at the moment of use now ai
understands the context understands what
we're asking understands the meaning of
our request and generates what it knows
if it needs it'll retrieve information
augments its understanding and generate
answer for us rather than retrieving
data it now generates answers
fundamentally changed how Computing is
done every single layer of computing has
been
transformed the last several years the
last couple two three years major
breakthrough happened fundamental
advance in artificial intelligence we
call it a gentic AI a gentic AI
basically means that you have an AI that
has agency it can perceive and
understand the context of the
circumstance it can reason very
importantly can reason about how to
answer or how to solve a problem and it
can plan and action it can plan and take
action it can use tools because it now
understands multimodality information it
can go to a website and look at the
format of the website words and videos
maybe even play a
video learns from what it learns from
that website understands it and come
back and use that information use that
new knowledge to do its job agentic AI
at the foundation of agentic AI of
course something that's very new
reasoning and then of course the next
wave is already happening we're going to
talk a lot about that today robotics
which is been enabled by physical ai ai
that understands the physical world it
understands things like friction and
inertia cause and
effect object permanence when something
goes around the corner doesn't mean has
disappeared from this universe it's
still there just not seeable and so that
ability to understand the physical world
the three-dimensional world is what's
going to enable a new era of AI we
called physical Ai and it's going to
enable robotics each one of these phases
each one of these waves opens up New
Market opportunities for all of us it
brings more and new partners to GTC as a
result GTC is now
jam-packed the only way to hold more
people at GTC is we're going to have to
grow San Jose and and we're working on
it we got a lot of land to work with we
got to grow San
Jose so that we can make GTC I have just
just you know as I'm standing here I
wish all of you could see what I see and
we're we're in the middle of a stadium
um and La last year was the first year
back that we did this live and it was it
was like a rock concert and it was
described GTC is was described as the
Woodstock of AI and this year it's
described as the Super Bowl of
AI the only difference is everybody wins
at this Super Bowl everybody's a winner
and so every single year more people
come because AI is able to solve more
interesting problems for more Industries
and more companies and this year we're
going to talk a lot about agentic Ai and
physical AI at its
core what enables each wave and each
phase of AI three
fundamental matters are involved the
first is how do you solve the data
problem and the reason why that's
important is because AI is a datadriven
computer science approach it needs data
to learn from it needs digital
experience to learn from to gain its dig
to learn knowledge and to gain digital
experience how do you solve the data
problem the second is how do you solve
the training problem without human in
the loop the reason why human in the
loop is fundamentally challenging is
because we only have so much time and we
would like and AI to be able to learn at
super human rates at Super realtime
rates and to be able to learn at a scale
that no humans can keep up with and so
the second question is how do you train
the model and the
third is how do you
scale how do you create how do you find
an algorithm whereby the more resource
you provide whatever the resource is the
small smarter the AI
becomes the scaling law well this last
year this is where almost the entire
world got it
wrong the computation requirement the
scaling law of AI is more
resilient and in fact hyper
accelerated the amount of computation we
need at this point as a result of
agentic AI as a result of reasoning is
easily a hundred times more than we
thought we needed this time last year
and let's reason about why that's true
the first part is let's just go from
what the AI can do let me work
backwards agentic AI as I mentioned at
this Foundation is reasoning we now have
AIS that can reason which is
fundamentally about breaking a problem
down step by step
maybe it approaches a problem in a few
different ways and selects the best
answer maybe it solves the problems the
same problem in a variety of
ways and and sure it has the best the
same answer consistency checking or
maybe after it's done deriving the
answer it plugs it back into the
equation maybe a quadratic equation to
confirm that in fact that's the right
answer instead of just
one shot bluring it out remember two
years ago when we started working with
chat GPT a miracle as it
was many complicated questions and many
simple questions it simply can't get
right and it's understandably so it took
a one shot whatever it learned by
studying pre-trained data whatever it
saw from other experiences pre-trained
data it does a one shot blurps it out
like a saon now we have AIS that can
reason step by step by step using a
technology called Chain of Thought best
of n consistency checking a variety of
different path planning a a variety of
different techniques we now have AIS
that can reason break a problem down
reason step by step by step well you
could imagine as a result the number of
tokens we generate and the fundamental
technology of AI is still the same
generate the next token predict the next
token token it's just that the next
token now makes up step
one then the next token after that after
it generates step one that step one has
gone into the input of the AI again as
it generates step two and step three and
step four so instead of just generating
one token or one word after next it
generate a sequence of words that
represents a step of reasoning the
amount of tokens that's generated as a
result is substantially higher and I'll
show you in a second easily 100 times
more now 100 times more what does that
mean well it could generate a 100 times
more tokens and you can see that
happening as I explained previously
or the model is more complex it
generates 10 times more tokens and in
order for us to keep the model
responsive
interactive so that we don't lose our
patients waiting for it to think
we now have to compute 10 times faster
and so 10 times tokens 10 times faster
the amount of computation we have to do
is 10 100 times more easily and so
you're going to see this in the rest of
the presentation the amount of
computation we have to do for inference
it's dramatically higher than it used to
be well the question then becomes how do
we teach an AI how to do what I just
described how to execute this chain of
thigh well one method is you have to
teach the AI how to reason and as I
mentioned earlier in training there are
two fundamental problems we have to we
have to solve where does the data come
from where does the data come from and
how do we not have it be limited by
human in the loop there's only so much
data and so much human demonstration we
can perform and so this is the big
breakthrough in the last couple years
reinforcement learning verifiable result
basically reinforcement learning of an
AI as Ito as it attacks or tries to
engage solving a problem step by step
well we have many problems that have
been solved in the history of humanity
where we know the
answer we know the equation of a
quadratic equation how to solve that we
know how to solve a Pythagorean theorem
um the the rules of a right triangle we
know many many rules of math and
geometry and logic and science we have
puzzle games that we could give
it constraints constraint constrainted
um uh uh type of problems like Sudoko um
those kind of problems on and on and on
we have hundreds of these problem spaces
where we can generate millions of
different
examples and give the AI hundreds of T
hundreds of chances to solve it step
byep step by step as we use
reinforcement learning to reward it as
it does a better and better job so as a
result you take hundreds of different
topics millions of different examples
hundreds of different tries each one of
the tries generating tens of thousands
of tokens you put that all together
we're talking about trillions and
trillions of tokens in order to train
that model and now with reinforcement
learning we have the ability to generate
an enormous amount of tokens synthetic
data generation basically using a
robotic approach to teach an AI the
combination of these two things has put
an
enormous enormous challenge of computing
in front of the industry and you can see
that the industry is responding this is
what I'm about to show
you is Hopper
shipments of the top four csps
the the top four csps they're the the
ones with the public clouds uh Amazon
Azure gcp and oci the top four C top
four csps not the AI companies that's
not included not all the startups not
included not Enterprise not included a
whole bunch of things not included just
those four just to give you a sense of
comparing the peak year of Hopper and
the first year of Blackwell okay the
peak year of Hopper oper in the first
Weir of black well so you can kind of
see that in fact AI is going through an
inflection
point it has become more useful because
it's smarter it can reason it is more
used you can tell it's more used because
whenever you go to chat GPT these days
the the it seems like you have to wait
longer and longer and longer which is a
good thing it says a lot of people are
using it with great effect and the
amount of computation necessary to train
those models and to influence those
models has grown tremendously so in just
one
year and blackw is just started shipping
in just one year you could see the
incredible growth in AI
infrastructure well that's been
reflected in Computing across the board
we're now seeing and this is the purple
is the forecast of uh of an of analysts
uh about the the next uh the increase of
capital expense
of the world's data centers including
csps and Enterprise and so on um the
world's data centers uh through uh the
end through the end of the decade so
2030 um I've said before that I expect
data center buildout to reach a trillion
dollars and I am fairly certain we're
going to reach that very soon two
Dynamics is happening at the same time
the first Dynamic is
that the vast majority of that growth is
likely to be accelerated meaning we've
known for some time that general purpose
Computing is run out of course run its
course and that we need a new Computing
approach and the world is going through
a platform
shift from hand-coded software running
on general purpose computers to machine
learning software running on
accelerators and gpus this way of doing
computation is at this Point past this
Tipping Point and we are now seeing the
inflection point happening the
inflection happening in the world's data
center build outs so the first thing is
a transition in the way we do
Computing second is an increase in
recognition that the future of software
requires capital investment now this is
a very big
idea whereas in the past we wrote the
software and we ran it on computers
in the future the computer's going to
generate the tokens for the software and
so the computer has become a generator
of tokens not a retrieval of files from
retrieval based Computing to generative
based Computing from the old way of
doing data centers to a new way of
building these infrastructure and I call
them AI factories they're AI factories
because it has one job and one job only
generating these incredible tokens that
we then reconstitute into music into
words into videos
into Research into chemicals or proteins
we reconstitute it into all kinds of
information of different types so the
world is going through a transition in
not just the amount of data centers that
will be built but also how it's
built well everything in the data center
will be accelerated not all of its Ai
and I want to say a few words about this
you know this slide this slide this
slide is is uh genuinely my favorite and
the reason for that is because for all
of you who' been coming to GTC uh all of
these years you've been listening to me
talk about these libraries uh this whole
time this this is in fact what GTC is
all about this one slide and in fact a
long time ago 20 years ago this is the
only only slide we had one library after
another library after another the
library you can't just accelerate
software just as we needed an AI
framework in order to create AIS and we
accelerate the AI Frameworks you need
Frameworks for physics and biology and
multiphysics and you know all kinds of
different quantum physics you need all
kinds of libraries and Frameworks we
call them Cuda X libraries acceleration
Frameworks for each one of these fields
of science and so this first one is
incredible this is C cpai numeric uh
numpy is the number one most downloaded
python Library most used python library
in the world downloaded 400 million
times this last year uh kitho is
computate and Cai numeric is a um uh
zero change drop in acceleration for
numpy so if any of you are using numpy
out there uh give Cai numeric a try
you're going to love it a kitho a
computational phography Library over the
course of four years we've now taken the
entire process of processing lithography
computational lithography which is the
second Factory in a Fab there's the
factory that manufactures the Wafers and
then there's the factory that
manufactures the information to
manufacture the
Wafers every industry every company that
has factories will have two factories in
the future the factory for what they
build and the factory for the
mathematics the factory for the
AI Factory for cars Factory for AIS for
the cars Factory
for smart speakers and factories for AI
for the smart speakers and so kitho is
our computational theography tsmc
Samsung asml our partners synopsis
Mentor incredible support all over I
think that this is now at its Tipping
Point in in another 5 years time every
mask every single lithography will be
processed on Nvidia Cuda Ariel is our
library for 5G turning a GPU into a 5G
radio why not signal processing is
something we do incredibly well once we
do that we can layer on top of it ai ai
for Rand or what we call AI Rand the
next generation of of uh of uh Radio
Radio Networks uh will be will have ai
deeply inserted into it why is it that
we're limited by the limits of
information Theory um because there's
only so much information Spectrum we can
get not if we add AI to it uh Coop
numerical or mathematical optimization
almost every single industry uses this
when you plan uh seats and uh flights uh
inventory and
customers um uh workers and plants uh uh
drivers and Riders
uh so on so forth where we have multiple
constraints multiple constraints um a
whole bunch of variables and you're
optimizing
for
time uh profit uh quality of service um
usage of resource whatever it happens to
be Nvidia uses it for our supply chain
management uh kuop is an incredible
Library it takes What It Takes what
would take hours and hours and it turns
into into seconds the reason why that's
a big deal is so that we can now explore
much larger space we announced uh that
we are going to open source Coop the
almost everybody is using either guui uh
goobi or um IBM clex uh or FICO uh we're
working with all three of them the
industry is so excited we're about to
accelerate The Living Daylights out of
the industry uh pair bricks for uh Gene
sequencing and Gene uh analysis Moni the
world's leading Medical Imaging Library
Earth 2 multif physics for pre for uh
predicting in very high resolution uh
local weather uh C Quantum and Cuda Q
we're going to have our
first Quantum day here at GTC we're
working with just about everybody in the
ecosystem either helping them research
on Quantum architectures Quantum
algorithms or in building a uh classical
accelerated Quantum uh heterogeneous
architecture and so really exciting work
there uh coup equivariance and censor
for tensor contraction quantum chemistry
of course this stack is world famous
people think that there's one piece of
software called CA but in fact on top of
Cuda is a whole bunch of libraries
that's integrated into all different
parts of the ecosystem and software and
infrastructure in order to make AI
possible uh I've got a new one here to
announce today
uh qdss uh our Spar solvers really
important for
CAE this is one of the biggest things
that has happened in the last year
working with cadence and synopsis and
ansis and the so and um and and well all
all of the uh the systems companies
we've now made possible uh just about
every important Eda and CAE library to
be accelerated what's amazing is until
recently Nvidia has been using general
purpose
computers running software super slowly
to design accelerated computers for
everybody else and the reason for that
is because we never had that software
that body of software optimized for a
Cuda until recently and so now our
entire industry is going to get
supercharged as we move to accelerated
Computing uh CDF a data frame for
structure data we now have a drop in
acceleration for spark and drop in
acceleration for pandas incredible and
then we have warp a library for physics
that runs in p a python library for
physics for Cuda we have a big
announcement there I'll save it in just
a
second this is just a
sampling of the libraries that make
possible accelerated Computing it's not
just Cuda we're so proud of Cuda but if
not for Cuda and the fact that we have
such a large install base none of these
libraries would be useful for any of the
developers who use them for all the
developers that use
them you use it because one it's going
to give you incredible speed up it's
going to give you incredible scale
up and two because the install base of
Cuda is now everywhere it's in every
cloud it's in every data center it's
available from every computer company in
the world it's every literally
everywhere and therefore by using one of
these
libraries your software your amazing
software can reach everyone and so we've
now reached the Tipping Point of
accelerated Computing Cuda has made it
possible and all of you this is what GTC
is about the ecosystem all of you made
this possible and so we made a little
short video for you thank you
to the
creators the Pioneers the Builders of
the
future Cuda was made for
you since
2006 6 million developers in over 200
countries have used Cuda and transformed
Computing with over 900 Cuda X libraries
and AI models you're accelerating
science
reshaping
Industries and giving machines the power
to see learn and
reason now Nvidia Blackwell is 50,000
times faster than the first Cuda
GPU these orders of magnitude gains in
speed and scale are closing the gap
between
simulation and realtime digital twins
[Music]
and for you this is still just the
beginning we can't wait to see what you
do next
[Music]
I love what we do I love even more what
you do with it and one of the things
that that most touch me in in my 33
years doing
this one scientist said to me Jensen
because of the work because of your work
I can do my life's work in my
lifetime and boy if that doesn't if that
doesn't touch you well you got to be a
corpse so this is all about you guys
thank you all right so we're going to
talk about
AI but you know AI started in a cloud it
started in the cloud for a good reason
because it turns out that AI needs
infrastructure it's machine learning in
if the science say machine learning then
you need a machine to do the science and
so machine learning requires
infrastructure and the cloud data
centers had infrastructure they also
have extraordinary computer science
extraordinary research the perfect
circumstance for AI to take off in the
cloud and the csps but that's not where
AI is limited to AI will go everywhere
and we're going to talk about AI in a
lot different ways and the cloud service
providers of course they they they like
our Leading Edge technology they like
the fact that we have full stack because
accelerated Computing as you know as I
was explaining earlier is not about the
chip it's not even just the chip in the
library the programming model is the
chip the programming model and a whole
bunch of software that goes on top of it
that entire stack is incredibly complex
each one of those layers each one of
those libraries is essentially like SQL
SQL as you know is called in storage
Computing it was the big revolution of
computation by IBM SQL is one Library
just imagine I just showed you a whole
bunch of them and in the case of AI
there's a whole bunch more so the stack
is complicated they also love the fact
that csps love that Nvidia Cuda
developers are CSP customers because in
the final analysis they're building
infrastructure for the world to use and
so the rich developer ecosystem is
really valued and really really uh
deeply appreciated well now that we're
going to take AI out to the rest of the
world the rest of the world has
different system
configurations operating environment
differences domain specific Library
differences usage differences and so AI
as it translates to Enterprise it as it
translates to manufacturing as it
translates to robotics or self-driving
cars or
even companies that are starting GPU
clouds there's a whole bunch of
companies maybe 20 of them who started
during the Nvidia time and what they do
is just one thing they host gpus they
call themselves GPU clouds and one of
our one of our great Partners cor weave
is in the process of going public and
we're super proud of them
and so GPU clouds they have their own
requirements but one of the areas that
I'm super excited about is
Edge and today we announced we announced
today that Cisco Nvidia T-Mobile the
largest telecommunications company in
the world cus
ODC are going to build a full
stack for Radio Networks here in the
United States and and that's going to be
the second stack so that this current
stack this current stack we're
announcing today will put AI into the
edge remember a hundred billion dollar
of the
world's Capital Investments each year is
in the Radio Networks and all of the
data centers provisioning for
Communications in the future there is no
question my mind that's going to be
accelerated Computing infused with ai ai
will do a far far better job adapting
the radio signals the massive MOS to the
changing environments and the traffic
conditions of course it would of course
we would use reinforcement learning to
do that of course myo is essentially one
giant radio robot of course it is and so
we will of course provide for those
capabilities of course AI could
revolutionize Comm
Communications you know when I call home
you don't have to say but that that few
words because my wife knows where I work
what that condition's like conversation
Carries On from yesterday she kind of
remembers what I like don't like and
often times just a few words you
communicated a whole bunch the reason
for that is because of context and human
priors prior knowledge well combining
those capabilities could revolutionize
commun Communications look what it's
doing for video processing look look
what I just described earlier in 3D
graphics and so of course we're going to
do the same for Edge so I'm super
excited about the announcement that we
made today T-Mobile Cisco Nvidia cus ODC
are going to build a full
stack well AI is going to go into every
industry that's just
one one of the earliest Industries that
AI went into was autonomous vehicles The
Moment I Saw alexnet and we've been
working on computer vision for a long
time the moment I saw alexnet was such
an ex inspiring moment such an exciting
moment it caused us to decide to go all
in on building self-driving cars so
we've been working on self-driving cars
now for over a
decade we build technology that almost
every single self-driving car company
uses it could be either in the data
center for example Tesla uses Nvidia
lots of Nvidia gpus in the data center
it could be in the data center or the
car wayo and wave uses Nvidia computers
in data centers as well as the car it
could be just in the car it's very rare
but sometimes it's just in the car or
they use all of our software in addition
we work with the car industry however
the car industry would like us to work
with them we build all three computers
the training computer the simulation
computer and the robotics computer the
self-driving car computer all the
software stack that sits on top of it
models and
algorithms just as we do with all of the
other industries that I've demonstrated
and so
today I'm super excited to
announce that GM has selected Nvidia to
partner with them to build their future
self-driving car Fleet
the time for autonomous vehicles has
arrived and we're work looking forward
to building with GM AI in all three
areas AI for manufacturing so they could
revolutionize the way they manufacture
AI for Enterprise so they could
revolutionize the way they work design
cars and simulate cars and and then also
AI for in the car so AI infrastructure
for GM partnering with with GM and
building with GM their AI so I'm super
excited about that one of the areas that
I'm deeply proud of and it rarely gets
any
attention is safety Automotive Safety
it's called
halos in our companies called
Halos safety
requires technology from Silicon to
systems to system software the
algorithms
the
methodologies everything from diversity
to ensuring
diversity monitoring and
transparency
explainability all of these different
philosophies have to be deeply ingrained
into every single part of how you
develop the system and the software
we're the first company in the world I
believe to have every line of code
safety assessed 7 million lines of code
safety assessed our chip our system our
system software and our algorithms are
safety assessed by Third parties that
crawl through every line of code to
ensure that it is designed to ensure
diversity transparency and
explainability we also have followed
over a thousand patents and during this
GTC and I really encourage you to do so
is to go spend time in the Halos
Workshop so that you could see all of
the different things that comes together
to ensure that cars of the future are
going to be safe as well as autonomous
and so this is something I'm very proud
of it barely it rarely gets any
attention and so I I thought I would
spend the extra time this time to talk
about that okay Nvidia
halos all of you have seen cars drive by
themselves um the wayo robo taxis are
incredible but we made a video to share
with you some of the technology we
use to solve the problems of data and
training and diversity so that we could
use the magic of AI to go create AI
let's take a
look Nvidia is Accel ating AI
development for AVS with Omniverse and
[Music]
Cosmos Cosmos prediction and reasoning
capabilities support AI first AV systems
that are endtoend trainable with new
methods of development model
distillation closed loop training and
synthetic data generation first model
distillation adapted as a policy model
cosmos's driving knowledge transfers
from from a slower intelligent teacher
to a smaller faster student inferenced
in the
car the teacher's policy model
demonstrates the optimal trajectory
followed by the student model learning
through
iterations until it performs at nearly
the same level as the
teacher the distillation process
bootstraps a policy model but complex
scenarios require further
tuning closed loop training enables
fine-tuning of policy
models log data is turned into 3D scenes
for driving closed loop in physics based
simulation using Omniverse neural
reconstruction variations of these
scenes are created to test a model's
trajectory generation
[Music]
capabilities Cosmos Behavior evaluator
can then score the generated driving
behavior to measure model
performance newly generated scenario
and their evaluation create a large data
set for Clos Loop training helping AVS
navigate complex scenarios more
robustly last 3D synthetic data
generation enhances av's adaptability to
diverse
environments from log data Omniverse
builds detailed 4D driving environments
by fusing maps and images and generates
a digital twin of the real world
including segmentation to guide Cosmos
by classifying each pixel Cosmos then
scales the training data by generating
accurate and diverse scenarios closing
the Sim to real
Gap Omniverse and Cosmos enable AVS to
learn adapt and drive intelligently
advancing safer Mobility
[Music]
Nvidia is the perfect company to do
that
gosh that's our
destiny use AI to recreate AI the
technology that we showed you there uh
is very similar to the technology that
you're enjoying um uh to uh take you to
a digital twin we call Nvidia all right
let's talk about data centers
that's not bad
huh gausian Splats just in case gaan
Splats well let's talk about data
centers uh Blackwell is in full
production and this is what it looks
like it's an incredible incredible you
know for for people for us this is a
sight of beauty would you
agree this
is how how is this not beautiful how is
this not beautiful well this is a big
deal
because we made a fundamental transition
in computer architecture I just want you
to know that in fact I've shown you a
version of this uh about 3 years ago it
was called uh Grace Hopper and the
system was called
ranger the ranger system uh is about uh
maybe about half of the width of the
screen and it was the world's first MV
link
32 3 years ago we showed Ranger working
and it
was way too large
but it was exactly the right idea we
were trying to solve scale up
distributed computing is about using a
whole lot of different computers working
together to solve a very large problem
but there's no replacement for scaling
up before you scale out both are
important but you want to scale up first
before you scale out while scaling up is
incredibly hard there is no simple
answer for it you're not going to scale
SC it up you're not going to scale it
out like Hadoop take a whole bunch of
commodity uh computers hook it up into a
large Network and do in storage
Computing using Hadoop Hadoop was a
revolutionary idea as we know it enabled
hyperscale data centers to solve
problems of gigantic
sizes and uh off using offto shelf
computers however the problem we're
trying to solve is so complex that
scaling
in that way would have simply cost way
too much power way too much energy it
would have never deep learning would
have never happened and so the thing
that we had to do was scale up first
well this is the way we scaled up I'm
not going to lift this this is this is
70 lbs this is the the the last
generation system architecture is called
hgx this revolutionized Computing As We
Know It This revolutionize artificial
intelligence this is eight
gpus eight gpus each one of them is kind
of like this okay this this is two gpus
two Blackwell gpus in one black wall
package two black wall gpus in one black
black black wall package and um uh
they're eight of these underneath
this okay and this connects into what we
call MV link
8 this then connects to a CPU
shelf like that so there's dual CPUs and
that sits on top and we connect it over
PCI Express and then many of these get
connected with
infiniband which turns into uh what is
an AI supercomputer this is the way it
was in the past this is the way this is
how we started well this is as far as we
scaled up before we scaled out but we
wanted to scale up even further and I I
told you that Ranger took this system
and scaled it out scaled it up by
another factor of four and so we had MV
link 32 but the system was way too large
and so we had to do something quite
remarkable re re-engineer how MV length
worked and how scale up worked and so
the first thing that we did was we said
listen the mvlink switches are in this
system embedded on the
motherboard we need we need to
disaggregate the mvlink system and take
it out so this is the mvy link system
okay this is an mvy link
switch this is the most this is the
highest performance switch the world's
ever made and this makes it possible for
every GPU to talk to every GPU at
exactly the same time at full bandwidth
okay so this is the mvlink switch we
disaggregated it we took it out and we
put
it in the center of the
chassis so there's all the there 18 of
these switches in nine n different racks
nine different
switch trays we call them and then the
switches are disaggregated the compute
is now sitting in here this is
equivalent to these two things in
compute what's amazing is this is
completely liquid cooled and by liquid
cooling it we can
compress all of these compute nodes into
one rack this is the big change of the
entire industry all of you in the
audience I know how many of you are here
I want to thank thank you for making
this fundamental shift from integrated
MV link to disaggregated MV link from
air
cooled to liquid
cooled from 60,000
components per computer or so to 600,000
components per rack
120
kilow fully liquid cooled and as a
result we have a one Exel flops computer
in one rack isn't it
incredible so this is the compute node
this is the compute
node okay and that now fits in in one of
these now
we 3,000
lb 5,000
cables about 2 miles
worth just an incredible Electronics
600,000 Parts I think that's like 20 20
cars 20 cars worth of parts and
integrates into one supercomputer
well our goal is to do this our goal is
to do scale up and this is what it now
looks like we essentially wanted to
build this chip it's just that no
retical limits can do this no process
technology can do this it's 30 trillion
transistors 20 trillion of it is used
for computing so it's not like you you
could you can't reasonably build this
anytime soon and so the way to solve
this problem is to disaggregate it as I
described into the grace Blackwell
mvlink
72 rack but as a result we have done the
ultimate scale up this is the most
extreme scale up the world has ever done
the amount of computation that's
possible here the memory bandwidth 570
terabytes per second everything is
everything in this machine is now in
teas everything's a trillion and you
have uh an EXA flops which is a million
trillion floating Point operations per
second well the reason why we wanted to
do
this is to solve an extreme
problem and that extreme problem a lot
of people misunderstood to be easy and
in fact it is the ultimate extreme
Computing problem and it's called
inference and the reason for that is
very
simple
inference is token Generation by a
factory and a factory is revenue and
profit
generating or lack
of and so this Factory has to be built
with
extreme efficiency with Extreme
Performance because everything about
this Factory directly affects
your quality of service your revenues
and your profitability let me show you
how to read this chart because I want to
come back to this a few more times
basically you have two axes on the x
axis is the tokens per second whenever
you chat when you uh put a prompt into
chat GPT what comes out is tokens those
to tokens are reformulated into
words you know it's more than a token
per word okay and they'll tokenize
things like th could be used for the it
could be used for them it could be used
for Theory it could be used for
theatrics it could be used for all kinds
of okay and so th is a Tok an example of
a token they reformulate these tokens to
turn into words well we've already
established that if you want your AI to
be smarter you want to generate a whole
bunch of tokens those tokens are
reasoning tokens consistency checking
tokens coming up with a whole bunch of
ideas so they can select the best of
those ideas tokens and so those tokens
might they it might be second guessing
itself it might be is this the best work
you could do and so it ask it talks to
itself just like we talk to ourselves
and so the more tokens you generate the
smarter your AI
but if you take too long to answer a
question the customer is not going to
come back this is no different than WB
search there is a real limit to how long
it can take before comes back with a
smart answer and so you have these two
Dimensions that you're fighting against
you're trying to generate a whole bunch
of tokens but you're trying to do it as
quickly as possible Therefore your token
rate
matters so you want your tokens per
second for that one user to be as fast
as
possible
however in Computer Sciences in
factories there's a fundamental tension
between latency response time and
throughput and and the reason is very
simple if you're in the large high
volume business you batch up it's called
batching you batch up a lot of customer
demand and you
manufacture a certain version of it for
everybody to consume
later however from the moment that they
batched up and
manufacture whatever they did to the
time that you consumed
it could take a long
time so no different for computer
science no different than no to no
different for AI factories that are
generating tokens and so you have these
two fundamental tensions on the one hand
you would like the customer quality of
service to be as good as possible smart
AIS that are super fast on the other
hand you're trying to get your data
center to produce tokens for as many
people as possible so you can maximize
your
revenues the perfect answer is to the
upper right
ideally the shape of that curve is a
square that you could generate very fast
tokens per person up until the limits of
the factory but no Factory can do that
and so it's probably some curve and your
goal is to maximize the area under the
curve okay the product of X and Y and
the further you push out more likely it
means the better of a factory that
you're building well it turns out that
in tokens per second for the whole
Factory and tokens per second response
time one of them requires enormous
amount of computation flops and then the
other dimension requires an enormous
amount of bandwidth and flops and so
this is a very difficult problem to
solve the the good answer is that you
should have lots of flops and lots of
bandwidth and lots of memory and lots of
everything
that's the best answer to start which is
the reason why this is such a great
computer you start with the most flops
you can the most memory you can the most
bandwidth you can of course the best
architecture you can the most Energy
Efficiency you can and you have to have
a programming model that allows you to
run software across all of this insanely
hard so that you can do this now let's
just take a look at this one demo to
give you a tactical feeling of what I'm
talking about please play
it traditional llms capture foundational
knowledge while reasoning models help
solve complex problems with thinking
tokens here a prompt asks to seat people
around a wedding table while adhering to
constraints like traditions photogenic
angles and feuding family
members traditional llm answers quickly
with under 500 tokens it makes mistakes
in seating the guests while the
reasoning model thinks with over 8,000
tokens to come up with the correct
answer it takes a pastor to keep the
peace okay as
as as as all of you know as all of you
know if you have a wedding party of
300 and you're trying to find the
perfect well the optimal seating for
everyone
that's a problem that only AI can solve
or a mother-in-law can solve and
so that's one of those problems that
that Koop cannot
solve okay so what you see here is that
that uh we gave it a problem that
requires reasoning and you saw uh R1
goes off and it reasons about it tries
all these different scenarios and it
comes back and a tests his own answer it
asks it asks itself whether it did it
right meanwhile
the last generation language model does
a one shot so the one shot is 439 tokens
it was fast it was effective but it was
wrong so it was 439 wasted
tokens on the other hand in order for
you to reason about this problem and
this is just a that was actually a very
simple problem you know we just give it
a few more un few more difficult
variables and it becomes very difficult
to reason through and it took 8,000
almost 9,000 tokens and it took a lot
more computation because the model's
more complex okay so that's one
dimension before I show you some results
let me just show let me explain
something else so the answer if you look
at if you look at um blackw you look at
the the Blackwell system and it's now
this scaled up MV link 72 the first
thing that we have to do is we have to
take this model and this model is not
small it's you know in the case of R1
people think R1 is small but it's 680
billion parameters Next Generation
models could be trillions of
parameters and the way that you solve
that problem is you take these trillions
and trillions of parameters and this
model and you uh distribute the workload
across the whole system of gpus you can
use uh tensor parallel you can take one
layer of the model and and run it across
multiple gpus you you could take um a a
slice of the pipeline and call that
pipeline parallel and put that on
multiple gpus you could take different
experts and put it across different gpus
we call it expert parallel the con the
combination of pipeline parallelism and
tensor parallelism and expert
parallelism the number of combinations
is insane and depending on the model
depending on the workload depending on
the conf the circumstance how you
configure that computer has to
change so that you can get the maximum
throughput out of it you also sometimes
optimize for very low lenes sometimes
you try trying to optimize for
throughput and so you have to do some
inflight batching a lot of different
techniques for batching and and uh
aggregating work and so the the software
the operating system for these AI
factories is insanely complicated well
one of the
observations and this is this is a
really terrific terrific thing about
having a homogeneous architecture like
mvlink 72 is that every single GPU could
do all the things that I just described
and we
observe
that these reasoning models are doing a
couple phases of computing one of the
phases of computing is thinking when
you're thinking you're not producing a
lot of tokens you're producing tokens
that you're maybe consuming yourself
you're thinking maybe you're reading
you're digesting information that
information could be a PDF the
information could be a website you could
literally be watching a video ingesting
all of that at Super linear rates and
you take all of that information and you
then formulate the answer formulate a
planned answer and so that digestion of
information context processing is very
flops
intensive on the other hand during the
next phase is called decode so the first
part we call pre-fill the next phase of
decode requires floating Point
operations but it requires an enormous
amount amount of bandwidth and it's
fairly easy to calculate you know if you
have a model and it's a few trillion
parameters well it takes a few terabytes
per second notice I was mentioning 576
terabytes per second it takes terabytes
per second to just pull the mod model in
from hbm
memory and to generate literally one
token and the reason it generates one
token is because remember that these
large language models are predicting the
next token that's why they say the next
token it's not predicting every single
token it's predicting the next token now
we have all kinds of new techniques
speculative decoding and all kinds of
new techniques for doing that faster but
in the final analysis you're predicting
the next token okay and so that you
ingest pull in the entire model and the
context we call it a KV cache and then
we produce one token and then we take
that one token we put it back into our
brain we produce the next token
every single one every single time we do
that we take trillions of parameters in
we produce one token trillions of
parameters in produce another token
trillions of parameters in produce
another token and notice that demo we
produced
8,000 600 tokens so trillions of btes of
information trillions of btes of
information have been taken into our
gpus and produce one token at a time
which is fundamental ly the reason why
you want mvy link Envy link gives us the
ability to take all of those gpus and
turn them into one massive
GPU the ultimate scale up and the second
thing is that now that everything is on
mvy link I can
disaggregate the prefill from the decode
and I could decide I want to use more
gpus for
prefill less for decode because I'm
thinking a lot
I'm doing it's agentic I'm reading a lot
of information I'm doing deep research
notice during deep
research you know and and earlier I was
listening to Michael and Michael was
talking about his his him doing research
and I do the same thing and we go off
and we write these really long research
projects for our Ai and I love doing
that because you know I already paid for
it and I just love making our gpus work
and nothing gives me more joy so so so I
I write up and then it goes off and it
does all this research and it went off
to like 94 different websites and I read
all this and I'm reading all this
information and it formulates an answer
and writes the report it's incredible
okay during that entire time prefill is
super busy and it's not really
generating that many tokens on the other
hand when you're chatting with the
chatbot and millions of us are doing the
same thing it is very token gener
generation heavy it's very decode heavy
okay and so um depending on the workload
we might decide to put more gpus in the
decode dep depending on the workload put
more gpus into prefill well this Dynamic
operation is really complic complicated
so I've just now described pipeline
pipeline parallel tensor parallel um
expert parallel pre uh inflight batching
disaggregated inferencing workload
management and then I've got to take
this thing called the KV cache I got to
Route it to the right GPU I've got to
manage it through all the memory
hierarchies that piece of software is
insanely complicated and so today we're
announcing the Nvidia
Dynamo Nvidia Dynamo does all that it is
essentially the operating system of an
AI
Factory whereas in the past in the way
that we ran data centers our operating
system would be something like VMware
and we would orchestrate and we still do
um you know we're big user orchestrate a
whole bunch of different Enterprise
applications running on top of our
Enterprise
it but in the future the application is
not Enterprise it it's agents and the
operating system is not something like
VMware it's something like Dynamo and
this operating system is running on top
of not a data center but on top of an AI
Factory now we call it Dynamo for a good
reason as you know the Dynamo was the
first instrument that started the last
Industrial Revolution the industrial
revolution of energy water comes in
electricity comes out it's pretty
fantastic you know water comes in you
light it on fire turn to the Steam and
it what comes out this this invisible
thing that's incredibly valuable it took
another 80 years to go to alternate new
current but Dynamo Dynamo is the where
it all started okay so we decided to
call this operating system this piece of
software insanely complicated software
the Nvidia Dynamo it's open source it's
open source and we're so happy that so
many of our partners are working with us
on it and one of one of my favorite
favorite Partners I just love them so
much because the Revolutionary work that
they do and also because Aran is such a
great guy but perplexity as a great
partner of ours in working through this
okay so anyhow uh really really
great okay so now we're going to have to
wait until we scale up all these
infrastructure but in the meantime we've
done a whole bunch of very indepth
simulation we have supercomputers doing
simulation of our supercomputers which
makes sense and and I'm now going to
show you the
benefit of everything that I've just
said and remember the factory diagram on
the x-axis on the xaxis is tokens per
second throughput excuse me in the y
axis tokens per second throughput of the
factory and the x-axis tokens per second
of the user experience and you want
super smart AIS and you want to produce
a whole bunch of it this is
Hopper okay so this is Hopper and it can
produce it can
produce for one user about for each user
about a 100 tokens per second 100 this
is eight gpus and it's connected with
infin band and the um um I'm normalizing
it to tokens per second per
megawatt so it's a one megawatt data
center which is not a very large AI
Factory but anyhow one megaw okay and so
it can produce for each user 100 tokens
per second and it can produce at this at
this level whatever that happens to be
100,000 tokens per second for that one
megawatt data center or it can produce
about 2 and half million tokens per
second 2 and a half million tokens per
second for that AI Factory if it was
super batched up and the customer is
willing to wait a very long time okay
does that make sense all right so
nod all right because this is this is
where you know every GTC there's there's
the price for entry you guys know and
it's like you get tortured with math
okay this is the only
only only at Nvidia do you get tortured
with math all right so Hopper you get
two and a half now what's that two and a
half million what's it what's how do you
translate that 2 and a half million
remember um Chad gbt is like $10 per
million
tokens right $10 per million tokens
Let's Pretend for for a second that that
that's I I I think the 10 million $10
per million tokens is probably down here
okay I I probably say it's down here but
let me pretend it's up there because 25
million um 10 so 25 million doll per
second does that make sense that's
that's how you think through it or on
the other hand if it's way down here
then the question is you know so it's
100,000 100,000 just divide that by 10
okay $250,000
per Factory per second and then as it
was 31 million 30 million seconds in a
year and that translates into revenues
for that 1 million that one megawatt
Data Center and so that's your goal on
the one hand you would like your your
token rate to be as fast as possible so
that you can make really smart AIS and
if you have Smart AIS people pay you
more money for it on the other hand the
smarter the AI the less you can make in
volume
very sensible
tradeoff and this is the curve we're
trying to bend now what I'm just showing
you right now is the fastest computer in
the world Hopper it's the computer that
revolutionized everything and so how do
we make that better so the first thing
that we do is we come up with Blackwell
with MV link
8 same same Blackwell that one same one
same compute and that one compute node
with MV link8 using fp8 and so black is
just
faster faster bigger more transistors
more everything but we like to do more
than that and so we introduce a new
Precision it's not quite as simple as
4bit floating point but using 4-bit
floating point we can quantize the model
use less
energy use less energy to do the same
and as a result when you use less energy
to do the same you could do more because
remember one big idea is that every
single data center in the future will be
power limited your revenues are power
limited you could figure out what your
revenues are going to be based on the
power you have to work
with this is no different than you know
like many other Industries and so we are
now a power limited industry our
revenues will associate with that well
based on that you want to make sure you
have the most energy efficient compute
architecture you can possibly get the
next
then we scale up with MV link 72 does
that make sense look at the difference
between that MV link 72 fp4 and then
because our architecture is so tightly
integrated and now we add Dynamo to it
Dynamo can
extend that even further are you
following me so Dynamo also helps Hopper
but Dynamo helps black wall incredibly
now yep
only at GTC do you get an Applause for
that and and
so so now notice what I put those two
shiny parts that's kind of where your
max Q
is you know that's likely where you'll
run your factory operations you're
trying to find that balance between
maximum throughput and maximum quality
of AI smartest AI the most of it those
two that XY intercept is really what
you're optimizing for and that's what it
looks like if you look underneath those
two squares Blackwell is way way better
than Hopper and remember this is not ISO
chips this is ISO
power this is Ultimate Mo's law this is
what Moors law was always about in the
past and now here we are
25x in one generation as isop
power there is not ISO chips it's not
ISO transistors it's not ISO anything
ISO power the ultimate the ultimate
limiter there's only so much energy we
can get into a Data Center and So within
ISO power black well is 25 times now
here's the that
rainbow that's incredible that's the fun
part look all the different config every
underneath the Paro the frontier Paro we
call it the frontier Paro under under
the Frontier paredo are millions of
points we could have configured the data
center to
do we could have paralized and split the
work and sharded the work in a whole lot
of different
ways and we found the most optimal
answer which is the Paro the frontier
Paro okay the Paro Frontier and each one
of them because of the color shows you
it's a different
configuration which is the reason why
this image says very very clearly you
want a programmable architecture that is
as homogeneously fungible as fungible as
possible because the workload changes so
dramatically across the entire Frontier
and look we got on the top expert
parallel 8 batch of 3000 disaggregation
off Dynamo off in the middle expert
parallel 64 with with uh
uh oh the P the 26%
of 26% is used for context so so Dynamo
is turned on 26% context the other 64%
is 74% is not batch of 64 and expert
parallel of 64 on one expert parallel
four on the other and then down here all
the way to the bottom you got you got
tensor parallel 16 with expert parallel
4 batch of two 1% context the
configuration of the computer is changed
ing across that entire spectrum and then
this is what happen so this is with
input sequence length this is a kind of
a commodity test case this this is a
test case that you can Benchmark
relatively easily um the input is 1,000
tokens the output is 2,000 notice
earlier we just showed you a demo where
the output is very simply 9,000 right
8,000 okay and so obviously this is not
representative of just that one chat now
this one is more representative
and this is what you know the goal is to
build these next Generation computers
for Next Generation workloads and so
here's an example of a reasoning model
and in a reasoning model Blackwell is 40
times 40 times the performance of Hopper
straight
up pretty
amazing you know I've said before
somebody actually asked you know why
would I say that but I said before that
when Blackwell starts shipping in volume
you couldn't give Hoppers
away and this is what I mean and this
makes sense if anybody if you're still
looking to buy a hopper don't be afraid
I'm it's okay
but I'm the chief re
Revenue
Destroyer my sales guys are going oh
no don't say that
there are circumstances where Hopper is
fine that's the best thing I could say
about Hopper there are circumstances
where you're
fine not
many if I have to take a
swing and so that's kind of my point um
when the technology is moving this fast
uh you you and because the workload is
so intense and you're building these
things they are factories you we really
we really like you to to um uh uh to
invest in the right right versions okay
just to put it in perspective this is
what 100 megawatt Factory looks like
this's 100 megawatt Factory you have
based on Hoppers you have 45,000 dies
1,400 racks and it produces 300 million
tokens per second okay and then this is
what it looks like with
Blackwell you have 8 yeah I know
[Applause]
that doesn't make any
sense okay so so we're not trying to
sell you less okay our sales guys are
going Jensen you're selling them less
this is better okay and so so anyways um
the more you buy the more you save
it's even better than that now the more
you buy the more you make you know and
so so anyhow uh remember everything is
in the context everything now in the
context of AI factories and and although
we talk about the chips you always start
from scale up we talk about the chips
but you always start from scale up the
full scale up what can you scale up to
the to the maximum um I want to show you
now what an Factory looks like but AI
factories are so complicated I just gave
you an example of one rack it has
600,000
Parts you know it's 3,000 lb now you've
got to take that and connect it with a
whole bunch of others and so we are
starting to build what we call the
digital twin of every data center before
you build a data center you have to
build a digital twin let's take a look
at this this is just incredibly
beautiful
the world is racing to build
state-of-the-art large scale AI
factories bringing up an AI gigafactory
is an extraordinary feat of engineering
requiring tens of thousands of workers
from suppliers Architects contractors
and Engineers to build ship and assemble
nearly 5 billion components and over
200,000 Mi of fiber nearly the distance
from the Earth to to the moon the Nvidia
Omniverse blueprint for AI Factory
digital twins enables us to design and
optimize these AI factories long before
physical construction Starts Here Nvidia
Engineers use the blueprint to plan a 1
gwatt AI Factory integrating 3D and
layout data of the latest Nvidia dgx
superpods an advanced power and cooling
systems from verv and Schneider
Electric and optimize topology from
Nvidia air a framework for simulating
Network logic layout and protocols this
work is traditionally done in silos the
Omniverse blueprint lets our engineering
teams work in parallel and
collaboratively letting us explore
various configurations to maximizing TCO
and power usage
Effectiveness Nvidia uses Cadence
reality digital twin accelerated by Cuda
and Omniverse libraries to simulate air
and liquid cooling
systems and Schneider Electric with EAP
an application to simulate power block
efficiency and
reliability realtime simulation lets us
iterate and run large scale whatif
scenarios in seconds versus hours we use
the digital twin to communicate
instructions to the large body of teams
and suppliers reducing execution errors
and accelerating time to bring up and
when planning for retrofits or upgrades
we can easily test and simulate cost and
down time ensuring a futureproof AI
Factory this is the first time anybody
who builds data oh that's so
beautiful all right I got a race here
because I'm turns out I got a lot to
tell you and and so if I if I go a
little too fast it's not because I don't
care about you it's just I got a lot of
information to go through all right so
so uh first uh our road map uh we're at
we're now in full production of blackw
uh computer companies all over the world
are ramping these incredible machines at
scale and uh
uh I'm just so so uh pleased and and so
grateful that all of you worked hard on
uh transitioning into this new
architecture and now uh in the second
half of this year we will uh easily
transition into the upgrade so we have
the Blackwell Ultra mbink 72 uh you know
it's a one and a half times more flabs
it's you know it's got a new instruction
for attention it's one and a half times
more memory all that memory is useful
for uh things like KV cache it's you
know two times more bandwidth okay for
networking bandwidth and so you're going
to now that we have the same
architecture we'll just kind of
gracefully uh glide into that and that's
called Blackwell Ultra okay so that's
coming second half of this year now
there's a reason why we we uh this is
the only product announcement in any
company where everybody's going yeah
next and in fact that's exactly the
response I was hoping to get and and
here's
why look we're building AI factories and
AI infrastructure it's going to take
years of planning this isn't this isn't
like buying a laptop
you know this isn't a this isn't
discretionary spend this is spend that
we have to go plan on and so we have to
plan on having of course the land and
the power and and we have to get get our
our capex ready and we get engineering
teams and and we have to lay it out a
couple two three years in advance which
is the reason why I show you our road
map a couple two three years in advance
so that you we don't surprise you in May
you know hi you know in another month
we're going to go to this incredible new
system I'll show you an example in a
second and so we planned this out in
multiple years the next the next click
one year
out is named after an astronomer and her
uh her grandkids are here her name is
Vera Ruben she discovered Dark Matter
okay it's
yep Vera Vera ruin is incredible because
the CPU is new it's twice the
performance of great had more memory
more bandwidth and yet just a little
tiny 50 wat CPUs is really quite
incredible okay and Reuben brand new
GPU CX9 brand new networking smart Nick
MV link 6 brand new MV link brand new
memories hbm 4 basically everything is
brand new except for the chassis and
this way we could take a whole lot of
risk in One Direction
and not risk a whole bunch of other
things related to the infrastructure and
so Vera ruin mvlink 144 is the second
half of next year now one one of the
things that I made a mistake on and so I
just need you to make this pivot we're
going to do this one
time Blackwell is really two gpus in one
Blackwell chip we call that one chip a
GPU and that was wrong and the reason
for that is it it screws up all the MV
link nomenclature and things like that
so going forward without going back to
Blackwell to fix it going forward when I
say mvlink 144 it just means that it's
connected to 144 gpus and each one of
those gpus is a GPU die and it could be
assembled in some package how it's
assembled could change from time to time
okay and so each GPU dies the GPU each
MV link is connected to the to uh to the
GPU and so very Ruben link 144 and then
this now sets the
stage for the second half of the year
the following year we call Reuben
Ultra okay so ver Ruben
Ultra I
know this one that's where you should
you
go all right so so this is ver Ruben
Reuben Ultra second half of 27 it's MV
link 5 76 extreme scale
up each rack is 600
KW 25 million
Parts okay and obviously a whole lot of
gpus and uh everything is X factored
more so 14 times more uh more flops 15
exif flops instead of one xof flop as
you me as I mentioned earlier is now 15
exop flops scaled up exop flops okay and
and it's
300 what 4.6 pedy so
4,600 terabytes per second scale up
bandwidth I don't mean aggregate I mean
scale up bandwidth and of course lots a
brand new MV link switch and CX9 okay
and so notice um uh 16 sites four gpus
in one
package extremely large MV link I just
put that in perspective this is what it
looks like okay now this is this is this
is this is going to be fun so this you
are just literally ramping up Grace
black wall at the moment and I I don't
mean to make it look like a laptop but
here we go okay so this is what Grace
black wall looks like and this is what
Reuben looks
like ISO ISO Dimension and so this is
another way of saying before you scale
out you have to scale up does that make
sense before you scale up scale out you
scale up and then after that you scale
scale out with amazing technology that
I'll show you in just a second all right
so first you scale up and then now that
gives you a sense of the pace at which
we're moving this is the amount of scale
up flops this is scale up
flops Hopper is 1x blackw is 68 x Reuben
is 900x scale up flops and then if I
turn it into essentially your TCO which
is power on top power per and the
underneath is the is the area underneath
the curve that I was talking to you
about the square underneath the curve
which is basically flops times bandwidth
okay so the the way you think about a
very easy gut feel gut check on whether
your AI factories are making progress
is Watts divided by those numbers and
you can see that Reuben it's going to
drop the cost down tremendously okay so
that's very quickly nvidia's road
map once a year once a year like like
clock ticks once a year okay how do we
scale up well we introduced we were pre
preparing to scale out that was scale up
as MV link our scale out network is
infiniband and Spectrum X
most were quite surprised that we came
into the ethernet world and the reason
why we decided to do ethernet is if we
could help ethernet
become like infiniband have the
qualities of infiniband then the network
itself would be a lot easier for
everybody to use and manage and so we uh
decided to invest in Spectrum we call it
Spectrum X and we brought to it the
properties of of uh congestion control
and and um uh very low latency and uh
and amount of software that's part of
our Computing Fabric and as a result we
made spectrx incredibly high performance
uh we scaled up the largest single GPU
cluster ever as one giant cluster with
Spectrum X right and that was Colossus
and so there are many other examples of
it spectrx is is unquestionably a huge
home run for us one of the areas that
I'm very excited about is Spectrum X is
not just for AI clouds but Spectrum X
also makes it possible for us to help
help every Enterprise become an AI
company and so uh was it last week or
the week before uh Chuck rubbins and
Cisco and Nvidia announced a partnership
for Cisco the world's largest Enterprise
networking company to take Spectrum X
and integrated into their product line
so that they could help the world's
Enterprises become AI
companies we're at 100,000
um with cx8 CX7 now cx8 is coming cx-9's
coming and during Ruben's time frame we
would like to scale out the number of
gpus to many hundreds of
thousands now the challenge with scaling
out gpus to many hundreds of thousands
is the
connection of the scale
out on the connection on scale up is
copper we should use copper as far as we
can and that's you know call it a meter
or two and that's incredibly good
connectivity very low very high
reliability very good Energy Efficiency
very low cost and so we use copper as
much as we can on scale up but on scale
out where the data centers are now the
size of the stadium we're going to need
something U much uh longdistance running
and this is where silicon photonics
comes in the challenge of silicon
photonics has been that the transceivers
consume a lot of energy to go from
electrical
to photonic has to go through a CIS go
through a transceiver and a ctis a
several CIS okay so first of all we're
announcing nvidia's first co- packaged
option silicon photonic system it is the
world's first
1.6 terabit per second CPO it is based
on a technology called micro ring
resonator modulator it is completely
built with this incred aible process
technology at tsmc that we've been
working with for some time and and we
partnered with just a giant ecosystem of
Technology providers to invent what I'm
about to show you this is really crazy
technology crazy crazy technology now
the reason why we decided to invest in
mrm is so that we could prepare
ourselves using mrm's incredible density
and power better density and power
compared to moander which is used for
telecommunications when you when you um
uh drive from one data center to another
data center uh in telecommunications or
even in the transceivers that we use we
use Mo Xander because the density
requirement is not very high until now
and so if you look at look at um these
transceivers this is an example of a
transceiver they did a very good job
tangling this up for me
oh
wow thank
[Music]
you oh Mother of
God okay this is where you got to turn
reasoning
on it's not as easy as you think these
are squirly little things all right so
this this one right here this is 30
Watts just so keep you remember this 30
watts and and if you get it on if you
buy in high volume it's
$1,000 this is a plug on this side on
this side it's electrical on this side
is is Optical okay so Optics come in
through the the yellow you plug this
into a switch it's electrical on this
side there's uh transceivers lasers um
uh and a tech technology called moander
and uh uh incredible and so we use this
to go from the GPU to the
switch to the next
switch and then the next switch down and
then next switch down to the GPU for
example and so each one of these if we
had 100,000
gpus we would have
100,000 of this side and then another
you know 100,000 which connects the the
switch to the switch and then on the
other side I attribute that to the other
to the other Nick if we had
250,000 we'll add another layer of
switches and so each GPU every GPU
250,000 every GPU would have six
transceivers every GPU would have six of
these plugs and these six plugs would
add 180 watts per
GPU 180 watts per GPU and $6,000 per GPU
okay and so the question is how do we
scale up now to millions of
gpus because if we had a million gpus
multiply by
six right it would be uh million 6
million
transceivers times 30
Watts 180 megaw
of transceivers they didn't do any math
they just move signals around and and so
the question is how do we how could we
afford and as I mentioned earlier energy
is our most important commodity
everything is related ultimate to energy
so this is going to limit our revenues
the our customers
revenues by subtracting out 180 megaw of
power and so this is the this is the
amazing thing that we did we invented
the world's first
mrm micro mirror and this is what it
looks like there's a little uh wave
guide you see that on that wave guide
goes to a ring that ring resonates and
it controls the amount of reflectivity
of the wave guide as it goes around and
limits and modulates the uh energy that
the amount of light that goes through
and it shuts It Off by absorbing it or
pass it on okay turns the light this
direct continuous laser beam into ones
and zeros and that's the miracle and
that technology is then uh the photonic
IC is stacked with the electronic IC
which is then stacked with a whole bunch
of micro lenses which is stacked with
this thing called fiber array these
things are all manufactured using this
technology at tsmc called they call it
Coupe and um package using a 3D Coos
technology working with all of these
technology providers a whole bunch of
them the names I just showed you earlier
and it turns it into to this incredible
machine so let's take a look at the
video of it
[Applause]
[Music]
[Music]
[Music]
[Music]
just a technology
Marvel and they turn into these switches
our infin band switch the Silicon is is
working fantastically second half of
this year we will ship the the Silicon
platonic switch in the second half of
this year and the second half of next
year we'll ship the Spectrum X because
of the mrm choice because of the ible
technology risks that over the last 5
years that we did and filed hundreds of
patents and we've licensed it to our
partners so that we can all build them
now we're in a position to put silicon
photonics with co- package
options no
transceivers fiber direct fiber in into
our switches with a Radix of 512 this is
the this is the 512 ports this would
just simply not possible any other way
and so this is this now set our set us
up to be able to scale up to these multi
100,000 gpus and multi-million gpus and
the benefit just so you you you you
imagine this it's incredible in a data
center we could we could save tens of
megawatts tens of megawatts let's say 10
megawatt well let's let's say 60
megawatt 60 well 6 megawatts is 10
Reuben Ultra
racks 6 megaw is 10 Reuben Ultra
racks right and 60 that's a lot 100
Reuben Ultra racks of power that we can
now deploy into Rubin all right so this
is our road map once a year once a year
an architecture every every uh two years
a new product line every single year X
factors up and we try to take silicon
risk or networking risk or system
chassis risk um in in pieces so that we
can move the industry forward as we
pursue these incredible technology uh
Vera Rubin and uh I really appreciate
the the uh the grandkids for being here
uh this is our opportunity to recognize
her and and to honor her for the
incredible work that she did our next
generation will be named after Fineman
okay nvidia's road map let me talk to
you about
Enterprise Computing this is really
important in order for us to bring AI to
the world's
Enterprise first we have to go to a
different part of
Nvidia the beauty of gaan
Splats okay in order in order for us to
take AI to Enterprise take a step back
for a second and remind yourself this
remember Ai and machine learning has
reinvented the entire Computing stack
the processor is different the operating
system is different the applications on
top are different the way the
applications are different the way you
orchestrate it are different and the way
you run them are different let me give
you one
example um the way you access data will
be fundamentally different than the past
instead of retrieving precisely the data
that you want and you read it to try to
understand it in the future we will do
what we do with perplexity instead of
doing doing retrieval that way I'll just
ask perplexity what I want ask it a
question and it will tell you the answer
this is the way Enterprise it will work
in the future as well we'll have ai
agents which are part of our digital
Workforce there's a billion knowledge
workers in the world they're probably
going to be 10 billion digital workers
working with us side by side 100 % of
software engineers in the future there
are 30 million of them around the world
100% of them are going to be AI assisted
I'm certain of that 100% of Nvidia
software Engineers will be AI assisted
by the end of this year and so AI agents
will be everywhere how they run what the
what Enterprises run and how we run it
will be fundamentally different and so
we need a new line of
computers and this is what started it
all this is the Nvidia
djx1 20 CPU cores
128 gigb of GPU
memory one pedop flops of
computation
$150,000
3,500
Watts let me now introduce you to the
new dgx
this is nvidia's new
dgx
and we call it djx
spark djx
spark now you'll be
surprised 20 CPU cores
we partnered with mediatech to build
this for us they did a fantastic job
it's been a great joy working with Ricki
and the mediate Tech Team I really
appreciate their their partnership built
us a chipto chip MV link CPU to GPU and
now the GPU has 128
gbt and this is fun one pedop
flops so this this is this is like the
original
djx1 with pin
particles you would have thought that
that's a joke that would land at
GTC okay well here's 30 million there
are 30 million uh software engineers in
the world and you know tens 10 20
million data scientists and this is this
is now this is clearly the gear of
choice thank you
Jan look at this in every bag this is
what you should
find
right this is this is the development
platform of every software engineer in
the world if you
have a family member spouse somebody you
care
about who who uh who's a software
engineer or AI researcher or or you know
just data scientist and you would like
to give them you know what the perfect
Christmas
present tell me tell me this isn't what
they want huh and so ladies and
gentlemen today uh we'll let you res we
will ship we will Reserve we will
reserve the first djx Sparks for the
attendees of GTC so go reserve yours
you already have one of these so now you
just got to get one of
these all right the next so that's thank
you Janine the next one is also a brand
new computer one that the world's never
had before so we're we're announcing a
whole new line of computers this is a
new personal computer new personal
workstation I know it's crazy check this
out Grace Blackwell
liquid
[Music]
cooled this is what a PC should look
like 20 pedop
flops unbelievable 72 CPU
cores chipto chip interface hbm memory
and just just in case some PCI Express
slots for your uh gForce
okay so so this is called djx station
djx spark and djx station are going to
be available by all of the OEM HP Dell
Lenovo assus uh it's going to be
manufactured uh for data scientists and
researchers all over the world this is
the computer of the age of
AI this is what computers should look
like and this is what computers will run
in the future and we have a whole lineup
for Enterprise now from little tiny one
to to workstation ones the server ones
to uh supercomputer ones and these will
be available uh by all of our partners
we will
also revolutionize the rest of the
Computing stack remember Computing has
three pillars there's Computing you're
looking at it there's networking as I
mentioned earlier Spectrum X going to
the world's
Enterprise an AI Network and the third
is Storage storage has to be completely
reinvented
rather than a retrieval
based storage system is going to be a
semantics based retrieval system a
semantics spased storage system and so
the storage system has to be
continuously embedding information in
the background taking raw data embedding
it into knowledge and then later when
you access it you don't retrieve it you
just talk to it you ask it questions you
give it problems and one of the one of
the examples I wish we had a video of it
um but Aaron at box even put one up in
the cloud worked with us to put it up in
the cloud and it's basically you know a
super smart storage system and in the
future you're going to have something
like that in every single Enterprise
that is the Enterprise storage of the
future and we're working with the entire
storage industry really fantastic
Partners uh ddn and Dell and HP
Enterprise and Hitachi and IBM and net
app and neutronics and Pure Storage and
vast and W basically the the entire
world storage industry will be offering
this this stack for the very first time
your storage system will be GPU
accelerated and so somebody thought I
was I didn't have enough
slides and so Michael thought I didn't
have enough slides so he he said Jensen
just in case you don't have enough
slides can I just put this in there and
so this is Michael slides but but this
is this he sent this to me he goes just
in case you don't have any slides and I
I got too many slides but this is such a
great slide and and let me tell you why
in one single slide he's explaining that
Dell is going to be offering a whole
line of
Nvidia Enterprise it AI infrastructure
systems and and all the software that
runs on top of it okay so you can see
that we're in the process of
revolutionizing the world's Enterprise
we're also announcing today this
incredible model that everybody can run
and so I showed you earlier R1 a
reasoning model I showed you versus
llama 3 a non- reasoning model
and obviously R1 is much smarter um but
we can do it even better than that and
we can make it possible to be Enterprise
ready for any company and it's now
completely open source it's part of our
system we call Nims and you can download
it you can run it anywhere you can run
it on djx spark you you can run it on
dgx station you can run on any of the
servers that the the oems make you can
run it in the cloud you can integrate
into any of your agentic AI Frameworks
and we're working with companies all
over the world and I'm going to flip
through these so watch very carefully
I've got some great Partners in the
audience want to recognize Accenture
Julie SED and her team are building
their AI Factory and their AI framework
uh AMD dos the world's largest
telecommunication software company uh
AT&T John Stanky and his team uh
building an AT&T AI system agentic
system Larry think and uh Black Rock
team building theirs uh Annie Roode uh
in the future not only will we hire ASC
designers we're going to hire a whole
bunch of digital ASC designers from
anude Cadence that will help us design
our chips and so Cadence is building
their uh AI framework and as you can see
in every single one of them there Nvidia
models Nvidia Nims and viia libraries
integrated throughout so that you can
run it on Prem in the cloud any Cloud uh
Capital One one of the most advanced
financial services companies and using
technology has Nvidia all over it uh
deoe Jason and his team Ian y Janet and
his team NASDAQ and Adena and her team
uh integrating Nvidia technology into
their AI Frameworks and then Chris Jen
and his team at sap uh bill mcder and
his team at service
now that was pretty good huh
first this is one of those Keynotes
where the first slide took 30 minutes
and then all the other slide took 30
minutes all right so so next let's go
somewhere else let's go talk about
robotics shall
[Music]
we let's talk about
robots well the time has come the time
have has come for
robots uh robots have the benefit the
benefit of being able to interact with
the physical world and do things that
otherwise digital information cannot we
know very clearly that the world is has
severe shortage of of human labors human
workers by the end of this decade the
world is going to be at least 50 million
workers short we'd be more than
delighted to pay them each $50,000 to
come to work we're probably going to
have to pay robots $50,000 a year to
come to work and so this is going to be
a very very large industry there are all
kinds of robotic systems your
infrastructure will be robotic billions
of cameras and warehouses and factories
10 20 million factories around the world
every car is already a robot as I
mentioned earlier and then now we're
building General robots let me show you
how we're doing
[Music]
that everything that moves will be
autonomous physical AI will embody
robots of every kind in every industry
three computers built by Nvidia enable a
continuous loop of robot AI simulation
training testing and Real World
Experience training robots requires huge
volumes of data Internet scale data
provides common sense and reasoning but
robots need action and control data
which is expensive to
capture with blueprints built on Nvidia
Omniverse and Cosmos developers can
generate massive amounts of diverse
synthetic data for training robot
policies first in Omniverse developers
aggregate real world sensor or
demonstration data according to their
different domains robots and tasks then
use Omniverse to condition Cosmos
multiplying the original captures into
large volumes of photoreal diverse
data developers use Isaac lab to
post-rain the robot policy IES with the
augmented data set and let the robots
learn new skills by cloning behaviors
through imitation learning or through
trial and error with reinforcement
learning AI
feedback practicing in a lab is
different than the real
world new policies need to be field
tested developers use Omniverse for
software and Hardware in the loop
testing simulating the policies in a
digital twin with real world
environmental Dynamics with domain
randomization physics feedback and High
Fidelity sensor
simulation real world operations require
multiple robots to work together Mega
and Omniverse blueprint lets developers
test fleets of post-train policies at
scale here foxc contests heterogeneous
robots in a virtual Nvidia Blackwell
production facility
as the robot brains execute their
missions they perceive the results of
their actions through sensor simulation
then plan their next action Mega lets
developers test many robot policies
enabling the robots to work as a system
whether for spatial reasoning navigation
Mobility or
dexterity amazing things are born in
simulation today we're introducing
Nvidia Isaac Groot
N1 Groot N1 is a generalist Foundation
model for humanoid
robots it's built on the foundations of
synthetic data generation and learning
in
simulation Groot N1 features a dual
system architecture for thinking fast
and slow inspired by principles of human
cognitive
processing the slow thinking system lets
the robot perceive and reason about its
environment and instructions and plan
the right actions to take the fast
thinking system translates the plan into
precise and continuous robot actions
Groot n1's generalization lets robots
manipulate common objects with ease and
execute multi-step sequences
collaboratively and with this entire
pipeline of synthetic data generation
and robot learning humanoid robot
developers can post-train Gro N1 across
multiple embodiments and tasks across
many
environments around the world in every
industry developers are using nvidia's 3
computers to build the next generation
of embodied AI
[Music]
physical Ai and
Robotics are moving so fast everybody
pay attention to this space this could
very well likely be the largest industry
of all at its core we have the same
challenges as I mentioned before there
are three that we focus on they are
rather systematic
one how do you solve the data
problem how where do you create the data
necessary to train the AI two what's the
model architecture and then
three what's the scaling loss how can we
scale either the data the compute or
both so that we can make AIS smarter and
smarter and smarter how do we scale and
those two those fundamental problems
exist in robotics as well in
robotics we created a system called
Omniverse it's our operating system for
physical a you've heard me talk about
Omniverse for a long time we added two
technologies to it today I'm going to
show you two things one of them is so
that we could scale AI with generative
capabilities and generative model that
understand the physical world we call it
Cosmos using
Omniverse to condition Cosmos and using
Cosmos to generate an infinite number of
environments
allows us to create data that is
grounded grounded controlled by
us and yet be systematically infinite at
the same time okay so you see Omniverse
we use candy colors to give you an
example of us controlling the robot in
the scenario perfectly and yet o Cosmos
can create all these virtual
environments the second thing just as as
we were talking about earlier one of the
incredible scaling capabilities of
language models today is reinforcement
learning verifiable rewards the question
is what's the verifiable rewards in
robotics and as we know very well is the
laws of physics verifiable physics
rewards and so we need an incredible
physics engine well most physics engines
have been designed for a variety of
reasons they could be designed because
we wanted to use it for large
machineries or uh maybe we design it for
uh virtual worlds video games and such
but we need a physics engine that is
designed for very fine
grain rigid and soft bodies designed for
being able to train tactile feedback and
fine motor skills and actuator controls
we needed to be GPU accelerated so that
we these virtual worlds could live in
super linear time super real time and
train these AI models incredibly fast
and we needed to be integrated
harmoniously into a framework that is
used by roboticist all over the world
Moko and so today we're announcing
something really really special it is a
partnership of three
companies Deep
Mind Disney research and Nvidia and we
call it
Newton let's let's take a look at Newton
[Music]
tell me that wasn't amazing
hey
blue how you
doing how do you like how do you like
your new physics engine you like it
huh yeah I bet I know tactile
feedback rigid body soft body
simulation super real
time can you imagine just now what you
were looking at is complete real time
simulation this is how we're going to
train robots in a future
uh just so you know blue has uh two
computers two Nvidia computers
inside look how smart you
are yes you're
smart
okay all right hey blue listen how about
let's take them home let's finish this
keynote it's
lunchtime are you ready let's finish it
up we have another announcement to
you're good you're good just stand right
here stand right here stand right
here all right good right
there that's good all right
Stan okay we have another amazing
news I told you the progress of our
robotics has been making
progress and today we're announcing that
Groot
N1 is open
sourced I want to thank all of you to
come let's wrap up I want to thank all
of you for coming to GTC we talked about
several things one Blackwell is in full
production and the ramp is incredible
customer demand is incredible and for
good reason because there's an
inflection point in AI the amount of
computation we have to do in AI is so
much greater as a result of reasoning Ai
and the training of reasoning AI systems
and agent agentic Systems Second
Blackwell mvlink 72 with Dynamo is 40
times the performance AI Factory
performance of Hopper and inference is
going to be one of the most important
workloads in the next decade as we scale
out AI third we have an
annual annual rhythm of road maps that
has been laid out for you so that you
could plan your AI infrastructure and
then we have two we have three AI
infrastructures we're building AI
infrastructure for the cloud AI
infrastructure for Enterprise and AI
infrastructure for
[Music]
robots we have one more treat for
you play it
[Music]
[Music]
[Music]
[Music]
a
[Music]
[Music]
[Music]
[Music]
[Music]
thank you everybody thank you for all
the partners that made this video
possible thank you everybody that made
this video possible have a great GTC
thank
you hey
blue let's go home good
job good little
[Music]
man thank you I love you too thank you

At some point, you have to believe something.
We've reinvented computing as we know it. What
is the vision for what you see coming next? We
asked ourselves, if it can do this, how far can
it go? How do we get from the robots that
we have now to the future world that you
see? Cleo, everything that moves will be
robotic someday and it will be soon. We
invested tens of billions of dollars before
it really happened. No that's very good, you
did some research! But the big breakthrough
I would say is when we...
That's Jensen Huang, and whether you know it or not
his decisions areshaping your future. He's the CEO of
NVIDIA, the company that skyrocketed over the past few
yearsto become one of the most valuable companies in
the world because they led a fundamental shift
in how computers work unleashing this current
explosion of what's possible with technology.
"NVIDIA's done it again!" We found ourselves being
one of the most important technology companies in
the world and potentially ever. A huge amount of
the most futuristic tech that you're hearing about 
in AI and robotics and gaming and self-driving
cars and breakthrough medical research relies on
new chips and software designed by him and his
company. During the dozens of background interviews
that I did to prepare for this what struck me most
was how much Jensen Huang has already influenced
all of our lives over the last 30 years, and how
many said it's just the beginning of something
even bigger. We all need to know what he's building
and why and most importantly what he's trying
to build next. Welcome to Huge Conversations...
Thank you so much for doing this. I'm so happy to do
it. Before we dive in, I wanted to tell you
how this interview is going to be a little bit
different than other interviews I've seen you
do recently. Okay! I'm not going to ask you any
questions about - you could ask - company finances,
thank you! I'm not going to ask you questions
about your management style or why you don't
like one-on ones. I'm not going to ask you
about regulations or politics. I think all
of those things are important but I think that our
audience can get them well covered elsewhere. Okay.
What we do on huge if true is we make optimistic
explainer videos and we've covered - I'm the worst
person to be an explainer video. I think you
might be the best and I think that's what I'm
really hoping that we can do together is make a
joint explainer video about how can we actually
use technology to make the future better. Yeah. And
we do it because we believe that when people see
those better futures, they help build them. So
the people that you're going to be talking to
are awesome. They are optimists who want to
build those better futures but because we
cover so many different topics, we've covered
supersonic planes and quantum computers and
particle colliders, it means that millions
of people come into every episode without
any prior knowledge whatsoever. You might be
talking to an expert in their field who doesn't
know the difference between a CPU and a GPU or a
12-year-old who might grow up one day to be you
but is just starting to learn. For my part,
I've now been preparing for this interview for
several months, including doing background
conversations with many members of your team
but I'm not an engineer. So my goal is to help that
audience see the future that you see so I'm going
to ask about three areas: The first is, how did we
get here? What were the key insights that led to
this big fundamental shift in computing that we're
in now? The second is, what's actually happening
right now? How did those insights lead to the world
that we're now living in that seems like so much
is going on all at once? And the third is, what is
the vision for what you see coming next? In order
to talk about this big moment we're in with AI
I think we need to go back to video games in the
'90s. At the time I know game developers wanted
to create more realistic looking graphics but
the hardware couldn't keep up with all of that
necessary math. NVIDIA came up with
a solution that would change not just games
but computing itself. Could you take us back
there and explain what was happening and what
were the insights that led you and the NVIDIA
team to create the first modern GPU? So in the
early '90s when we first started the company
we observed that in a software program inside
it there are just a few lines of code, maybe
10% of the code, does 99% % of the processing
and that 99% of the processing could be done
in parallel. However the other 90% of the code
has to be done sequentially. It turns out that
the proper computer the perfect computer is one
that could do sequential processing and parallel
processing not just one or the other. That was the
big observation and we set out to build a company
to solve computer problems that normal computers
can't. And that's really the beginning of NVIDIA.
My favorite visual of why a CPU versus a
GPU really matters so much is a 15-year-old
video on the NVIDIA YouTube channel where the
Mythbusters, they use a little robot shooting
paintballs one by one to show solving problems
one at a time or sequential processing on a
CPU, but then they roll out this huge robot
that shoots all of the paintballs at once
doing smaller problems all at the same
time or parallel processing on a GPU.
"3... 2... 1..." So Nvidia unlocks all of this new power
forvideo games. Why gaming first? The video games
requires parallel processing for processing
3D graphics and we chose video games because,
one, we loved the application, it's a simulation
of virtual worlds and who doesn't want to go to
virtual worlds and we had the good observation
that video games has potential to be the largest
market for for entertainment ever. And it turned
out to be true. And having it being a large market
is important because the technology is complicated
and if we had a large market, our R&D budget could
be large, we could create new technology. And that
flywheel between technology and market and greater
technology was really the flywheel that
got NVIDIA to become one of the most important
technology companies in the world. It was all
because of video games. I've heard you say that
GPUs were a time machine? Yeah. Could you tell me
more about what you meant by that? A GPU is like a
time machine because it lets you see the future
sooner. One of the most amazing things anybody's
ever said to me was a quantum chemistry
scientist. He said, Jensen, because of NVIDIA's work,
I can do my life's work in my lifetime. That's time
travel. He was able to do something that was beyond
his lifetime within his lifetime and this is
because we make applications run so much faster
and you get to see the future. And so when you're
doing weather prediction for example, you're seeing
the future when you're doing a simulation
a virtual city with virtual traffic and we're
simulating our self-driving car through
that virtual city, we're doing time travel. So
parallel processing takes off in gaming and it's
allowing us to create worlds in computers that
we never could have before and and gaming is sort
of this this first incredible cas Cas of parallel
processing unlocking a lot more power and then
as you said people begin to use that power across
many different industries. The case of the of the
quantum chemistry researcher, when I've heard you
tell that story it's that he was running molecular
simulations in a way where it was much faster to
run in parallel on NVIDIA GPUs even then than it
was to run them on the supercomputer with the CPU
that he had been using before. Yeah that's true. So
oh my god it's revolutionizing all of these other
industries as well, it's beginning to change
how we see what's possible with computers and my
understanding is that in the early 2000s you
see this and you realize that actually doing
that is a little bit difficult because what that
researcher had to do is he had to sort of trick
the GPUs into thinking that his problem was a
graphics problem. That's exactly right, no that's
very good, you did some research. So you create
a way to make that a lot easier. That's right
Specifically it's a platform called CUDA which
lets programmers tell the GPU what to do using
programming languages that they already know like
C and that's a big deal because it gives way more
people easier access to all of this computing
power. Could you explain what the vision was that
led you to create CUDA? Partly researchers
discovering it, partly internal inspiration and
and partly solving a problem. And you know a
lot of interesting interesting ideas come out
of that soup. You know some of it is aspiration
and inspiration, some of it is just desperation you
know. And so in the case of CUDA is very
much this the same way and probably the first
external ideas of using our GPUs for parallel
processing emerged out of some interesting work
in medical imaging a couple of researchers
at Mass General were using it to do CT
reconstruction. They were using our graphics
processors for that reason and it inspired us.
Meanwhile the problem that we're trying to solve
inside our company has to do with the fact that
when you're trying to create these virtual worlds
for video games, you would like it to be beautiful
but also dynamic. Water should flow like water and
explosions should be like explosions. So there's
particle physics you want to do, fluid dynamics
you want to do and that is much harder to do if
your pipeline is only able to do computer graphics.
And so we have a natural reason to want to do it
in the market that we were serving. So
researchers were also horsing around with using
our GPUs for general purpose uh acceleration and
and so there there are multiple multiple factors
that were coming together in that soup, we
just when the time came and we decided
to do something proper and created a CUDA as
a result of that. Fundamentally the reason why
I was certain that CUDA was going to be successful
and we put the whole company behind it was
because fundamentally our GPU was going to be
the highest volume parallel processors built in
the world because the market of video games was so
large and so this architecture has a good chance
of reaching many people. It has seemed to me like
creating CUDA was this incredibly optimistic "huge
if true" thing to do where you were saying, if we
create a way for many more people to use much
more computing power, they might create incredible
things. And then of course it came true. They did.
In 2012, a group of three researchers submits an
entry to a famous competition where the goal is
to create computer systems that could recognize
images and label them with categories. And their
entry just crushes the competition. It gets way
fewer answers wrong. It was incredible. It blows
everyone away. It's called AlexNet, and it's a kind
of AI called the neural network. My understanding
is one reason it was so good is that they used
a huge amount of data to train that system
and they did it on NVIDIA GPUs. All of a sudden,
GPUs weren't just a way to make computers faster
and more efficient they're becoming the engines
of a whole new way of computing. We're moving from
instructing computers with step-by-step directions
to training computers to learn by showing them a
huge number of examples. This moment in 2012 really
kicked off this truly seismic shift that we're
all seeing with AI right now. Could you describe
what that moment was like from your perspective
and what did you see it would mean for all of
our futures? When you create something new like
CUDA, if you build it, they might not come. And
that's always the cynic's perspective
however the optimist's perspective would say, but
if you don't build it, they can't come. And that's
usually how we look at the world. You know we
have to reason about intuitively why this would
be very useful. And in fac, in 2012 Ilya Sutskever,
and Alex Krizhevsky and Geoff Hinton in the University
of Toronto the lab that they were at they reached
out to a gForce GTX 580 because they learned about
CUDA and that CUDA might be able to to be used as
a parallel processor for training AlexNet and
so our inspiration that GeForce could be the the
vehicle to bring out this parallel architecture
into the world and that researchers would somehow
find it someday was a good was a good strategy. It
was a strategy based on hope, but it was also
reasoned hope. The thing that really caught
our attention was simultaneously we were trying
to solve the computer vision problem inside the
company and we were trying to get CUDA to
be a good computer vision processor and we
were frustrated by a whole bunch of early
developments internally with respect to our
computer vision effort and getting CUDA to be
able to do it. And all of a sudden we saw AlexNet,
this new algorithm that is completely
different than computer vision algorithms before
it, take a giant leap in terms of capability
for computer vision. And when we saw that it was
partly out of interest but partly because we were
struggling with something ourselves. And so we were
we were highly interested to want to see it work.
And so when we when we looked at AlexNet we were
inspired by that. But the big breakthrough I
would say is when we when we saw AlexNet, we
asked ourselves you know, how far can AlexNet
go? If it can do this with computer vision, how
far can it go? And if it if it could go to the
limits of what we think it could go, the type
of problems it could solve, what would it mean for
the computer industry? And what would it mean for
the computer architecture? And we were,
we rightfully reasoned that if machine learning,
if the deep learning architecture can scale,
the vast majority of machine learning problems
could be represented with deep neural networks. And
the type of problems we could solve with machine
learning is so vast that it has the potential of
reshaping the computer industry altogether,
which prompted us to re-engineer the entire
computing stack which is where DGX came from
and this little baby DGX sitting here, all of
this came from from that observation that we ought
to reinvent the entire computing stack layer by
layer by layer. You know computers, after 65 years
since IBM System 360 introduced modern general
purpose computing, we've reinvented computing as we
know it. To think about this as a whole story, so
parallel processing reinvents modern gaming and
revolutionizes an entire industry then that way
of computing that parallel processing begins to
be used across different industries. You invest
in that by building CUDA and then CUDA and the
use of GPUs allows for a a step change in neural
networks and machine learning and begins a sort
of revolution that we're now seeing only
increase in importance today... All of a sudden
computer vision is solved. All of a sudden speech
recognition is solved. All of a sudden language
understanding is solved. These incredible
problems associated with intelligence one
by one by one by one where we had no solutions
for in past, desperate desire to have solutions
for, all of a sudden one after another get solved
you know every couple of years. It's incredible.
Yeah so you're seeing that, in 2012 you're
looking ahead and believing that that's
the future that you're going to be living in now,
and you're making bets that get you there, really
big bets that have very high stakes. And then my
perception as a lay person is that it takes a
pretty long time to get there. You make these bets -
8 years, 10 years - so my question is:
If AlexNet that happened in 2012 and this audience
is probably seeing and hearing so much more about
AI and NVIDIA specifically 10 years later,
why did it take a decade and also because you
had placed those bets, what did the middle
of that decade feel like for you? Wow that's
a good question. It probably felt like today. You
know to me, there's always some problem and
then there's some reason to be to be
impatient. There's always some reason to be
happy about where you are and there's always
many reasons to carry on. And so I think as I
was reflecting a second ago, that sounds like this
morning! So but I would say that in all things that
we pursue, first you have to have core beliefs. You
have to reason from your best principles
and ideally you're reasoning from it from principles
of either physics or deep understanding of
the industry or deep understanding of the
science, wherever you're reasoning from, you
reason from first principles. And at some point you
have to believe something. And if those principles
don't change and the assumptions don't change,
then you, there's no reason to change your
core beliefs. And then along the way there's always
some evidence of you know of success and
and that you're leading in the right
direction and sometimes you know you go a
long time without evidence of success and you
might have to course correct a little but
the evidence comes. And if you feel like you're
going in the right direction, we just keep on going.
The question of why did we stay so committed for
so long, the answer is actually the opposite: There
was no reason to not be committed because we are,
we believed it. And I've believed in NVIDIA
for 30 plus years and I'm still here working
every single day. There's no fundamental
reason for me to change my belief system and
I fundamentally believe that the
work we're doing in revolutionizing computing
is as true today, even more true today than it
was before. And so we'll stick
with it you know until otherwise. There's
of course very difficult times along the way. You
know when you're investing in something and nobody
else believes in it and cost a lot of money and
you know maybe investors or or others would rather
you just keep the profit or you know whatever it
is improve the share price or whatever it is.
But you have to believe in your future. You have to
invest in yourself. And we believe this so
deeply that we invested you know tens
of billions of dollars before it really
happened. And yeah it was, it was 10 long
years. But it was fun along the way.
How would you summarize those core beliefs? What
is it that you believe about the way computers
should work and what they can do for us that keeps
you not only coming through that decade but also
doing what you're doing now, making bets I'm sure
you're making for the next few decades? The first
core belief was our first discussion, was about
accelerated computing. Parallel computing versus
general purpose computing. We would add
two of those processors together and we would do
accelerated computing. And I continue to believe
that today. The second was the recognition
that these deep learning networks, these DNNs, that
came to the public during 2012, these deep neural
networks have the ability to learn patterns and
relationships from a whole bunch of different
types of data. And that it can learn more and
more nuanced features if it could be larger
and larger. And it's easier to make them larger and
larger, make them deeper and deeper or wider and
wider, and so the scalability of the architecture
is empirically true. The fact
that model size and the data size being larger
and larger, you can learn more knowledge is
also true, empirically true. And so if that's
the case, you could you know, what what are the
limits? There not, unless there's a physical limit
or an architectural limit or mathematical limit
and it was never found, and so we believe that you
could scale it. Then the question, the only other
question is: What can you learn from data? What
can you learn from experience? Data is basically
digital versions of human experience. And so what
can you learn? You obviously can learn object
recognition from images. You can learn speech
from just listening to sound. You can learn
even languages and vocabulary and syntax and
grammar and all just by studying a whole bunch
of letters and words. So we've now demonstrated
that AI or deep learning has the ability to learn
almost any modality of data and it can translate
to any modality of data. And so what does that mean?
You can go from text to text, right, summarize a
paragraph. You can go from text to text, translate
from language to language. You can go from text
to images, that's image generation. You can go from
images to text, that's captioning. You can even go
from amino acid sequences to protein structures.
In the future, you'll go from protein to words: "What
does this protein do?" or "Give me an example of a
protein that has these properties." You know
identifying a drug target. And so you could
just see that all of these problems are around
the corner to be solved. You can go from words
to video, why can't you go from words to action
tokens for a robot? You know from the computer's
perspective how is it any different? And so it
it opened up this universe of opportunities and
universe of problems that we can go solve. And
that gets us quite excited. It feels like
we are on the cusp of this truly enormous change.
When I think about the next 10 years, unlike the
last 10 years, I know we've gone through a lot of
change already but I don't think I can predict
anymore how I will be using the technology that is
currently being developed. That's exactly right. I
think the last 10, the reason why you feel that way
is, the last 10 years was really about the science
of AI. The next 10 years we're going to have plenty
of science of AI but the next 10 years is going to
be the application science of AI. The fundamental
science versus the application science. And so the
the applied research, the application side of AI
now becomes: How can I apply AI to digital biology?
How can I apply AI to climate technology? How can
I apply AI to agriculture, to fishery, to robotics,
to transportation, optimizing logistics? How can
I apply AI to you know teaching? How do I apply AI
to you know podcasting right? I'd love to
choose a couple of those to help people see how
this fundamental change in computing that we've
been talking about is actually going to change
their experience of their lives, how they're
actually going to use technology that is based
on everything we just talked about. One of the
things that I've now heard you talk a lot about
and I have a particular interest in is physical
AI. Or in other words, robots - "my friends!" - meaning
humanoid robots but also robots like self-driving
cars and smart buildings or autonomous warehouses
or autonomous lawnmowers or more. From what
I understand, we might be about to see a huge
leap in what all of these robots are capable of
because we're changing how we train them. Up until
recently you've either had to train your robot in
the real world where it could get damaged or wear
down or you could get data from fairly limited
sources like humans in motion capture suits. But
that means that robots aren't getting as many
examples as they'd need to learn more quickly.
But now we're starting to train robots in digital
worlds, which means way more repetitions a day, way
more conditions, learning way faster. So we could
be in a big bang moment for robots right now and
NVIDIA is building tools to make that happen. You
have Omniverse and my understanding is this is 3D
worlds that help train robotic systems so that
they don't need to train in the physical world.
That's exactly right. You just just announced
Cosmos which is ways to make that 3D universe
much more realistic. So you can get all kinds
of different, if we're training something on
this table, many different kinds of lighting on the
table, many different times of day, many different
you know experiences for the robot to go through
so that it can get even more out of Omniverse. As
a kid who grew up loving Data on Star Trek, Isaac
Asimov's books and just dreaming about a future with
robots, how do we get from the robots that we have
now to the future world that you see of robotics?
Yeah let me use language models maybe ChatGPT
as a reference for understanding Omniverse and
Cosmos. So first of all when ChatGPT first
came out it, it was extraordinary and
it has the ability to do to basically from
your prompt, generate text. However, as amazing as
it was, it has the tendency to hallucinate if
it goes on too long or if it pontificates about
a topic it you know is not informed about, it'll
still do a good job generating plausible answers.
It just wasn't grounded in the truth. And so
people called it hallucination. And
so the next generation shortly it was, it had
the ability to be conditioned by context, so
you could upload your PDF and now it's grounded
by the PDF. The PDF becomes the ground truth. It
could be it could actually look up search and
then the search becomes its ground truth. And
between that it could reason about what is how
to produce the answer that you're asking for. And
so the first part is a generative AI and the
second part is ground truth. Okay and so now let's
come into the the physical world. The
world model, we need a foundation model just like
we need ChatGPT had a core foundation model
that was the breakthrough in order for robotics
to to be smart about the physical world. It has to
understand things like gravity, friction, inertia,
geometric and spatial awareness. It has to uh
understand that an object is sitting there even
when I looked away when I come back it's still
sitting there, object permanence. It has to
understand cause and effect. If I tip it, it'll
fall over. And so these kind of physical
common sense if you will has to be captured or
encoded into a world foundation model so that
the AI has world common sense. Okay and so we
have to go, somebody has to go create that, and
that's what we did with Cosmos. We created a world
language model. Just like ChatGPT was a language model,
this is a world model. The second thing we have to
go do is we have to do the same thing that we did
with PDFs and context and grounding it with
ground truth. And so the way we augment Cosmos
with ground truth is with physical simulations,
because Omniverse uses physics simulation which
is based on principled solvers. The mathematics
is Newtonian physics is the, right, it's the math we
know, all of the the fundamental laws of
physics we've understood for a very long
time. And it's encoded into, captured into Omniverse.
That's why Omniverse is a simulator. And using the
simulator to ground or to condition Cosmos, we can
now generate an infinite number of stories of the
future. And they're grounded on physical truth. Just
like between PDF or search plus ChatGPT, we can
generate an infinite amount of interesting things,
answer a whole bunch of interesting questions. The
combination of Omniverse plus Cosmos, you could
do that for the physical world. So to illustrate
this for the audience, if you had a robot in a
factory and you wanted to make it learn every
route that it could take, instead of manually
going through all of those routes, which could
take days and could be a lot of wear and tear on
the robot, we're now able to simulate all of them
digitally in a fraction of the time and in many
different situations that the robot might face-
it's dark, it's blocked it's etc - so the robot
is now learning much much faster. It seems to
me like the future might look very different than
today. If you play this out 10 years, how do you see
people actually interacting with this technology
in the near future? Cleo, everything that moves
will be robotic someday and it will be soon. You
know the the idea that you'll be pushing around
a lawn mower is already kind of silly. You know
maybe people do it because because it's fun but
but there's no need to. And every car is
going to be robotic. Humanoid robots, the technology
necessary to make it possible, is just around
the corner. And so everything that moves will be
robotic and they'll learn how to be
a robot in Omniverse Cosmos and we'll generate
all these plausible, physically plausible futures
and the the robots will learn from them and
then they'll come into the physical world and you
know it's exactly the same. A future where
you're just surrounded by robots is for certain.
And I'm just excited about having my own R2-D2.
And of course R2-D2 wouldn't be quite the can that
it is and roll around. It'll be you know R2-D2
yeah, it'll probably be a different physical
embodiment, but it's always R2. You know so my R2
is going to go around with me. Sometimes it's in my
smart glasses, sometimes it's in my phone, sometimes
it's in my PC. It's in my car. So R2 is with me
all the time including you know when I get home
you know where I left a physical version of R2. And
you know whatever that version happens to
be you know, we'll interact with R2. And so I
think the idea that we'll have our own R2-D2 for
our entire life and it grows up with us, that's
a certainty now yeah. I think a lot of news media
when they talk about futures like this they focus
on what could go wrong. And that makes sense. There
is a lot that could go wrong. We should talk about
what could go wrong so we could keep it from from
going wrong. Yeah that's the approach that we like
to take on the show is, what are the big challenges
so that we can overcome them? Yeah. What buckets do
you think about when you're worrying about this
future? Well there's a whole bunch of the
stuff that everybody talks about: Bias or toxicity
or just hallucination. You know speaking with
great confidence about something it knows nothing
about and as a result we rely on that information.
Generating, that's a version of generating
fake information, fake news or fake images
or whatever it is. Of course impersonation.
It does such a good job pretending to be a
human, it could be it could do an incredibly good
job pretending to be a specific human. And so
the spectrum of areas we
have to be concerned about is fairly clear and
there's a lot of people who are
working on it. There's a some of the stuff,
some of the stuff related to AI safety requires
deep research and deep engineering and
that's simply, it wants to do the right thing it
just didn't perform it right and as a result hurt
somebody. You know for example self-driving car
that wants to drive nicely and drive properly
and just somehow the sensor broke down or it
didn't detect something. Or you know made it
too aggressive turn or whatever it is. It did
it poorly. It did it wrongly. And so that's
a whole bunch of engineering that has to
be done to to make sure that AI safety is upheld
by making sure that the product functions properly.
And then and then lastly you know whatever what
happens if the system, the AI wants to do a good
job but the system failed. Meaning the AI wanted
to stop something from happening
and it turned out just when it wanted to do
it, the machine broke down. And so this is
no different than a flight computer inside
a plane having three versions of them and then
so there's triple redundancy inside the
system inside autopilots and then you have two
pilots and then you have air traffic control
and then you have other pilots watching out for
these pilots. And so that the AI safety
systems has to be architected as a community
such that such that these AIs one, work,
function properly. When they don't
function properly, they don't put people in harm's
way. And that they're sufficient safety and
security systems all around them to make sure
that we keep AI safe. And so there's
this spectrum of conversation is gigantic and and
you know we have to take the parts, take the
parts apart and and build them as engineers. One
of the incredible things about this moment that
we're in right now is that we no longer have a
lot of the technological limits that we had in a
world of CPUs and sequential processing. And we've
unlocked not only a new way to do computing and
and but also a way to continue to improve. Parallel
processing has a a different kind of physics to it
than the improvements that we were able to make
on CPUs. I'm curious, what are the scientific or
technological limitations that we face now in
the current world that you're thinking a lot
about? Well everything in the end is about how much
work you can get done within the limitations of
the energy that you have. And so that's
a physical limit and the laws of
physics about transporting information and
transporting bits, flipping bits and transporting
bits, at the end of the day the energy it takes
to do that limits what we can get done. And the
amount of energy that we have limits what we can
get done. We're far from having any fundamental
limits that keep us from advancing. In the meantime,
we seek to build better and more energy efficient
computers. This little computer, the the
big version of it was $250,000 - Pick up? - Yeah
Yeah that's little baby DIGITS yeah. This is
an AI supercomputer. The version that I delivered,
this is just a prototype so it's a mockup.
The very first version was DGX 1, I
delivered to Open AI in 2016 and that was $250,000.
10,000 times more power, more energy necessary
than this version and this version has six times
more performance. I know, it's incredible. We're
in a whole in the world. And it's only since 2016
and so eight years later we've in increased the
energy efficiency of computing by 10,000 times.
And imagine if we became 10,000 times more energy
efficient or if a car was 10,000 times more
energy efficient or electric light bulb was
10,000 times more energy efficient. Our light
bulb would be right now instead of 100 Watts,
10,000 times less producing the same illumination.
Yeah and so the energy efficiency of
computing particularly for AI computing that we've
been working on has advanced incredibly and that's
essential because we want to create you
know more intelligent systems and and we want to
use more computation to be smarter and so
energy efficiency to do the work is our number one
priority. When I was preparing for this interview, I
spoke to a lot of my engineering friends and this
is a question that they really wanted me to ask. So
you're really speaking to your people here. You've
shown a value of increasing accessibility
and abstraction, with CUDA and allowing more
people to use more computing power in all kinds of
other ways. As applications of technology get more
specific, I'm thinking of transformers in AI for
example... For the audience, a transformer is a very
popular more recent structure of AI that's now
used in a huge number of the tools that you've
seen. The reason that they're popular is because
transformers are structured in a way that helps
them pay "attention" to key bits of information and
give much better results. You could build chips
that are perfectly suited for just one kind of AI
model, but if you do that then you're making them
less able to do other things. So as these specific
structures or architectures of AI get more popular,
my understanding is there's a debate between how
much you place these bets on "burning them into the
chip" or designing hardware that is very specific
to a certain task versus staying more general and
so my question is, how do you make those bets? How
do you think about whether the solution is a car
that could go anywhere or it's really optimizing
a train to go from A to B? You're making bets
with huge stakes and I'm curious how you think
about that. Yeah and that now comes back
to exactly your question, what are your
core beliefs? And the question, the core
belief either one, that transformer is the last AI
algorithm, AI architecture that any researcher will
ever discover again, or that transformers
is a stepping stone towards evolutions of
transformers that are uh barely recognizable as a
transformer years from now. And we believe the
latter. And the reason for that is because you
just have to go back in history and ask yourself,
in the world of computer algorithms, in
the world of software, in the world of
engineering and innovation, has one idea stayed
along that long? And the answer is no. And so that's
kind of the beauty, that's in fact
the essential beauty of a computer that it's able
to do something today that no one even imagined
possible 10 years ago. And if you would have, if
you would have turned that computer 10 years ago
into a microwave, then why would the applications
keep coming? And so we believe, we believe in the
richness of innovation and the
richness of invention and we want to create an
architecture that let inventors and innovators
and software programmers and AI researchers
swim in the soup and come up with some amazing
ideas. Look at transformers. The fundamental
characteristic of a transformer is this idea
called "attention mechanism" and it basically says
the transformer is going to understand the meaning
and the relevance of every single word with every
other word. So if you had 10 words, it has to figure
out the relationship across 10 of them. But if you
have a 100,000 words or if your context is
now as large as, read a PDF and that read a whole
bunch of PDFs, and the context window is now like
a million tokens, the processing all of it across
all of it is just impossible. And so the way you
solve that problem is there all kinds of new ideas,
flash attention or hierarchical attention or you
know all the, wave attention I just read about
the other day. The number of different types of
attention mechanisms that have been invented
since the transformer is quite extraordinary.
And so I think that that's going to continue
and we believe it's going to continue and that
that computer science hasn't ended and that AI
research have not all given up and we haven't
given up anyhow and that having a
computer that enables the flexibility of
of research and innovation and new ideas is
fundamentally the most important thing. One of the
things that I am just so curious about, you design
the chips. There are companies that assemble the
chips. There are companies that design hardware to
make it possible to work at nanometer scale. When
you're designing tools like this, how do you think
about design in the context of what's physically
possible right now to make? What are the things
that you're thinking about with sort of pushing
that limit today? The way we do it is even
though even though we have things made like for
example our chips are made by TSMC. Even though
we have them made by TSMC, we assume that we need
to have the deep expertise that TSMC has. And so
we have people in our company who are incredibly
good at semiconductive physics so that we have a
feeling for, we have an intuition for, what are the
limits of what today's semiconductor physics
can do. And then we work very closely with them to
discover the limits because we're trying to push
the limits and so we discover the limits together.
Now we do the same thing in system engineering and
cooling systems. It turns out plumbing is really
important to us because of liquid cooling.
And maybe fans are really important to us
because of air cooling and we're trying to design
these fans in a way almost like you know they're
aerodynamically sound so that we could pass the
highest volume of air, make the least amount of
noise. So we have aerodynamics engineers in our
company. And so even though even though we don't
make 'em, we design them and we have to deep
expertise of knowing how to have them made. And
and from that we try to push the
limits. One of the themes of this conversation is
that you are a person who makes big bets on the
future and time and time again you've been right
about those bets. We've talked about GPUs, we've
talked about CUDA, we've talked about bets you've
made in AI - self-driving cars, and we're going to
be right on robotics and - this is my question. What
are the bets you're making now? the latest bet we
just described at the CES and I'm very very proud
of it and I'm very excited about it is the
fusion of Omniverse and Cosmos so that we have
this new type of generative world generation
system, this multiverse generation system. I
think that's going to be profoundly important in
the future of robotics and physical systems.
Of course the work that we're doing with human
robots, developing the tooling systems and the
training systems and the human demonstration
systems and all of this stuff that that you've
already mentioned, we're just seeing the
beginnings of that work and I think the
next 5 years are going to be very interesting in
the world of human robotics. Of course the work
that we're doing in digital biology so that
we can understand the language of molecules and
understand the language of cells and just as
we understand the language of physics and the
physical world we'd like to understand the language
of the human body and understand the language of
biology. And so if we can learn that, and we can
predict it. Then all of a sudden our ability to
have a digital twin of the human is plausible.
And so I'm very excited about that work. I love
the work that we're doing in climate science
and be able to, from weather predictions, understand
and predict the high resolution regional climates,
the weather patterns within a kilometer above
your head. That we can somehow predict that with
great accuracy, its implications is really quite
profound. And so the number of things that
we're working on is really cool. You know we
we're fortunate that we've created this
this instrument that is a time machine and
we need time machines in all of these areas that
we just talked about so that we can see
the future. And if we could see the future and
we can predict the future then we have a better
chance of making that future the best version
of it. And that's the reason why scientists
want to predict the future. That's the reason why,
that's the reason why we try to predict the future
and everything that we try to design so that we
can optimize for the best version. So if
someone is watching this and maybe they came into
this video knowing that NVIDIA is an incredibly
important company but not fully understanding why
or how it might affect their life and they're now
hopefully better understanding a big shift that
we've gone through over the last few decades in
computing, this very exciting, very sort of strange
moment that we're in right now, where we're sort
of on the precipice of so many different things.
If they would like to be able to look into the
future a little bit, how would you advise them to
prepare or to think about this moment that they're
in personally with respect to how these tools
are actually going to affect them? Well there are
several ways to reason about the future that
we're creating. One way to reason about it is,
suppose the work that you do continues to
be important but the effort by which you
do it went from you know being a week long
to almost instantaneous. You know that the
effort of drudgery basically goes to zero.
What is the implication of that? This is, this
is very similar to what would change if all
of a sudden we had highways in this country?
And that kind of happened you know in the last
Industrial Revolution, all of a sudden we have
interstate highways and when you have interstate
highways what happens? Well you know suburbs start
to be created and and all of a sudden you know
distribution of goods from east to west is
no longer a concern and all of a sudden gas
stations start cropping up on highways and
and fast food restaurants show up and you
know someone, some motels show up because people
you know traveling across the state, across the
country and just wanted to stay somewhere for a
few hours or overnight, and so all of a sudden
new economies and new capabilities, new economies.
What would happen if a video conference made
it possible for us to see each other without
having to travel anymore? All of a sudden
it's actually okay to work further away from
home and from work, work and live
further away. And so you ask yourself kind of
these questions. You know what would happen
if I have a software programmer with me
all the time and whatever it is I can dream up,
the software programmer could write for me. You
know what would, what would happen
if I just had a seed of an idea and
and I rough it out and all of sudden a you know
a prototype of a production was put in front
of me? And what how would that change my life and
how would that change my opportunity? And you
know what does it free me to be able to do and
and so on so forth. And so I think that the next
the next decade intelligence, not for everything
but for for some things, would basically become
superhuman. But I can tell
you exactly what that feels like. I'm surrounded
by superhuman people, super intelligence from
my perspective because they're the best in the
world at what they do and they do what they
do way better than I can do it. and I'm
surrounded by thousands of them and yet what it
it never one day caused me to to think all of a
son I'm no longer necessary. It actually empowers
me and gives me the confidence to go tackle more
and more ambitious things. And so suppose,
suppose now everybody is surrounded by these
super AIs that are very good at specific things
or good at some of the things. What would that
make you feel? Well it's going to empower you,
it's going to make you feel confident and
and I'm pretty sure you probably use ChatGPT and
AI and I feel more empowered today, more
confident to learn something today. The knowledge
of almost any particular field, the barriers to
that understanding, it has been reduced and I have
a personal tutor with me all of the time. And
so I think that that feeling should be universal.
If there's one thing that I would
encourage everybody to do is to go get yourself
an AI tutor right away. And that AI tutor could
of course just teach your things, anything you
like, help you program, help you write,
help you analyze, help you think, help you reason,
you know all of those things is going to
really make you feel empowered and and I think
that going to be our future. We're
going to become, we're going to become super humans,
not because we have super, we're going to become
super humans because we have super AIs. Could you
tell us a little bit about each of these objects?
This is a new GeForce graphics card and yes and
this is the RTX 50 Series. It is essentially
a supercomputer that you put into your PC and we
use it for gaming, of course people today use it
for design and creative arts and it does amazing
AI. The real breakthrough here and this is
this is truly an amazing thing, GeForce
enabled AI and it enabled Geoff Hinton, Ilya Sutskever,
Alex Krizhevsky to be able to train AlexNet. We
discovered AI and we advanced AI then AI came back
to GeForce to help computer graphics. And so here's
the amazing thing: Out of 8 million pixels or so in
a 4K display we are computing, we're processing
only 500,000 of them. The rest of them we use AI
to predict. The AI guessed it and yet the image is
perfect. We inform it by the 500,000 pixels that we
computed and we ray traced every single one and it's
all beautiful. It's perfect. And then we tell the
AI, if these are the 500,000 perfect pixels in this
screen, what are the other 8 million? And it goes it
fills in the rest of the screen and it's perfect.
And if you only have to do fewer pixels, are you
able to invest more in doing that because you have
fewer to do so then the quality is better so the
extrapolation that the AI does... Exactly. Because
whatever computing, whatever attention you have,
whatever resources you have, you can place it into
500,000 pixels. Now this is a perfect example of
why AI is going to make us all superhuman, because
all of the other things that it can do, it'll do
for us, allows us to take our time and energy and
focus it on the really really valuable things that
we do. And so we'll take our own resource which is
you know energy intensive, attention intensive, and
we'll dedicated to the few 100,000 pixels and
use AI to superres, upres it you know to
everything else. And so this this graphics card
is now powered mostly by AI and the computer
graphics technology inside is incredible as
well. And then this next one, as I mentioned
earlier, in 2016 I built the first one for AI
researchers and we delivered the first one to Open AI
and Elon was there to receive it and this
version I built a mini mini version and the
reason for that is because AI has now gone from AI
researchers to every engineer, every student, every
AI scientist. And AI is going to be everywhere.
And so instead of these $250,000 versions we're
going to make these $3,000 versions and schools
can have them, you know students can have them, and
you set it next to your PC or Mac and all of
a sudden you have your own AI supercomputer. And
you could develop and build AIs. Build your own
AI, build your own R2-D2. What do you feel like is
important for this audience to know that I haven't
asked? One of the most important things I would
advise is for example if I were a student today
the first thing I would do is to learn AI. How do
I learn to interact with ChatGPT, how do I learn
to interact with Gemini Pro, and how do I learn
to interact with Grok? Learninghow to
interact with with AI is not unlike being
someone who is really good at asking questions.
You're incredibly good at asking questions and
and prompting AI is very very similar.
You can't just randomly ask a bunch of questions
and so asking an AI to be assistant
to you requires some expertise and
artistry and how to prompt it. And so if I were,
if I were a student today, irrespective whether
it's for for math or for science or chemistry
or biology or doesn't matter what field of science
I'm going to go into or what profession, I'm
going to ask myself, how can I use AI to do my job
better? If I want to be a lawyer, how can I use
AI to be a better lawyer? If I want to be a better
do doctor, how can I use AI to be a better doctor?
If I want to be a chemist, how do I use AI to be
a better chemist? If I want to be a biologist, I how
do I use AI to be a better biologist? That question
should be persistent across everybody. And just as
my generation grew up as the first generation
that has to ask ourselves, how can we use computers
to do our jobs better? Yeah the generation before
us had no computers, my generation was the first
generation that had to ask the question, how do I
use computers to do my job better? Remember I came
into the industry before Windows 95 right, 1984
there were no computers in offices. And after that,
shortly after that, computers started to emerge and
so we had to ask ourselves how do we use computers
to do our jobs better? The next generation doesn't
have to ask that question but it has to ask
obviously next question, how can I use AI to
do my job better? That is start and finish I think
for everybody. It's a really exciting and scary and
therefore worthwhile question I think for everyone.
I think it's going to be incredibly fun. AI is
obviously a word that people are just learning
now but it's just you know, it's
made your computer so much more accessible. It is
easier to prompt ChatGPT to ask it anything you
like than to go do the research yourself. And so
we've lowered a barrier of understanding, we've
lowered a barrier of knowledge, we've
lowered a barrier of intelligence, and
and everybody really had to just go try
it. You know the thing that's really really crazy
is if I put a computer in front of somebody and
they've never used a computer there is no chance
they're going to learn that computer in a day.
There's just no chance. Somebody really has to
show it to you and yet with ChatGPT if you
don't know how to use it, all you have to do is
type in "I don't know how to use ChatGPT, tell
me," and it would come back and give you some
examples and so that's the amazing thing.
You knowthe amazing thing about intelligence is
it'll help you along the way and make you uh
superhuman you know along the way. All right I have
one more question if you have a second. This is
not something that I planned to ask you but on the
way here, I'm a little bit afraid of planes,
which is not my most reasonable quality, and
the flight here was a little bit bumpy mhm very
bumpy and I'm sitting there and it's moving and
I'm thinking about what they're going to say at my
funeral and after - She asked good questions, that's
what the tombstone's going to say  - I
hope so! Yeah. And after I loved my husband and my
friends and my family, the thing that I hoped that
they would talk about was optimism. I hope that
they would recognize what I'm trying to do here.
And I'm very curious for you, you've you've been
doing this a long time, it feels like there's
so much that you've described in this vision
ahead, what would the theme be that you would
want people to say about what you're trying to do?
Very simply, they made an extraordinary impact.
I think that we're fortunate because of some
core beliefs a long time ago and sticking with
those core beliefs and building upon them
we found ourselves today being one of
the most, one of the many most important and
consequential technology companies in
the world and potentially ever. And so
we take that responsibility veryseriously.
We work hard to make sure that
the capabilities that we've created are
available to large companies as well as
individual researchers and developers, across
every field of science no matter profitable or
not, big or small, famous or otherwise. 
And it's because of this understanding of
the consequential work that we're doing and the
potential impact it has on so many people
that we want to make make this capability
as pervasively as possible and I
do think that when we look back in a few
years, and I do hope that what the
next generation realized is as they, well
first of all they're going to know us because of
all the you know gaming technology we create.
I do think that we'll look back and the whole
field of digital biology and life sciences has
been transformed. Our whole understanding of of
material sciences has completely been
revolutionized. That robots are helping
us do dangerous and mundane things all over the
place. That if we wanted to drive we can drive
but otherwise you know take a nap or enjoy
your car like it's a home theater of yours,
you know read from work to home and at that
point you're hoping that you live far
away and so you could be in a car for longer.
And you look back and
you realize that there's this company almost at
the epicenter of all of that and happens
to be the company that
you grew up playing gameswith.
I hope for that to be
what the next generation learn.
Thank you so muchfor your time.
I enjoyed it, thank you! I'm glad!

Heat. Heat.
[Music]
[Music]
Tokens.
Tokens.
[Music]
Fore tokens.
[Music]
Soldier
[Music]
tokens.
Fore tokens.
[Music]
How
[Music]
many Sh.
[Music]
Young woman.
[Music]
[Applause]
Welcome to the stage, Nvidia founder and
CEO, Jensen Wong.
Taiwan. It's great to be here. My
parents are also in the
audience. They're up
there. Nvidia has been coming to Taiwan
for over 30 years. This is the home of
many of our
treasured
partners and dear friends. Over the
years, you have seen Nvidia grow up and
seen us
accomplished many exciting things and
have been partner with me all along the
way.
Today we're going to talk
about where we
are in the industry, where we're going
to
go, announce some new products, exciting
new products and surprising products
that open new markets for
us, creates new markets, new growth.
We're going to talk
about great partners and how we're going
to develop this ecosystem
together. As you know, we are at the
epicenter of the computer
ecosystem, one of the most important
industries of the
world. And so it stands to
reason when new markets has to be
created, we have to create it starting
here.
at the center of the computer
ecosystem. And I have
some surprises for
you, things that you probably wouldn't
have guessed. And then of course, I
promise I'll talk about
AI and we'll talk about
robotics. The NVIDIA
story is the reinvention of the computer
industry.
In fact, the Nvidia story is also the
reinvention of our
company. As I said, I was been coming
here for 30
years. Many of you have been through
many of my
keynotes. Some of you, all of
them. And just as you reflect on the
conversation, the things we talked about
in the last 30 years, how dramatic we
changed. We started out as a chip
company with a goal of creating a new
computing
platform and in
2006 we
introduced
CUDA which has revolutionized how
computing is done.
In 2016, 10 years later, we realized
that a new computing approach has
arrived. And this new computing approach
requires a reinvention of every single
layer of the technology stack. The
processor is new, the software stack is
new. It stands to reason the system is
new. And so we invented a new system. A
new system that on the day I announced
it at GTC
2006, no one understood what I was
talking about and nobody gave me a
PO. That system was called
DGX1.
DGX1. I donated the first one to a
nonprofit company called
OpenAI and it started the AI
revolution. Years later, we realized
that in fact this new way of doing
software which is now called artificial
intelligence is unlike traditional ways
of running software. Whereas many
applications ran
on a few
processors in a large data center. We
call that hypers
scale. This new type of
application
requires many processors working
together serving queries for millions of
people and that data center would be
architected fundamentally different. We
realized there were two types of
networks. one for north south because
you still have to control the storage.
You still have to have a control plane.
You still have to connect to the
outside. But the most important network
was going to be east
west. The computers talking to each
other to try to solve a problem.
We recognized the most the best
networking company in east west traffic
for high performance computing largecale
distributed
processing. A company that was very dear
to our company and very close to our
heart a company called Melanox. And we
bought them five years ago
2019. We converted an entire data center
into one computing unit. And you heard
me say
before, the modern computer is an entire
data center. The data center is a unit
of computing. No longer just a PC, no
longer just the server. The entire data
center is running one job and the
operating system would
change. NVIDIA's data center journey is
now very well
known. Over the last three years, you've
seen some of the ideas that we're
shaping and how we are starting to see
our company
differently. No company in history,
surely no technology company in history
has ever revealed a road map for five
years at a
time. No one would tell you what is
coming next. They keep it as a secret,
extremely confidential.
However, we realized that Nvidia is not
a technology company only
anymore. In fact, we are an essential
infrastructure company. And how can you
plan your
infrastructure, your land, your shell,
your power, your electricity, all of the
necessary financing around all over the
world? How would you possibly do that if
you didn't understand what I was going
to
make? And so we described our company's
road map in fair detail, enough detail
that everybody in the world can go off
and start building data
centers. We realize now we are an AI
infrastructure company. An
infrastructure company that's essential
all around the world.
Every region, every industry, every
company will build these
infrastructures. And what are these
infrastructure? These infrastructure in
fact not unlike the first industrial
revolution when people realized GE,
Westinghouse, Seammens realized that
there was a new type of technology
called electricity and new
infrastructure has to be built all
around the world. And these
infrastructure became essential part of
social infrastructure. That
infrastructure is now called
electricity. Years later, this is during
all of our generation, we realized there
was a new type of infrastructure. And
this new infrastructure was very
conceptual, very hard to understand. And
this infrastructure called
information. This information
infrastructure, the first time it was
described, made no sense to anybody. But
we now realized it is the internet and
every internet is everywhere and
everything is connected to it. Well,
there's a new infrastructure. Now, this
new infrastructure is built on top of
the first two. And this new
infrastructure is an infrastructure of
intelligence. I know that right now when
we say there's an intelligence
infrastructure, it makes no sense. But I
promise you in 10 years time you will
look back and you will realize that AI
has now integrated into everything. And
in fact we need AI everywhere and every
region, every industry, every country,
every company all needs AI. AI has now
part of infrastructure and this
infrastructure just like the internet
just like electricity needs
factories and these factories are
essentially what we build today. They're
not data centers of the past. A1
trillion dollar industry providing
information and storage supporting all
of our ERP systems and our employees.
It's that's a data center. A data center
of the past. This
is similar in the sense that it came
from the same industry. It came from all
of us, but it's going to emerge as
something completely different,
completely separated from the world's
data center. And these AI data centers,
if you will, are improperly described.
They are in fact AI factories. You apply
energy to it and it produces something
incredibly valuable. And these things
are called tokens. to the point where
companies are starting to talk about how
many tokens they produced last quarter
and how many tokens they produced last
month. Very soon we will be talking
about how many tokens we produce every
hour just as every single factory does.
And so the world has fundamentally
changed. We went from a company on the
day that we started our
company. I was trying to figure out how
big our opportunity was in 1993 and I
came to the conclusion Nvidia's business
opportunity was enormous.
$300
million. We're going to be rich.
$300 million chip
industry to a data center opportunity
that represents about a trillion dollars
to now an AI factory and AI
infrastructure industry that will be
measured in trillions of
dollars. And this is the exciting future
that we're
undertaking. Now, at its core,
everything we do is founded on several
important technologies. Of course, I
talk about accelerated computing a great
deal.
I talk about AI a great deal. What makes
Nvidia really special is the fusion of
these
capabilities and very specially very
especially the algorithms, the
libraries, what we call the CUDA X
libraries. We're talking about libraries
all the time and in fact we're the only
technology company in the world that
talks about libraries
non-stop and the reason for that is
because libraries is at the core of
everything that we do. Libraries is what
started it all. And I'm going to show
you a few new ones today. But before I
do that, let me show you a preview of
what I'm going to tell you today.
Everything you're about to see,
everything you're about to see is
simulation, science, and artificial
intelligence. Nothing you see here is
art. It's all simulation. It just
happens to be beautiful. Let's take a
look.
[Music]
[Music]
[Music]
Heat. Heat.
[Music]
Heat.
[Music]
Heat. Heat.
[Music]
Heat.
[Music]
Heat. Heat.
[Music]
[Music]
[Applause]
This is uh real-time computer graphics
I'm standing in front
of. This is not a video. This is
computer graphics is generated by
GeForce. This is a brand new GeForce
5060 RTX
5060. And this this is from Asus. My
good friend Johnny is in the front row.
And this this is from MSI. And we took
this incredible GPU and we shrunk it in
here. Does that make any
sense? See, this is incredible. And so
this is this is MSI's new uh laptop with
5060 in it. GeForce brought CUDA to the
world. Right now what you're seeing is
every single pixel is ray
traced. How is that possible that we're
able to simulate photon and deliver this
kind of frame rate at this resolution?
Well, the reason for that is artificial
intelligence.
We are only rendering we're only
rendering one out of 10
pixels. So every pixel that you see only
one out of 10 is actually computed. The
other nine AI
guessed. Does that make any
sense? And it's
perfect. It's completely perfect. It
guessed it perfectly. Of course the
technology is called DLSS neural
rendering. It took us many many years to
develop. We started developing it the
moment we started working on AI. So it's
been a 10-year journey and the advance
in computer graphics has been completely
revolutionized by AI. GeForce brought AI
to the world. Now AI came back and
revolutionized GeForce. So really really
amazing. Ladies and gentlemen, GeForce
You know, when you're CEO, you have many
children. And and GeForce brought us
here. And now all of our keynotes is 90%
not
GeForce. But it's not because we don't
love GeForce. GeForce RTX 50 series just
had its most successful launch ever, the
fastest launch in our history. and PC
gaming is now 30 years old. So that
tells you something about how incredible
GeForce is. Let's talk about
libraries. At the core, of course,
everything starts with CUDA. And by
making CUDA as as uh performant as
possible, as pervasive as possible, so
that the install base is all over the
world. Then applications can find a CUDA
GPU quite easily. The larger the install
base, the more developers want to create
libraries. The more
libraries, more amazing things are done,
better applications, more benefits to
users, they buy more computers. The more
computers, more CUDA. That feedback path
is vitally important. However,
accelerated computing is not general
purpose computing. General purpose
computing writes software. Everybody
writes it in, you know, Python or C or
C++ and you compile it. The methodology
for general purpose computing is
consistent throughout. Write the
application, compile the application,
run it on a CPU. However, that
fundamentally doesn't work in
accelerated computing because if you
could do that, it would be called a
CPU. What's the point of not of not just
changing the CPU? So, you could have
write the software, compile the
software, run it on a CPU. The fact that
you would have to do something different
is actually quite sensible. And the
reason for that is because so many
people worked on general purpose general
purpose computing, trillions of dollars
of innovation. How is it possible that
all of a sudden a few widgets inside a
chip and all of a sudden computers
become 50 times faster, 100 times
faster? That makes no sense. And so the
logic that we applied is that we could
accelerate applications if you
understood more about it. You can add
you can accelerate applications if you
were to create a architecture that are
better suited to accelerate to run at
the speed of light 99% of the runtime
and just even though it's only 5% of the
code which is quite surprising most
applications small small parts of the
code consumes most of the runtime. We
made that observation and so we went
after one domain after another. I just
showed you computer graphics. We also
have numeric. This is Kai numeric. This
is uh Kupi is the most uh most pervasive
numerical library. Aerial and Shona.
Aerial is the world's first GPU
accelerated radio signal processing for
5G and 6G. Once we make it
softwaredefined, then we can put on top
of it AI. So now we could bring AI to 5G
and 6G. Parabrics for genomics analysis.
Moni for medical imaging. Earth 2 for
weather prediction. Coo quantum for
quantum classical computer architectures
and computer systems. Coupravarians and
coup tensor contraction of tensor uh
mathematics. Uh megatron. This whole row
this whole column here consists of all
of our deep learning and all of our
libraries necessary for training as well
as inference uh for deep learning. just
revolutionized computing and it all
started with all these libraries not
just CUDA but CUDNN on top of KUDNN
there was Megatron Megatron then Tensor
RTLM and then now lately this brand new
operating system for large AI factories
Dynamo QDF for data frames like Spark
and SQL structure data has can be
accelerated as well QML classical
machine learning warp a framework for a
Pythonic framework for describing CUDA
kernels incredibly successful
coup mathematical operations
optimizations things like traveling
salesperson uh the ability to
optimize highly constrained large number
of uh variables uh type of problems like
supply chain optimization this is an
incredible success I'm very excited
about coup DSS and coup sparse for
sparse structure simulators uh those are
used for CAE and CAD fluid dynamics uh
finite element analysis incredibly
important for EDAS and CAE industry and
then of course KU litho one of the most
important libraries for computational
lithography mask making takes could
easily take a month and that mask making
process is extremely computationally
intensive and now with KU litho we could
ex we could accelerate that computation
by 50 times 70 times as a result this is
going set the the stage, open the world
for applying AI to lithography in the
future. We have great partners here.
TSMC is using Kitho quite extensively.
ASML, Synopsis, excellent partners
working with us in Kitho. So the the
libraries
themselves is what makes it possible for
us one domain of application after
another domain of science after another
domain of physics to be able to
accelerate those applications. But it
also opens up markets for us. We look at
particular regions and particular
markets and we say that area could
really be important to transform to the
new way of doing computing. If general
purpose computing after all these years
has run its course, why hasn't it run
its course in every single industry? So
one of the most important industries of
course is telecommunications.
Just as the world's cloud data centers
have now become softwaredefined, it
stands to reason that telecommunication
should also be software defined. And so
that's the reason why we've taken now
some six years to refine and optimize a
fully accelerated radio access network
RAN RANS stack that does incredible
performance for data rate per megawatt
or data rate data rate per watt. We are
now
on par with the state-of-the-art AS6.
And so once we could do that, once we
could achieve that level of performance
and functionality, then after that we
can layer on top AI. And so we have
great partners here. You could see
SoftBank and T-Mobile, Indoad and
Vodafone are doing trials. Nokia,
Samsung, Kiosera are working with us on
the full stack. Fujitsu and Cisco are
working on the systems. And so now we
have the ability to
introduce the idea of AI on 5G or AI on
6G um along with uh AI on
computing. We're doing that with quantum
computing. Quantum computing is still at
the noisy intermediate state
intermediate scale quantum called NISK.
However, there are many many good
applications we could already started to
do and so we're excited about that.
We're working on a class a quantum
classical or quantum GPU computing
platform. We call it CUDA Q working with
amazing companies around the
world. GPUs could be used for
pre-processing and post-processing for
error correction for control. And so in
the future I predict that all
supercomputers will have quantum
accelerators all have quantum QPUs
connected to it. And so a supercomputer
would be a QPU with QPUs and GPUs and
some CPUs. And that would be the
representation of a modern computer. So
working with a lot of great companies in
this area.
AI 12 years ago, we started with
perception AI models that can understand
patterns, recognize speech, recognize
images. That was the beginning. The last
five years, we've been talking about
generative AI, the ability for AI to not
just understand, but to generate. And
so, it could generate from text to text.
We use that all the time in chatbt. text
to images, text to video, video to text,
images to text, almost anything to
anything. Which is the really amazing
thing about AI that we've discovered a
universal function approximator, a
universal translator. It can translate
from anything to anything else if we can
simply tokenize it, represent the uh the
bits of information. Well, now we have
reached a level of AI that's really
important. Generative AI gave us oneshot
AI. You give a text and it gives you
text back. That was the two years ago
when we first engaged chat GPT. That was
the big amazing breakthrough. You give a
text and it gives you text back. It
predicts the next word, predicts the
next paragraph.
However, intelligence is much more than
just what you've learned from a lot of
data that you've studied. Intelligence
includes the ability to reason, to be
able to solve problems that you've not
seen before, to break it down step by
step, to maybe apply some rules and
theorems to solve a problem you've never
seen, to be able to simulate multiple se
multiple options and weigh its benefits.
Some of the technology you might have
heard about chain of thought breaking
down step by step, tree of thought
coming up a whole bunch of coming up a
whole bunch of paths. All of these
technologies are leading it leading uh
the ability for AI to be able to reason.
Now the amazing thing is once you have
the ability to reason and you have the
ability to perceive that is let's say
multimodal read PDFs you could do search
you can use tools you have now agentic
AI this agentic AI just does something
that I've just described all of us do we
take we're given a goal we break it down
step by step we reason about what to do
what's the best way to do it we consider
its consequences is and then we start
executing the plan. The plan might
include doing some research, might
include doing some work as using some
tools. It might include reaching out to
another AI agent to collaborate with it.
Agentic
AI is basically understand, think, and
act. Well, understand, think, and act is
the robotics loop. Agentic AI is
basically a robot in a digital form.
These are going to be really important
in the coming years. We're seeing
enormous progress in this area. The next
wave beyond that is physical AI. AI that
understands the world. They understand
things like inertia, friction, cause and
effect. That if I if I roll a ball and
it goes under a car, depending on the
speed of the ball, it probably went to
the other side of the car, but the ball
did not disappear. Object permanence.
You might be able to reason that if
there's a table in front of you and you
have to go you have to go to the other
side, the best way to do it is not to go
right through it. The best way maybe go
around it or underneath it. To be able
to reason about these physical things is
really essential to the next era of AI.
We call that physical AI. And so in this
particular case, you're seeing you're
seeing us simply prompt the AI and it
generates videos to train a self-driving
car in different scenarios. And I'll
show you more of that later. That's a
dog said, "Generate me a dog. Gener me
one with a bird with people." And it
started out with the image on the left.
And then the the phase after that we
take reasoning
systems, generative
systems, physical AI and this level of
capability would now go into a physical
embodiment. We call it a robot. If you
could imagine that you can prompt an AI
to generate a video to reach and pick
pick up a bottle. Of course, you could
imagine telling a robot to reach out and
pick up the bottle. The AI capability
today has the ability to do those
things. Well, the computer that we those
that's where we're going in the near
future. The computer that we're building
to make this possible has properties
that are very different than the
previous. The revolutionary computer
called Hopper came into the world about
three years ago and it revolutionized AI
as we know it. It became probably the
most popular, most well-known computer
in the world. In the last several years,
we we've been working on a new computer
to make it possible for us to do
inference time scaling or basically
thinking incredibly fast. Because when
you think, you're generating a lot of
tokens in your head, if you will. You're
generating a lot of thoughts and you
iterate in your brain before you produce
the answer. So what used to be oneshot
AI is now going to be thinking AI,
reasoning AI, inference time scaling AI
and that's going to take a lot more
computation. And so we created a new
system called Grace
Blackwell. Grace Blackwell does several
things. It has the ability to scale up.
Scale up means to turn what is a
computer into a giant computer. Scale
out is to take a computer and connect
many of them together and let the work
be done in many different computers.
Scaling out is easy. Scaling up is
incredibly
hard. Building larger
computers that is beyond the limits of
semiconductor physics is insanely hard.
And that's what Grace Blackwell does.
Grace Blackwell broke just about
everything. and all of you in in the
audience, many of you are partnering
with us to build Grace Blackwell
systems. I'm so happy to say that we're
in full production, but I am also we can
also say it was incredibly challenging.
Although the bl the Blackwell systems
based on HGX has been in full production
since the end of last year and has been
available since February, we are now
just putting online all the great Grace
Blackwell systems. They're coming online
all over the place every single day.
It's available in coreweave now for
several weeks. It's already being used
by many CSPs and now you're starting to
see it coming up from everywhere.
Everybody started to tweet out that
Grace Blackwell is in for production. In
Q3 of this year, just as I promised,
every single year, we will increase the
performance of our platform every single
year like Rhythm. And this this year in
Q3 we'll upgrade to Grace Blackwell
GB300. The GB300 will increase the is
the same
architecture, same architecture, same
physical footprint, same electrical,
mechanicals, but the chips inside have
been upgraded. It has upgraded with a
new black wall chip is now one and a
half times more inference performance.
Has one and a half times more HBM memory
and it has two times more networking.
And so the overall system performance is
higher. Well, let's take a look at
what's inside Grace
Blackwell. Grace Blackwell
starts Grace Blackwell starts with this
compute node. this compute node right
here. This is one of the compute nodes.
This is um what the last generation
looks like, the B200. This is what B300
looks like. Notice right here in the
center, it's 100% liquid cooled now, but
otherwise externally it's the same. You
could plug it into the same systems and
same chassis. And so this is the Grace
Blackwell GB300 system. It's one and a
half times more inference performance.
The training performance is about the
same but the inference performance is
one and a half times more. Now this
particular system here is 40 pedaflops
which is approximately the performance
of
the Sierra supercomput in
2018. The Sierra supercomput has 18,000
GPUs voltage GPUs. This one node here
replaces that entire supercomput.
4,000 times increase in performance in
six
years. That is extreme Moore's
law. Remember, I've said before that AI,
Nvidia has been scaling computing by
about a million times every 10 years,
and we're still on that track. But the
way to do that is not just to make the
chips faster. There's only a limit to
how fast you can make chips and how big
you can make chips. In the case of
Blackwell, we even connected two chips
together to make it possible. TSMC
worked with us to invent a new co-as
process called cos l that made it
possible for us to create these giant
chips. But still, we want chips way
bigger than that. And so we had to
create what is called MVLink. This is
the world's fastest switch. This MVL
link here, right here is 7.2 2 terabytes
per second. Nine of these go into that
rack. And that
nine, those nine switches are connected
by this miracle. This is um quite
heavy. That's because I'm quite strong.
I made it I made it look so light, but
this is almost this 70
pounds. And so this is the MVLink spine.
Two miles of
cables, 5,000 cables
structured, all
coaxed impen matched and it connects all
72 GPUs to all of the other 72 GPUs
across this network called MVLink
switch. 130 terabytes per second of
bandwidth across the MVLink spine. So
just put in
perspective the
peak traffic of the entire
internet the peak traffic of the entire
internet is
900
terabits per
second. Divide that by
eight.
This moves more traffic than the entire
internet. one MVLink spine across this
MV nine of these MVLink switches so that
every single GPU can talk to every other
GPU at exactly the same time. This is
the miracle
of
GB200 and because there's a limit to how
far you can drive Certis. This is as far
as any Certis has ever driven from CH.
This goes chip to the switch out to the
spine to any other any other ch any
other switch any other chip all
electrical. And so that limit caused us
to put everything in one rack. That one
rack is 120 kilowatts which is the
reason why everything has to be liquid
cooled. We now have the
ability to disagregate the
GPUs out of one motherboard essentially
across an entire rack. And so that
entire rack is one motherboard. That's
the miracle completely disagregated. And
now the GPU performance is incredible.
The amount of memory is incredible. The
networking bandwidth is incredible. And
now we can really scale these out. Once
we scale it up, then we can scale it out
into large systems. And notice almost
everything Nvidia builds are gigantic.
And the reason for that is because we're
not building data centers and servers.
We're building AI factories. This is
coreweave. This is Oracle cloud. The
power density of each rack is so great.
They have to they have to uh put them
further apart so that the power density
could be distributed. But really in the
end we're not building data centers.
We're building AI factories. And this is
the XAI Colossus factory. This is
Stargate. 4 million square feet. 4
million square feet. one gigawatt. And
so just think about this factory here.
This one gigawatt factory, this one
gigawatt factory is probably going to be
about, you know, 60 to 80 billion. Out
of that 60 to80 billion, the
electronics, the computing part of it,
these systems are 4050 billion of it.
And so these are gigantic factory
investments.
The reason why people build factories is
because you know you know the answer.
The more you the more you
buy. Say it with me. The more you buy,
the more you
make. That's what factories
do. Okay.
The technology is so complicated. The
technology is so complicated. And in
fact, just looking at it here, you still
cannot get the deep appreciation of the
amazing work that's being done in all of
our partners and all of the companies
here in the audience in Taiwan. And so
we made you a movie. I made you a movie.
Take a look.
Blackwell is an engineering marvel.
It begins as a blank silicon wafer at
[Music]
TSMC. Hundreds of chip processing and
ultraviolet lithography steps build up
each of the 200 billion transistors
layer by layer on a 12in
wafer. The wafer is scribed into
individual blackwell dye, tested and
sorted, separating the good dyes to move
forward. The chip on wafer on substrate
process done at TSMC spill and amcore
attaches 32 Blackwell dieseS and 128 HBM
stacks on a custom silicon interposer
wafer. Metal interconnect traces are
etched directly into it connecting
Blackwell GPUs and HBM stacks into each
system and package unit locking
everything into place. Then the assembly
is baked, molded, and cured, creating
the Blackwell B200 Super Chip. At KYC,
each Blackwell is stress tested in ovens
at 125 C and pushed to its limits for
several
hours. Back at Foxcon, robots work
around the clock to pick and place over
10,000 components onto the Grace
Blackwell PCB.
Meanwhile, additional components are
being prepared at factories across the
globe. Custom liquid cooling copper
blocks from Cooler Master, AVC, ARAS,
and Delta keep the chips at optimal
temperatures. At another Foxcon
facility, Connect X7 Supernicks are
built to enable scale out communications
and Bluefield 3DPUs to offload and
accelerate networking, storage, and
security tasks.
All these parts converge to be carefully
integrated into GB200 compute
[Music]
trays. MVLink is the breakthrough
high-speed link that Nvidia invented to
connect multiple GPUs and scale up into
a massive virtual GPU.
The MVLink switch tray is constructed
with MVLink switch chips providing 14.4
terabytes per second of all toall
bandwidth. MVLink spines form a custom
blindmated backplane integrating 5,000
copper cables to deliver 130 tab per
second of alltoall bandwidth. This
connects all 72 black wells or 144 GPU
diese into one giant GPU.
From around the world, parts arrive from
Foxcon, Wistron, Quanta, Dell, Asus,
Gigabyte, HPE, Super Micro, and other
partners to be assembled by skilled
technicians into a rack scale AI
supercomput. In total, 1.2 million
components, 2 m of copper
cable, 130 trillion transistors,
weighing 1,800 kg.
[Music]
From the first transistor etched into a
wafer to the last bolt fastening the
Blackbell rack, every step carries the
weight of our partners' dedication,
precision, and
craft. Blackwell is more than a
technological wonder. It's a testament
to the marvel of the Taiwan technology
ecosystem.
[Music]
We couldn't be
prouder of what we've achieved
[Music]
together. Thank you, Taiwan.
Thank
you. Thank you. That was pretty
incredible, right? But that was
you. That was you. Thank
you. Well,
Taiwan doesn't
just build supercomputers for the world.
Today I'm very happy to
announce that we're also building AI for
Taiwan. And so today we're announcing
that Foxcon,
Taiwan, the Taiwanese government,
Nvidia, TSMC, we're going to build the
first giant AI supercomputer here for
the AI infrastructure in the AI
ecosystem of Taiwan. Thank you.
Is there anybody who needs an AI
computer?
Any AI researchers in the audience?
[Applause]
every single student, every researcher,
every scientist, every
startup, every large established
company. TSMC themselves does enormous
amounts of AI and scientific research
already. And so, Foxcon does enormous
amount of work in robotics. I know that
there are many other companies in the
audience. I'm going to mention you in
just a second that are doing robotics
research and AI research and so having a
worldclass AI infrastructure here in Tai
in Taiwan is really important.
All of that is so that we could build a
very large
chip and MVLink and Blackwell this
generation made it possible for us to
create these incredible systems. Here's
one from Pegatron and QCT and Wistron
and Wei Win. This is from Foxcon and
Gigabyte and ASUS. And you could see the
front and the back of it.
And its entire goal, its entire goal is
to take these black weld chips that are,
you know, you could see how big they are
and turn it into one massive chip. Now
the ability to do that of course made
was made possible by MVLink but it
understates the complexity of the system
architecture the rich software ecosystem
that connects it all together the entire
ecosystem of 150 companies that came
together to build this this
architecture and the entire ecosystem in
technology in software in industry
has been the work of three
years. This is a massive industrial
investment and now we would like to make
it possible for anybody anybody who
wants to build data centers. It could be
a whole bunch of NVIDIA GB200s or 300s
and an accelerated computing systems for
Nvidia. It could be somebody else. And
so today we're announcing something very
special. We're announcing Nvidia MVLink
Fusion. MVLink
Fusion is so that you can
build
semi-custom AI
infrastructure, not just semi-custom
chips, because those are the good old
days.
You want to build AI infrastructure and
everybody's AI infrastructure could be a
little different. Some of you could have
a lot more CPUs and some of it could
have a lot more Nvidia GPUs and some of
it could be somebody's semi-custom AS6.
And those systems are so insanely hard
to build and they're all missing this
one incredible ingredient. This
incredible ingredient called MVLink.
MVLink so that you could scale up these
semi-custom systems and build really
powerful computers. And so today we're
announcing the MVLink Fusion. MVLink
Fusion kind of works like this. This is
the Nvidia platform 100% Nvidia. You got
Nvidia CPU, Nvidia GPU, the MVLink
switches, the networking from Nvidia
called Spectrum X or
Infiniband,
Nyx, network
interconnects,
switches and all of the entire system,
the entire infrastructure built end to
end. Now, of course, you can mix and
match it if you like and we now today
make it possible for you to mix and
match it even at the compute level. This
would be what you would do using your
custom
ASIC. And we have great partners I'll
announce in a second who are working
with us to integrate your special TPU or
your special ASIC, your special
accelerator. And it doesn't have to be
just a transformer accelerator. could be
an accelerator of any kind that you
would like to integrate into a large
scaleup
system. We create an MVLink chiplet.
It's basically a switch that a butts
right up to your
chip. There's IP that will be available
to integrate into your semi-custom ASIC.
And then once you do that, it fits right
into the compute boards that I mentioned
and it fits into this n
this ecosystem of an AI supercomputer
that I've shown you. Now maybe what you
would like is you would like to use your
own CPU. You've been building your own
CPU for some time and maybe your CPU has
built a very large ecosystem and you
would like to integrate Nvidia into your
ecosystem and now we make it possible
for you to do that. You could do that by
building a your custom CPU. We provide
you with our MVLink chipto-chip
interface into your ASIC. We connect it
with MVLink chiplets and now it connects
and directly abuts
into the Blackwell chips and our next
generation Reuben chips and again it
fits right into this ecosystem. This
incredible body of work now becomes
flexible and open for everybody to
integrate into. And so your AI
infrastructure could have some Nvidia, a
lot of yours, a lot of yours, you know,
and and uh a lot of CPUs, a lot of AS6,
maybe a lot of NVIDIA GPUs as well. And
so in any case, you have the benefit of
using the MVLink infrastructure and the
MVLink ecosystem that's and it's
connected perfectly to Spectrum
X and all of that you know is industrial
strength and has the benefit of an
enormous ecosystem of industrial
partners who have already made it
possible. So this is the MVL link
fusion. Whether you buy completely from
us that's fantastic. Nothing gives me
more joy than when you buy everything
from
Nvidia. I just want you guys to know
that.
But it gives me tremendous joy if you
just buy something from
Nvidia. And so we have some great
partners. We have some great partners.
Lchip, Astero Labs, Marll and one of our
great partners, MediaTek are going to be
partnering with us to work with ASIC or
semicustom customers, hyperscalers who
would like to build these things or CPU
vendors who would like to build these
things and they would be their
semi-custom ASIC provider. We also have
Fujitsu and Qualcomm who are building
semi who are building their CPUs with
MVLink to integrate into our ecosystem
and Cadence and Synopsis. We've worked
with them to inteer our IP to them so
that they can work with all of you and
make that IP available to all of your
chips. So this ecosystem is incredible.
But this just highlights the MVLink
Fusion ecosystem. Once you work with
them, you instantly get integrated into
the entire larger Nvidia ecosystem that
makes it possible for you to scale up
into these AI supercomputers. Now, let
me talk to you about some new product
categories. As you know, I've shown you
a couple of different computers.
However, in order to serve the vast
majority of the world, there are still
some computers that are missing. And so,
I'm going to talk about them. But before
I do that, I want to give you an update
that in fact this new computer we call
DGX Spark is in full
production. DGX
Spark will be ready will be available
shortly probably in a few weeks. We have
tremendous partners working with us.
Dell,
HPI, Asus, MSI,
Gigabyte,
Lenovo, incredible partners with working
with us. And this is the DJX Spark. This
is actually a production unit. This is
our version. This is our version.
However, our partners are building a
whole bunch of different
versions. This is designed for AI native
developers. If you're a developer,
you're a student, you're a researcher,
and you don't want to keep opening up
the cloud and getting it prepared and
then when you're done scrubbing it,
okay, but you would just like to have
your own basically your own AI cloud
sitting right next to you and it's
always on, always waiting for you. It
allows you to do your prototyping, early
development. And this is what's amazing.
This is um DJX Spark. It's one
pedlops and 128
gigabytes. In
2016 when I delivered DGX1, this is just
the bezel. I can't lift a whole
computer. It's 300 lb. This is
DGX1. This is one
pedlops and 128
gigabytes. Of course, this is 128
gigabytes of HBM memory. And this is 128
gigabytes of
LPDDR5X. The performance is in fact
quite similar. But what's most important
is that the work that you could do you
could work on this is the same work you
could do here. It's an incredible
achievement over just the course of
about 10 years. Okay. So this is DGX
Spark for anybody who would like to have
their own AI supercomputer. And it's um
I'll let all of our partners price it
for themselves, but one thing for
sure, everybody can have one for
Christmas. Okay, I've got another
computer I want to show you. If that's
not if that's not
enough and you would still like to have
your own personal Thank you, Janine.
This is Janine Paul, ladies and
gentlemen.
If that one isn't big enough for you,
here's one. This is another desk side.
This is also going to be available from
Dell and HPI, Asus, Gigabyte, MSI,
Lenovo. Uh, it'll be available from Box,
from Lambda, amazing workstation
companies. And this is going to be your
own personal
DGX supercomput.
This computer is the most performance
you can possibly get out of a wall
socket. You could put this in your
kitchen, but just
barely. If you put this in your kitchen
and then somebody runs the microwave, I
think that's the limit. And so this is
the limit. This is the limit of what you
can get out of a wall outlet. And this
is a DGX station. The programming model
of this and the giant systems that I
showed you are the same. That's the
amazing thing. One architecture, one
architecture and this has the ability,
enough capacity and performance to run a
1 trillion parameter AI model. Remember
Llama is Llama 70B. A one trillion
parameter model is going to run
wonderfully on this machine. Okay, so
that's the DGX station. So now let's
talk
about remember these systems are Thank
you, Jenny. These systems these systems
are AI
natives. They're AI native computers.
They're computers built for this new
generation of
software. It doesn't have to be x86
compatible. It doesn't have to run
traditional IT
software. It doesn't have to run
hypervisors. It doesn't have to run all
of the It doesn't have to run Windows.
These computers are designed for the
modern AI native
applications. Of course, these AI
applications could be APIs that can be
called upon by the tra traditional and
the classical applications. But in order
for us to bring AI into a new world and
this new world is enterprise
IT, we have to go back to our roots and
we have to
reinvent computing and bring AI into
traditional enterprise computing. Now
enterprise computing as we know is
really three layers. It's not just the
computing layer. It's compute, storage,
and networking. It's always compute,
storage, and networking. And just as AI
has changed everything, it stands to
reason that AI must have changed
compute, storage, and networking for
enterprise IT as well. Well, that lower
layer has to be completely reinvented
and we're in the process of doing that.
I'm going to show you some new products
that opens up, unlocks enterprise IT for
us. It has to work with the traditional
IT industry and it has to add a new
capability and the new
capability for enterprise is agentic AI.
basically
digital marketing campaign manager, a
digital researcher, a digital software
engineer, digital customer service,
digital chip designer, digital supply
chain manager, digital versions, AI
versions of all of the work that we used
to do. And as I mentioned earlier,
Agentic AI has the ability to reason,
use tools, work with other
AIs. So in a lot of
ways these are digital workers. They're
digital
employees. The world has a shortage of
labor. We have a shortage of workers by
2030 by about 30 to 50 million shortage.
It's actually limit limiting the world's
ability to grow. And so now we have
these digital agents that can work with
us. A 100% of NVIDIA software engineers
now have digital agents working with
them so that they can help them assist
them in developing better code and more
productively. And so in the future
you're going to have this layer that's
our vision you're going to have a layer
of agentic AIs AI agents. And so what's
going to happen to the world? What's
going to happen to enterprise? Whereas
we have HR for human workers, we're
going to have it becoming the HR of
digital
workers. And so we have to create the
necessary
tools for today's IT industry, today's
IT workers to be able to
manage, improve, evaluate a whole family
of AI agents that are working inside
their company. And so that's the vision
of what we want to build. But first, we
have to reinvent computing. Remember
what I said, enterprise it works on
x86. It runs traditional software such
as hypervisors from VMware or IBM Red
Hat or Nanix. It runs a whole bunch of
classical applications. And we need to
have computers that do the same thing
while it adds this new
capability while it adds this new cap
called agent AI. And so let's take a
look at that.
Okay, this
is this is the brand new RTX Pro. RTX
Pro Enterprise and Omniverse server.
This server can run everything. It has
x86 of course. It can run all of the
classical
hypervisors. It runs Kubernetes and
those hypervisors. So the way that your
IT department wants to manage your
network and how how they want to manage
your your clusters and orchestrate
workload works exactly the same way. It
has the ability to even stream Citrix
and what other what other virtual
desktops to your to your PC. Everything
that runs in the world today should run
here. Omniverse runs on here perfectly.
But in addition to that, in addition to
that, this is the
computer for enterprise AI
agents. Those AI agents could be only
text. Those AI agents could also be
computer
graphics, little TJs, you know, coming
to you, little toy Jensen's coming to
see you, you know, helping you do work.
And so those AI agents could be either
in text form, it could be in graphics
form, it could be in video form. All of
those workloads work on this system. No
matter the modality, every single model
that we know of in the world, every
application that we know of should run
on this. In fact, even Crisis works on
here. Okay, so anybody who's a GeForce
gamer, there are no GeForce gamer in the
room. Okay, what connects these eight
GPUs, the Blackwell, new Blackwell RTX,
RTX Pro 6000s, is this new motherboard.
This new motherboard is actually a
switched network. CX8 is a new category
of chips. It's a switch first,
networking chip second. It's also the
most advanced networking chip in the
world. This is now in volume production.
CX8 in the CX8. You plug you plug in the
GPUs. The CX8s are in the
back. PCI Express connected here. CX8
communicates between them and the
networking bandwidth is incredibly high
at 800 gigabits per second. And this is
the transceiver that plugs into here. So
each one of these GPUs have their own
networking interface. All of the GPUs
are now communicating to all of the
other GPUs on east west traffic.
Incredible performance. Now the
surprising part is this how incredible
it is.
So this is this is um RTX
Pro. This is the performance and I
showed you guys at GTC how to think
about performance in the world of AI
factories. The way to think about this
is throughput. This is tokens per second
which is the y- axis. The more output
your factory, the more tokens you
produce. Okay? So throughput is
measuring tokens per second. However,
every AI model is not the same. And some
AI models require much more reasoning.
And so you need those AI models, you
need the performance per user to be very
high. So the tokens per second per user
has to be high. And this is the problem
with factories. Factories either like to
have high throughput or low latency, but
it doesn't like to have both. And so the
challenge is how to create an operating
system that allows us to have high
throughput. the y- axis while having
very low latency which is the y- axis
interactivity tokens per second per
user. And so this chart tells you
something about the overall performance
of the computer of the overall computers
of the
factory. Look at all those different
colors. It reflects on it represents the
different ways you have to configure all
of our GPUs to achieve the performance.
Sometimes you need pipeline parallelism.
Sometimes you want expert parallelism.
Sometimes you want to batch, sometimes
you want to do spec speculative
decoding, sometimes you don't. And so
all of those different types of
algorithms have to be applied separately
and differently depending on the
workload. And the paro, the outside
area, the overall area of that curve
represent the capability of your
factory. Okay? And so notice something.
Hopper is
our this is the most famous computer in
the world. Hopper H100. The HGX
$225,000 is Hopper is down there. And
the Blackwall server you just saw, the
enterprise server is 1.7 times its
performance. But this is amazing. This
is Llama 70B. This is Deepseek R1.
Deepseek R1 is four times. Now the
reason for that of course is that
Deepseek R1 has been optimized and this
is Deepseek R1 is genuinely a gift to
the world's AI industry. The the amount
of computer science breakthrough is is
really quite significant and has really
opened up a lot of great research for uh
researchers in United States around the
world. Everywhere I go re deepse R1 has
made a real impact in how people think
about think about AI and how think about
inference and how think about reasoning
AIs. They've made a great contribution
to uh to the industry and to the world.
And so this is Deepseek R1. The
performance is four times the
state-of-the-art H100. That kind of puts
it in perspective. Okay. And so if
you're building enterprise AI, we now
have a great server for you. We now have
a great system for you. It's a computer
you could run anything on. It's a
computer that has incredible performance
and it, you know, whether it's x86 or
AI, all of it runs. Okay, it's going to
it's a our RTX Pro server is in volume
production across all of our partners in
the industry. This is likely the largest
go-to market of any system we have ever
taken to market. So, thank you very
much.
The compute platform is different. The
storage platform is different. And the
reason for that is because humans query
structured databases like
SQL. People query structured databases
like SQL. But AI wants to query
unstructured data. They want semantic.
They want meaning. And so we have to
create a new type of storage platform.
And this is the NVIDIA AI data platform
on top just as a just as SQL servers,
SQL software and file storage software
from your storage vendors that you work
with. There's a layer of very
complicated software that goes with
storage. Most storage companies, as you
know, is mostly a software
company. That software layer is
incredibly complicated. And so on top of
a new type of story system is going to
be a new query system we call IQ, Nvidia
AIQ or IQ. And this it's really
state-of-the-art. It's fantastic. And
working with basically everybody in the
storage industry. Your future storage is
no longer CPUs sitting on top of a rack
of storage. It's going to be GPU sitting
on top of a rack of storage. And the
reason for that is because you need the
system to embed find the meaning in the
data in the in the unstructured data in
the raw data. You have to index you have
to do the search and you do the ranking.
So that process is very compute
intensive. And so most storage servers
in the future will have a compute a GPU
computing node in front of it. It's
based on the models that we create.
Almost everything that I'm about to show
you starts with great AI models. We
create AI models. We put a lot of energy
and technology into post-training of
open AI models. We train we post train
these AI models with data that is
completely transparent to you. It is
safe and secure data and it's uh
completely completely uh uh okay to use
to train and we make that list available
to you to see and so it's completely
transparent. We make the data available
to you. We post train the models and our
post-trained model performance is really
incredible. It's right now downloadable
open-source reasoning model. The llama
neotron reasoning model is the world's
best. It's been downloaded tremendously.
It's and also we also surround it with a
whole bunch of AI other AI models so
that you can do what is called IQ the
retrieval part of it. It's 15 times
faster than what's available out there.
50% better query results. And so these
models are available all available to
you.
blueprints are the IQ blueprint are open
source and we work with the storage
industry to integrate these models into
their storage stack, their AI platform.
This is Vast. This is what it looks
like. I'm not going to go into it. I
just want to give you a texture of the
AI models that are integrated into their
platform. Let's take a look at what Vast
has done.
[Music]
Agentic AI changes how businesses use
data to make
decisions. In just 3 days, VAS built a
sales research AI agent using the NVIDIA
IQ blueprint and its accelerated AI data
platform using Nemo Retriever. The
platform continuously extracts, embeds,
and indexes data for fast semantic
search.
First, the agent drafts an
outline, then taps into CRM
systems, multimodal knowledge bases and
internal
tools. Finally, it uses Llama Nematron
to turn that outline into a step-by-step
sales plan.
Sales planning that took days now starts
with an AI prompt and ends with a plan
in
minutes. With Vast's accelerated AI data
platform, organizations can create
specialized agents for every
employee. Okay, so that's
Vast. Dell has a great AI platform, one
of the world's leading storage vendors.
Hitachi has a great AI platform. AI data
platform. IBM is building an AI data
platform with NVIDIA Nemo. NetApp is
building a net AI platform. As you could
see, all of these are open to you. And
if you're building an AI platform with a
semantic query AI in front of it, Nvidia
Nemo is the world's best. Okay. So that
gives you now compute for enterprise and
storage for enterprise. The next part is
a new layer of software called AI ops.
Just as supply chain has their ops and
HR has their ops in the future, it will
have AI ops and they will curate data.
They'll fine-tune the models, they'll
evaluate the models, guardrail the
models, secure the models, and we have a
whole bunch of libraries and models
necessary to integrate into the AI ops
ecosystem. We got great partners to help
us do that to take it to market for us.
Crowd Crowdstrike is working with us.
Data IQ is working with us. Data robots
is working with us. You could see these
are all AI
operations creating fine-tuning models
and deploying models for agentic AI in
enterprise. And you could see Nvidia
libraries of models integrated all over
it. So data robots, here's data
stacks, this is elastic. I think I heard
somewhere that they're downloaded 400
billion times. This is
Newonix. This is Red Hat. This is Trend
Micro here in Taiwan. I think I saw Eva
earlier. Okay. Hi, Eva. Okay. Wait some
biases. Okay. And so, so that's it. This
is this is how we're going to bring the
world bring to the world's enterprise IT
the ability to add AI to everything that
you do. You're not going to rip out
everything from it the enterprise IT
organizations because companies have to
run. But we can add AI into it. And now
we have systems that are enterprise
ready with ecosystem partners.
Incredible ecosystem partners. I think I
saw Jeff earlier. There's Jeff Clark,
the great Jeff Clark, been coming to
Taiwan for as long as I have been coming
to Taiwan and has been a partner of all
yours for a long time. So there's Jeff
Clark and and so our ecosystem partners
Dell and others are going to take this
platform, these platforms uh to the
world's enterprise IT. Okay, let's talk
about
robots. So agent AIS, agentic AIs, AI
agents, a lot of different ways to say
it. Agents are essentially digital
robots. Re re reason for that is because
a robot perceives, understands and plans
and that's essentially what agents do.
But we would like to build also physical
robots. And these physical robots first
it starts with the ability to learn to
be a robot. The ability to learn to be a
robot can't be done in the physical
world productively. You have to create a
virtual world where the robot can learn
how to be a good robot. that ro that
virtual world has to obey the laws of
physics. Most physics engines don't have
the ability to with fidelity deal with
rigid and soft body simulation. And so
we partner with deep uh with uh with
deep mind Google deep mind and Disney
research to build Newton the world's
most advanced physics engine. It's going
to be open sourced in
July. It's incredible what it can do.
It's completely GPU
accelerated. It's differentiable, so you
could learn from
experience. It is incredibly high
fidelity. It's super real
time. And so we could use that Newton
engine and it's integrated into Mujoko.
It's integrated into Nvidia's Isaac sim.
So irrespective of the simulation
environment and framework you use. And
so with that, we can bring these robots
to life.
[Music]
[Music]
[Music]
Who doesn't want that? I want that.
Can you imagine one of those little ones
or a few of them running around the
house chasing your
dogs, driving them crazy? And so, did
you see what was happening? It wasn't an
animation. It was a
simulation. And he was slipping and
sliding in the sand, in the dirt. All of
it was simulated. The software of the
robot is running in the simulation. And
so it wasn't animated, it was simulated.
And so in the future, we'll take the AI
models that we train and we put it into
that robot in simulation and let it
learn how to be a great robot. Well, our
we're working on several things to help
the robotics industry. Now, you know
that we've been working in in uh
autonomous systems for some time. Our
self-driving car basically has three
systems. There's the system for creating
the AI model and that's GB200, GB300.
It's going to be used for that, training
the AI model. Then you have Omniverse
for simulating the AI model. And then
when you're done with that AI model, you
put that model, the AI into the
self-driving car. Okay, this year we're
deploying Mercedes around the world our
self-driving car stack end to end stack.
But we create all of this in all the and
the way we go to market is exactly the
same that we work everywhere else. We
create the entire stack. We open the
entire stack and for our partners they
use whatever they want to use. They
could use our computer and not our
library. They could use our computer our
library and also our runtime. However
much you would like to use, it's up to
you because there's a lot of different
engineering teams and different
engineering styles and different
engineering capabilities. We want to
make sure that we provide our technology
in a way that makes it as easy as
possible for everybody to integrate NVs
technology. You know, like I said, I
love it if you buy everything from me,
but just please buy something from
me. Very
practical. And so, so we're doing
exactly the same thing in robotic
systems just like cars. And so this is
our Isaac Groot platform. The simulation
is exactly the same. It's omniverse. The
compute the training system is the same.
When you're done with the model, you put
it into inside this Isaac group
platform. And Isaac Group platform
starts with a brand new computer called
Jetson Thor. This is just started in
production. It is an incredible
processor. Basically a robotic processor
goes to self-driving cars and it goes
into a human or robotic system. On top
is an operating system we called Isaac.
Nvidia Isaac. The Nvidia Isaac operating
system is the runtime. It does all of
the neuronet network processing, the
sensor processing, pipelines, all of it
and deliver actuated results and then on
top of it pre-trained models that we
created with an crazy an amazing re uh
amazing robotics team uh that are
pre-training these models and uh all the
tools necessary in creating this we make
available including the model. And so
today we're announcing that Isaac Groot
N1.5 is now open sourced and it's open
to the world to use. It's been
downloaded 6,000 times already and the
popularity and the likes and the and the
and the uh uh appreciation from the
community is incredible. And so that's
creating the
model. We also open the way we created
the model. The biggest challenge in
robotics is and well the biggest
challenge in AI overall is what is your
data
strategy and your data strategy has to
be that's where great deal of research
and a great deal of technology goes into
in the case of robotics human
demonstration just like we demonstrate
to our children or a a coach
demonstrates to an athlete you
demonstrate using telea operations you
demonstrate to the robot how to perform
the task and the robot can generalize
ize from that demonstration because AI
can generalize and we have technology
for generalization. You can generalize
from that one demonstration other
techniques. Okay. And so what
if what if you want to teach this robot
a whole bunch of skills? How many
different teleoperation people do you
need? Well, it turns out to be a lot.
And so what we decided to do was use AI
to
amplify the human demonstration systems.
And so this is essentially going from
real to real and using an AI to help
us expand amplify the amount of data
that was collected during human
demonstration to train an AI model.
Let's take a look.
The age of generalist robotics has
arrived.
with breakthroughs in mechatronics,
physical AI, and embedded
computing. Just in time, as labor
shortages limit worldwide industrial
growth, a major challenge for robot
makers is the lack of large-scale, real,
and synthetic data to train models.
Human demonstrations aren't scalable,
limited by the number of hours in a day.
Developers can use NVIDIA Cosmos
physical AI world foundation models to
amplify
data. Group dreams is a blueprint built
on Cosmos for largecale synthetic
trajectory data generation, a realtoreal
data
workflow. First developers fine-tune
Cosmos with human demonstrations
recorded by teleoperation of a single
task in a single environment.
Then they prompt the model with an image
and new instructions to generate dreams
or future world
states. Cosmos is a generative model so
developers can prompt using new action
words without having to capture new
teleop
data. Once a large number are
generated, Cosmos reasons and evaluates
the quality of each dream, selecting the
best for training. But these dreams are
still just pixels. Robots learn from
actions. The Groot Dreams blueprint
generates 3D action trajectories from
the 2D dream videos. This is then used
to train the robot
model. Groot Dreams lets robots learn a
huge variety of new actions with minimal
manual captures. So a small team of
human demonstrators can now do the work
of thousands.
Groot Dreams brings developers another
step closer to solving the robot data
challenge. Is that
great? So in order for robotics to
happen, you need you need AI. But in
order to teach the AI, you need AI. And
so this is really the great thing
about the era of agents where we need a
a large amount of synthetic data
generation. robotics, a large amount of
synthetic data generation and skill
learning called fine-tuning, which is a
lot of reinforcement learning and
enormous amount of compute. And so this
is an er this is a whole era where the
training of these AI, the development of
these as well as the running of the AI
needs an enormous amount of compute.
Well, as I mentioned earlier, the world
has a severe shortage of labor. And the
reason why humano robotics is so
important is because it is the only form
of robot that can be deployed almost
anywhere brownfield. It doesn't have to
be green field. It could fit into the
world we created. It could do the task
that we made for ourselves. We
engineered the world for ourselves and
now we could create a robot that fit
into that world to help us. Now the
amazing thing about human robotics is
not just the fact that if it worked it
could be quite versatile. It is likely
the only robot that is likely to work.
And the reason for that is because
technology needs
scale. Most of the robotic systems we've
had so far are too low volume. And those
low volume systems will never achieve
the technology scale to get the flywheel
going far enough fast enough so that
we're willing to dedicate enough
technology into it to make it better.
But human or robot, it is likely to be
the next multi-t trillion dollar
industry and the technology innovation
is incredibly fast and the consumption
of computing and data centers enormous.
But this is one of those applications
that needs three computers. One computer
is an AI for learning. One computer is a
simulation engine where the AI could
learn how to be a robot in a uh in a
virtual environment and then also the
deployment of it. Everything that moves
will be robotic. As we put these robots
into the factories, remember the
factories are also
robotic. Today's factories are so
incredibly
complex. This is
Delta's manufacturing line and they're
getting it ready for a robotic future.
It is already robotics and software
defined and now in the future there will
be robots working in it. In order for us
to create robots and design robots that
operate in and as a fleet, as a team,
working together in a factory that is
also robotic, we have to give it
omniverse to learn how
to work together. And that digital twin,
you now have a digital twin of the
robot. You have a digital twin of all of
the equipment. You're going to have
digital twin of the factory. Those
nested digital twins are going to be
part of what Omniverse is able to do.
This is Delta's digital twin. This is
WiiW's digital twin. Now, while you're
looking at this, if you're not if if you
look at it too closely, you think that
it's in fact photographs. These are all
digital twins. They're all simulations.
They just look beautiful. The image just
looks beautiful, but they're all digital
twins. This is Pegatron's digital twin.
This is Foxcon's digital
twin. This is Gigabytes digital
twin. This is
Quantis. This is
Wistrons. TSMC is building a dig digital
twin of their next
fab. As we speak, there are five
trillion dollars of plants being planned
around the world. Over the next three
years, $5 trillion dollars of new plants
because the world is reshaping because
re-industrialization moving around the
world. New plants are being built
everywhere. This is an enormous
opportunity for us to make sure that
they build it well and cost effectively
and on time. And so putting everything
into a digital twin is really a great
first step and preparing it for a
robotic future. In fact, building that
$5 trillion doesn't include a new type
of factory that we're building. And even
our own factories we put in a digital
twin. This is the Nvidia AI factory in a
digital
twin. Gong is a digital twin. They made
Gausong a digital
twin. They're already hundreds of
thousands of buildings, millions of
miles of roads. And so, yes, Gaus is a
digital twin. Let's take a look at all
of
this. Taiwan is pioneering
softwaredefined
manufacturing. TSMC, Foxcon, Wistron,
Pegatron, Delta Electronics, Quanta,
Wiiwin, and Gigabyte are developing
digital twins on Nvidia Omniverse for
every step of the manufacturing process.
TSMC with MEDAI generate 3D layouts of
an entire fab from 2D
CAD and develop AI tools on
COP that can simulate and optimize
intricate piping systems across multiple
floors saving months of time.
Quanta, Wistron, and Pegatron plan new
facilities and production lines
virtually prior to physical
construction, saving millions in costs
by reducing
downtime. Pegatron simulates solder
paste
dispensing, reducing production defects.
Quanta uses Seaman's Team Center X with
Omniverse to analyze and plan multi-step
processes. Foxcon, Wistron, and Quanta
simulate power and cooling efficiency of
test data centers with Cadence reality
digital
twin. and to develop physical AI enabled
robots. Each company uses its digital
twin as a robot gym to develop, train,
test, and simulate
robots, whether manipulators, AMRs,
humanoids, or vision AI agents as they
perform their tasks or work together as
a diverse
fleet. And when connected to the
physical twin with IoT, each digital
twin becomes a realtime interactive
dashboard. Pegatron uses NVIDIA
Metropolis to build AI agents who help
employees learn complex
[Music]
techniques. Taiwan is even bringing
digital twins to its cities.
LinkerVision and the city of Kaos use a
digital twin to simulate the effects of
unpredictable
scenarios and built agents that monitor
city camera
streams, delivering instant alerts to
first
responders. The age of industrial AI is
here. Pioneered by the technology
leaders of
Taiwan. Powered by
[Music]
Omniverse. My entire keynote is your
work. It's so excellent.
Well, it stands to reason it stands to
reason that Taiwan at the center of the
most
advanced
industry, the epicenter where AI and
robotics is going to come from. It
stands to reason that this is an
extraordinary opportunity for
Taiwan. This is also the largest
electronics manufacturing region in the
world. And so it stands to reason that
AI and robotics will transform
everything that we do. And so it's
really quite extraordinary that for one
of the first times in history that the
work you
do has revolutionized every industry and
now it's going to come back to
revolutionize yours. At the beginning I
said that GeForce brought AI to the
world and then AI came
back and transformed GeForce.
You brought AI to the world. AI will now
come back and transform everything that
you do. It's been a great pleasure
working with all of you. Thank
you. I have a new product. I announced
several products already today, but I
have a new product to announce. I have a
new product to announce. We've been
building out in space dock for some
time and
um I think it's time for us to reveal
one of the largest products that we've
ever built. And it's uh parked outside
uh waiting for us. Let's uh let's see
how it goes.
[Music]
[Applause]
Heat. Heat.
[Music]
[Applause]
[Music]
Heat. Heat.
[Applause]
[Music]
[Applause]
Nvidia
Constellation. Nvidia Constellation.
Well, as you know, we have been growing
and all of our partnerships with
you have been growing. The number of
engineers we have here in Taiwan have
been
growing. And so, we are growing beyond
the limits of our current
office. And so, I'm going to build them
a brand new NVIDIA Taiwan office. and
it's called Nvidia
Constellation. We've also been selecting
the
sites. We've been selecting the sites
and all of the mayors and all the
different cities have been very kind to
us
and I think we got some nice
deals. I'm not sure. Seems quite
expensive.
But prime real estate is prime real
estate. And so today today I'm very
pleased to announce that Nvidia
constellation will be at Beto Shilling.
[Applause]
We have we have negotiated
uh the transfer of the lease from the
current current uh current owners of
that lease.
However, I understand that in order for
the mayor to approve that
lease, he wanted to know
whether the people of
Taipei approve of us building a large,
beautiful Nvidia constellation here. Do
you
He also asked for you to call
him and so I I'm sure you know his
numbers. Everybody call him right away.
Tell him that you think it's a great
idea. So this is going to be Nvidia
Constellation. We're going to build it.
We're going to start building as soon as
we can. We need the office space. Nvidia
Constellation Beetho Sheiling. Very
exciting.
Okay. Well, I want to thank all of you.
I want to thank all of you for your
partnership over the years. We are at a
once-in-a-lifetime opportunity. It is
not It is not an understatement to say
that the opportunity ahead of us is
extraordinary. For the very first time
in all of our time together, not only
are we
creating the next generation of IT,
we've done that several times from PC to
internet to cloud to mobile cloud. We've
done that several times. But this time,
not only are we creating the next
generation of IT, we are in fact
creating a whole new industry. This
whole new
industry is going to expose us to giant
opportunities ahead. I look forward to
partnering with all of you on building
AI factories, agents for enterprises,
robots, all of you amazing partners,
building the ecosystem with us around
one architecture. And so I want to thank
all of you for coming today. Have a
great Computex everybody.
Shash. Thank you.
Thank you for coming. Thank you.
[Music]
[Music]

This is how intelligence is made.
A new kind of factory.
Generator of tokens
The building blocks of AI
Tokens have opened a new frontier
The first step into an extraordinary world.
Where endless possibilities are born.
Tokens transform images into scientific data.
Charting alien atmospheres.
And guiding the explorers of tomorrow.
They probe the Earth's depths.
To seek out hidden danger.
They turn potential
Into plenty
And help us harvest our bounty.
Tokens see disease before it takes hold.
Cure with precision
And learn
What makes us tick?
Token connect the dots
So we can protect our most noble creatures.
Tokens decode the laws of physics.
To move us faster.
And make our days more efficient.
Tokens don't just teach robots how to move,
but to bring joy.
And comfort
Mar. Hi.
Are you ready to see the Dr.? What's that?
It's my enchanting tool.
Tokens help us move forward.
One small step for man.
Becomes one giant leap for mankind.
So we can boldly go
Where no one has gone before.
And here
Is where it all begins.
Welcome to the Stage NVIDIA
Founder and CEO Jensen Huang.
Oh.
Bonjour.
NVIDIA's first GTC in Paris.
This is incredible.
Thank you for all the
partners who are here with us.
We have so many people that
we work with over the years.
In fact, we've been in Europe for a very
long time though. This is my first GTC
Paris. I have a lot to tell you.
NVIDIA, once upon a time,
wanted to create a new computing platform.
To do things that normal computers cannot.
We accelerated the CPU, created a new type of
computing called accelerated computing, and one
of our first applications was molecular dynamics.
We've come a long way since.
So many different libraries. And in fact,
what makes accelerated computing special?
It's not just a new processor
that you compile software to.
You have to reformulate how you do computing.
You have to reformulate your algorithm and it
turns out to be incredibly hard for
people to reformulate software and
algorithms to be highly parallelized.
And so we created libraries to help.
Each market, each domain of application.
Becomes accelerated. Each one of these
libraries opens up new opportunities for the
developers and it opens up new opportunities
for growth for us and our ecosystem partners.
Computational lithography. Probably the single
most important application in semiconductor
design today, runs in a factory at TSMC.
Samsung large semiconductor fabs. Before the
chip is made, it runs through an inverse physics
algorithm called cuLitho, computational
lithography.
Direct sparse solvers
Algebraic multi-grid solvers.
CuOpt, we just opened sourced.
incredibly exciting Application Library.
This library accelerates decision making
To optimize problems with millions of
variables for millions of constraints
like traveling salespeople problems.
Warp, a Pythonic
Framework for expressing
geometry and physics solversreally important.
cuDF cuML, Structured databases, DataFrames,
classical machine learning algorithms
cuDf accelerates Spark with zero
lines of code change.
cuML accelerates Sidekick
learn with zero lines of code change.
Dynamo and CuDNN
cuDNN is probably the single most important
library NVIDIA has ever created. It accelerates
the primitives of deep neural networks. And
Dynamo is our brand new library that makes
it possible to dispatch, orchestrate,
distribute extremely complex inference
workloads across an entire AI factory.
cuEquivariance and cuTensor tensor
contraction algorithms.
Equivariance is for neural
networks that obey the laws of geometry, such as
proteins, molecules, Aerial, and Sionna. Really
important framework to enable AI to run 6G.
Earth-2, our simulation environment for
foundation models of weather and climate models.
Kilometer-squared, incredibly high resolution.
MONAI, our framework for medical imaging,
incredibly popular. Parabricks is a solver
for genomics analysis incredibly successful.
cuQuantum, CUDA-Q I'll talk about in just
a second for quantum computing and
cuPyNumeric acceleration for NumPy and
SciPy. As you can see, these are just a few of
the examples of libraries. There are 400 others.
Each one of them accelerates
a domain of application.
Each one of them opens up new opportunities.
Well, one of the most exciting.
Most exciting is CUDA-Q.
CUDA-X is this suite of libraries.
Library suite for accelerating applications and
algorithms on top of CUDA. We now have CUDA-Q.
CUDA-Q is for quantum computing. For
quantum classical computing based on GPUs.
We've been working on CUDA now for several years.
And today I can tell you there's an inflection
point happening in quantum computing.
As you know.
The first physical qubit was
demonstrated some nearly 30 years ago.
An error correction algorithm
was invented in 1995.
And in 2023, almost 30 years later.
The world's first logical
qubit was demonstrated by Google.
Since then, a couple of years later, the number
of logical qubits which is represented by a whole
lot of physical qubits with error correction,
the number of logical qubits are starting to grow.
Well, just like Moore's Law would. I could totally
expect ten times more logical qubits every
five years, 100 times more logical qubits
every ten years. Those logical qubits would
become better error corrected, more robust,
higher performance, more resilient, and
of course will continue to be scalable.
Quantum computing is reaching an inflection
point. We've been working with quantum
computing companies all over the
world in several different ways,
but here in Europe there's a large community.
I saw Pascal last night. I saw Barcelona
supercomputing last night. It is clear now.
We are within reach of being able to apply
quantum computing, quantum classical
computing, in areas that can solve some
interesting problems in the coming years.
This is a really exciting time. and
so we've been working with all of the
supercomputing centers. It's very clear now.
That over the next several years, or at
least the next generation of supercomputers,
every single one of them will have a QPU
assigned and QPU connected to GPUs. The QPU
will do quantum computing, of course.
And the GPUs would be used for
pre processing for control.
Error correction, which would
be intensely computationally intensive,
post processing and such. Between the two
architectures, just as we accelerated the
CPU, now there's QPU working with the GPU
to enable the next generation of computing.
Well, today we're announcing that our entire
quantum algorithm stack is now accelerated on
Grace Blackwell 200 and the speed up is utterly
incredible. We work with the Quantum computing
industry in several different ways. One way is
using cuQuantum to simulate the qubits or simulate
the algorithms that runs on top of these quantum
computers, essentially using a classical computer
to simulate or emulate a quantum computer.
At the other extreme,
extremely important is CUDA-Q.
Basically inventing a new CUDA that
extends CUDA into quantum classical so
that applications that are developed on CUDA.
Can run before the quantum computer arrives
in an emulated way or after the quantum
computer arrives in a collaborative way.
A quantum classical
accelerated computing approach.
And so today we're announcing CUDA
is available for Grace Blackwell.
The ecosystem here is incredibly rich.
Of course, Europe is deep with science
and deep with supercomputing
expertise and deep with heritage
in this area. And it's not surprising to see.
quantum computing advance here. in the next
several years, we're going to see a really
fantastic inflection point. So anyways, for
all of the quantum industry that have been working
on this for three decades now, I congratulate you
for just the incredible accomplishment
and the milestones today. Thank you.
Let's talk about AI.
You might be surprised
That I would I would be talking to
you about AI. the same, the same GPU.
That
Ran and enabled all of these
applications that I mentioned. That same GPU.
Enabled artificial intelligence to come
to the world. Our first contact
was in 2012 just prior to that.
Working with developers on a new
type algorithm called deep learning.
It enabled the AlexNet big bang of AI in 2012.
In the last 15 years or so AI has progressed
incredibly fast. The first
wave of AI was perception.
For computers to recognize information.
Understand it. The second wave,
which most of us were talking about.
The last five years or so was Generative AI.
It's multimodal, meaning that an AI was
able to learn both images and Language.
Therefore, you could prompt with
Language and it could generate images.
The ability of AI to be multimodal as
well as able to translate and generate
content enabled the Generative AI revolution.
Generative AI, the ability to generate content
is fundamentally vital. For us to be productive
Well, we've got a new, we are starting a new
wave of AI. and this last couple of years
we've seen enormous progress in AI's ability.
Fundamentally, intelligence
is about understanding.
Perception
Reasoning
Planning a task, how to solve a problem, and
then executing the task, perception, reasoning,
planning. the fundamental cycles of intelligence.
It allows us to apply some previously learned
rules to solve problems we've never seen before.
That's why intelligent people are considered
intelligent to be able to take a complicated
problem, break it down step by step.
Reason about how to solve the
problem, maybe do research.
Maybe go learn some new information.
Get some help, use tools, and solve
problems step by step. Well, the words
that I just described are fundamentally
possible today with what is called Agentic
AI, and I'll show you more in just a second.
In the physical implementation of that,
the embodiment of that Agentic AI.
Now the Generative capability is generating
motion. Instead of generating videos and
generating images or generating text, this AI
generates locomotion, the ability to walk or
reach out and grab something, use tools.
The ability for AI to be embodied in
a physical form is basically robotics.
These capabilities. the fundamental technology
To enable agents, which are basically information
robots and embodied AI, physical robots. These two
fundamental capabilities are now upon us. really,
really exciting times for AI. But it all started.
It all started with GeForce.
And GeForce brought computer
graphics. This is the first accelerated
computing application we had ever worked on.
And it's incredible how far
computer graphics has come.
GeForce brought CUDA to the world, which
enabled machine learning researchers and
AI researchers to advance deep learning.
Then deep learning revolutionized computer
graphics and made it possible for us to
bring computer graphics to a whole new level.
Everything I'm going to show you today.
Everything I'm going to you today,
I'm going to give you a preview of what I'm going
to show you. But everything I'm going to show you
today is computer simulation, not animation.
It's photon simulation, physics simulation,
particle simulations.
Everything is fundamentally
simulation, not animation, not art. It just
looks incredibly beautiful because it turns
out the world is beautiful and it turns out
math is beautiful. So let's take a look.
What do you think?
Numbers in action. Numbers in
action. That's essentially what simulations are
and it's just incredibly beautiful to look at.
But because of the scale.
And the speed by which we
can now simulate almost everything.
We can turn everything into a digital
twin. And because everything can be a digital
twin, it could be designed, planned, optimized,
and operated completely digitally before we
put it into the physical world. The idea that
we would build everything in software is now upon
us. Everything physical will be built digitally.
Everything that's built magnificently
will be built digitally. Everything
that's operated at gigantic scale will be first
built digitally, and there will be digital twins
that operate it. And so today we're going
to talk a lot about digital twins. Well,
what started out as a GeForce graphics
cardAnybody in here know what a GeForce is?
Okay.
Right. Well, what started out as GeForce?
Looks like this now this is the new GeForce. It is
two tons, two and a half tons, 1.2 million parts.
About $3 million
120 KW.
Manufactured in 150 factories.
200 technology partners
working with us to do this.
Probably something along the lines of $40
billion in R&D budget in order to create
what is GB200 and now moving to GB300. It is
completely in production. And this machine was
designed to be a thinking machine. A thinking
machine in the sense that it reasons, it plans.
It spends a lot of time talking to itself.
Just like you do.
We spend most of our time generating words for
our own mind, generating images for our own mind
before we produce it. And so the thinking machine
is really architecturally what Grace Blackwell was
designed to do. It was designed to be one giant
GPU. I compared it to GeForce for a good reason.
GeForce is one GPU, so is GB200. It is one giant
virtual GPU. Now we had to disaggregate it into
a whole bunch of components, create a bunch of
new networking technology and SerDes technology,
incredibly low, low power, high energy efficiency
interconnects to connect all of these chips and
Systems together into one virtual
GPU. This is the hopper version.
This is the world famous hopper system.
Eight GPUs connected together on NVLink.
What's not shown here is a CPU
tray, a CPU tray with dual CPU
and system memory that sits on top together.
This represents one node of an AI supercomputer
About half a million dollars. This is
the hopper system. This is the system
that really put us on the map of AI and It
was under allocation for a very long time.
Because the market took off so quickly. But this
is the famous Hopper system. Well, this entire
system, including the CPU, is replaced by this.
Great Blackwell node. This is one compute tray.
Right here we'll replace that entire
system. It is fully liquid cooled.
And the CPUs are integrated directly
connected to the GPUs. So you could
see it here, two CPUs, four GPUs it is
more performant than that entire system.
But what's amazing is this: we wanted to connect
a whole bunch of these systems together. How would
you connect all of these together was really hard
for us to imagine. So we disaggregated it. What we
did was we took that entire motherboard.
We disaggregated into this and this.
This is the revolutionary NVLink system.
Scaling out computing is not that hard.
Just connect more CPUs with Ethernet. Scaling
out is not hard. Scaling up is incredibly hard.
You can only build as large of
a computer as you can build.
The amount of technology and electronics that
you could fit into one memory Model is incredibly
hard to do. And so what we decided to do was we
create a new interconnect called NVLink. NVLink
is a memory semantics interconnect.
It's a compute fabric, not a network.
It directly connects to the CPU of
all of these different NVLink systems
compute nodes. This is the switch.
Nine of these stand on top. Nine of
it sits on the bottom. In the middle are
the NVLink switches and what connects it
together is this miracle.
This is the NVLink spine.
This is 100% copper.
Copper coax. It directly
connects all of the NVLink chips to all
of the GPUs directly connected over.
This entire spine so that every single one of
the 144 Blackwell dies in 72 different packages
are talking to each other at the same time.
Without blocking all across this NVLink spine,
the bandwidth of this is about
130 terabytes per second.
130. I know.
No, wait for it. wait for it.
130 TBps if it's in bits.
130 TBps
It is more than the data rate of the
peak traffic of the world's entire
internet traffic, on this backplane. Yeah.
So this is how you shrink the internet into
60 pounds.
NVLink
And so we did all that. We did all that
because the way you think about computers
is going to be fundamentally different in the
future and I'll spend more time on this but
it was designed to give Blackwell a giant leap
above hopper. Remember Moore's Law semiconductor
physics is only giving you about two times
more performance every three to five years.
How could we achieve 30, 40 times more
performance in just one generation? And
we need a 30, 40 times more performance because
the reasoning models are talking to themselves.
Instead of one-shot ChatGPT, it's now a reasoning
model and it generates a ton more tokens. When
you're thinking to yourself, you're breaking
the problem down step by step. You're reasoning,
you're trying a whole bunch of different
paths. Maybe it's chain of thoughts,
maybe it's tree of thoughts. Best of end, it's
reflecting on its own answers. You've probably
seen these research models reflecting on the
answer, saying, is this a good answer? Can you do
better than that? And they the, oh yeah, I can do
better than that goes back and thinks some more.
And so those thinking models, reasoning models
achieve Incredible performance, but it requires
a lot more computational capability. And the net
result? NVLink 72 with Blackwells architecture
resulted in a giant leap in performance.
The way to read this is the x axis is how fast
it's thinking. The y axis is how much the factory
can output supporting a whole bunch of users at
one time. And so you want the throughput of the
factory to be as high as possible so you could
support as many people as possible so that the
revenues of your factory is as high as possible.
You want this axis to be as large as possible
because the AI is smarter here than it is here.
The faster it can think, the more it can think
before it answers your question. And so this
has to do with the ASP of the average selling
price of the tokens and this has to do with the
throughput of the factories. These two combined
in that corner is the revenues of the factory.
This factory based on Blackwell can generate a
ton more revenues as a result of the architecture.
It is such an incredible thing what we built.
We made a movie for you just to give you a
sense of the enormity of the engineering
that went into building Grace Blackwell,
take a look.
[video]
Blackwell is an engineering marvel.
It begins as a blank silicon wafer.
Hundreds of chip processing
and ultraviolet lithography
steps build up each of the two hundred billion
transistors layer by layer on a twelve -wafer.
The wafer is scribed into individual
Blackwell dies, tested and sorted,
separating the good dies to move forward.
The chip on wafer on substrate process
attaches 32 Blackwell dies and 128 HBM
stacks on a custom silicon interposer wafer.
Metal interconnect traces are etched directly
into it, connecting Blackwell GPUs and HBM
stacks into each system and package
unit, locking everything into place.
Then the assembly is baked, molded,
and cured, creating the Blackwell
B200 superchip.
Each Blackwell
is stress-tested in ovens at 125 C.
And pushed to its limits for several hours.
Robots work around the clock
to pick and place over 10,000
components onto the Grace Blackwell PCB.
Meanwhile, custom liquid cooling copper
blocks are prepared to keep the
chips at optimal temperatures.
At another facility, ConnectX-7 super NICs are
built to enable scale-out communications and
BlueField-3 DPUs to offload and accelerate
networking, storage, and security tasks.
All these parts converge to be carefully
integrated into GB200 compute trays.
NVLink is the breakthrough high speed link
that NVIDIA invented to connect multiple
GPUs and scale up into a massive virtual GPU.
The NVLink switch tray is constructed with NVLink
switch chips providing 14.4 TBps of all-to-all
bandwidth. NVLink spines form a custom blind mated
backplane with 5,000 copper cables connecting all
72 Blackwells or 144 GPU dies into one giant GPU
delivering 130 TBps of all-to-all bandwidth,
more than the global internet's peak traffic.
From around the world, parts arrive to
be assembled by skilled technicians into
a rack-scale AI supercomputer.
In total 1.2 million components,
2 miles of copper cable, 130 trillion
transistors, weighing nearly two tons.
Blackwell is more than a technological
wonder. It's a testament to the power
of global collaboration and innovation,
fueling the discoveries and solutions
that will shape our future everywhere.
We are driven to enable the geniuses of
our time to do their lifes work and we can't
wait to see the breakthroughs you deliver.
Grace Blackwell Systems
All in production. It is really a miracle.
It's a miracle from a technology perspective.
But the supply chain that came together to build
these GB200 systems, two tons each, we're
producing them now, 1,000 systems a week.
No one has ever produced mass produced
supercomputers at this scale before.
Each one of these racks is essentially
an entire supercomputer. Only in 2018,
the largest Volta system, the Sierra supercomputer
in 2018 is less performant than one of these racks
and that system was 10 MW. This is 100 KW. So the
difference generationally between 2018 and now,
we've really taken supercomputing,
AI supercomputing to a whole new
level and we're now producing these machinery at
enormous scales. And this is just the beginning.
In fact, what you've seen is just one system,
Grace Blackwell. The entire world is talking
of this one system, clamoring for it to get
deployed into the world's data centers for
training and inferencing and Generative AI.
However, not everybody and not every data
center handle these liquid cooled systems.
Some data centers require enterprise stacks,
the ability to run Linux, Red Hat, or or VMware.
Storage systems from Dell, Emc, Hitachi, Netapp,
Vast, Weka, so many different storage systems,
so many different IT systems. And the management
of those has to be done in a way that's consistent
with traditional IT systems. We have so many new
computers to ramp into production and I'm so happy
to tell you that every single one of these are
now in production. You haven't seen them yet.
They're all flying off the shelves, flying off
the ramps, the manufacturing lines starting here.
DGX Spark enables you to have essentially
the Grace Blackwell system on your desktop
in the case of Spark desktop, in the case of DGX
station desk side. This way you don't have to sit
on a supercomputer while you're developing your
software, while you're developing your AI, but
you want the architecture to be exactly the same.
These systems are identical from an architecture
perspective. From a software developer
perspective, it looks exactly the same.
The only difference is scale and speed.
And then on this side are all the X86
systems. The world's IT organizations still
prefer X86 and appreciate X86 wherever they
can take advantage of the most advanced AI
native systems. They do where they can't
and they want to integrate into the enterprise IT
systems, we now offer them the ability to do so.
One of the most important systems and it has
taken us the longest to build because of the
software and the architecture is
so complicated is how to bring.
The AI native architecture and infuse it
into the traditional enterprise IT system.
This is our brand new RTX Pro
server. This is an incredible system.
The motherboard is completely redesigned.
Ladies and gentlemen, Janine Paul.
This motherboard looks so simple.
And yet on top of this motherboard
are eight super NIC switches that connect eight
GPUs across a 200 Gbps state of the art networking
chip that then connects eight of these GPUs and
these Blackwell RTX Pro 6000 GPUs, brand new,
just entered into production.
Eight of these go into a
server. Now what makes it special?
This server is the only server in
the world that runs everything the world has ever
written and everything NVIDIA has ever developed.
It runs AI, Omniverse, RTX for video
games. It runs Windows, it runs Linux,
runs Kubernetes, it runs Kubernetes and VMware.
It runs basically everything. If you want to
stream Windows desktop from a computer to your
to your remote device, no problem. If you want
to stream Omniverse, no problem. If you want
to run your robotics stack, no problem. Just
the QA of this particular machine is insane. The
applications that it runs are basically universal.
Everything the world's ever developed should
run on here, including If you're a video gamer,
including crisis.
And so if you can
run crisis, you can run anything.
Okay, this is the RTX Pro Server
brand new enterprise system.
So something is changing.
We know that AI is incredibly
important technology.
We know for a fact now that AI is software that
could revolutionize, transform every industry.
It can do these amazing things.
That we know
We also know that the way you
process AI is fundamentally
different than the way we used to
process software written by hand.
Machine learning software is developed
differently and it runs differently.
The architecture of the systems, the architecture
of the software are completely different. The way
the networking works is completely different.
The way it acts is completely different.
So we know that the technology can do
different things, incredible things.
It's intelligent. We also know that
it's developed in a fundamentally
different way and needs new computers.
The thing that's really interesting,
is what does this all mean, to
countries, to companies, to society.
And this, this is an observation that
we made almost a decade ago that now
everyone is awakening to, that in fact these
AI data centers are not data centers at all.
They're not data centers in
the classical sense of a data
center storing your files that you retrieve.
These data centers are not storing our files.
It has one job and one job only: to produce
intelligent tokens, the generation of AI.
These factories of AI look like
data centers in the sense that
they have a lot of computers inside.
But that's where everything breaks down.
How it's designed, the scale at
which its manufactured or scaled,
designed and built and how it's used.
And how it's orchestrated and
provisioned, operated.
How you think about it?
For example, nobody really thinks about their
data center as a revenue generating facility.
I said something that everybody
goes, yeah, I think you're right.
Nobody ever thinks about a data center as a
revenue-generating facility, but they think
of their factories, their car factories, as
revenue-generating facilities. And they can't
wait to build another factory because whenever
you build a factory, revenue grows shortly
after you could build more things for more people.
Those ideas are exactly the same ideas in these AI
factories. They are revenue-generating facilities
and they are designed to manufacture tokens.
And these tokens can be reformulated into
productive intelligence for so many industries
that AI factories are now part of a country's
infrastructure which is the reason why you see
me running around the world talking to heads of
state, because they all want to have AI factories.
They all want AI to be part of their
infrastructure. They want AI to be a
growth manufacturing industry for them.
And this is genuinely profound.
And I think we're talking about,
as a result of all that, a new
industrial revolution because every single
industry is affected, and a new industry.
Just as electricity became a new industry.
At first when it was described as a
technology and demonstrated as a technology.
It was understood as a technology, but then we
understood that it's also a large industry.
Then there's the information in industry.
Which we now know as the internet. And both of
them, because it affected so many industries,
became part of infrastructure.
We now have a new industry.
An AI industry and it's now part of the
new infrastructure called intelligence
infrastructure. Every country, every society,
every company will depend on it. And you could
see its scale. This is one that's being talked
about a lot. This is Stargate. This doesn't look
like a data center. It looks like a factory. This
is 1 GW. It will hold about 500,000 GPU dies.
And produce an enormous amount of
intelligence that could be used by everybody.
Well, Europe has now awakened to the
importance of these AI factories,
the importance of the AI infrastructure, and
I'm so delighted to see so much activity here.
This is European telcos building
AI infrastructure with NVIDIA.
This is the European cloud service providers
building AI infrastructure with NVIDIA.
And this is the European Supercomputing
centers, building next generation AI
supercomputers and infrastructure with NVIDIA.
And this is just the beginning. This is.
In addition to what will come in the
public clouds. This is in addition to
the public clouds. So indigenous-built AI
infrastructure here in Europe by European
companies for the European market.
And then there's 20 more being planned.
20 more AI factories and several
that are gigawatt factories.
In total in just two years we will
increase the amount of AI computing
capacity in Europe by a factor of ten.
And so the researchers, the startups:
your AI shortage, your GPU shortage will be
resolved for you soon. It's coming for you.
Now we're partnering with each country to
develop their ecosystem. And so we're building AI
technology centers in seven different countries.
And the goal of these AI technology centers
is one, to do collaborative research
To work with the startups and also to build
the ecosystem. Let me show you what an ecosystem
looks like in the Uk. I was just there yesterday.
The ecosystems are built on top of
the NVIDIA stack. So for example.
Every single NVIDIA. As you know,
NVIDIA is the only AI architecture
that's available in every cloud.
It's the only computing architecture
aside from X86 that's available everywhere.
We partner with every cloud service provider.
We accelerate applications from the most important
software developers and in the world, Siemens
here in Europe, Cadence, Red Hat, Servicenow.
We've reinvented the computing stack. As you know,
computing is not just a computer, but
it's compute, networking, and storage.
Each one of those layers, each one of those stacks
has been reinvented. Great partnership with Cisco,
who announced a brand new model yesterday
at their conference based on NVIDIA. Dell.
Great partnerships. Netapp. Nuts. A
whole bunch of great partnerships.
As I mentioned earlier, the way you develop
software has been fundamentally changed.
It's no longer just write C programs,
compile C programs, deliver C programs.
It's now Devops, MLOps, AI Ops. So that
entire ecosystem is being reinvented and
we have ecosystem partners everywhere
and then of course solution integrators
and providers who could then help every
company integrate these capabilities.
Well, here in the UK we have special companies
that we work with, really terrific companies
from researchers to developers, to partners to
help us upskill the local economy and upskill
the local talent, enterprises that consume the
technology, and of course cloud service providers.
We have great partners in the UK,
We have great partners in Germany.
Incredible, incredible partnerships in Germany. We
have partnerships in Italy, and we of course have
amazing partnerships here in France.
That's right, go France.
President Macron is going to be here later on.
We're going to talk about some new announcements.
So we have to show some enthusiasm for AI.
Okay. Yeah. There you go. Show
him some enthusiasm.
So great partnerships.
Here in France, one particular one I want to
highlight is our partnership with Schneider.
Building, even building these AI factories,
we build them digitally now. We design them
digitally, we build them digitally, we operate
them or optimize them digitally, and we will
even eventually optimize them and operate
them completely digitally in a digital twin.
AI factories are so expensive.
$50 billion sometimes.
$100 billion in the future.
If the utilization of that factory is
not at its fullest, the cost to the factory
owner is going to be incredible. And so we
need to digitalize and use AI wherever we can.
Put everything into Omniverse so that we have
direct and constant telemetry. We have a great
partnership here that we're announcing today.
Young company , a CEO. I really like, and
he's trying to build a European AI company.
The name of the company is Mistral. Today
we're announcing that we're going to build
an in AI cloud together here to deliver their
models as well as deliver AI applications for
the ecosystem of other AI startups so
that they can use the Mistral models,
or any model that they like. And so with Mistral
we are going to be partnering to build a very
sizable AI cloud here. And we'll, we'll talk about
more of it later on today with President Macron.
AI technology is moving at light speed.
And what I'm showing you here, proprietary
models on the left moving at light speed.
However, the open models are also moving
at light speed, only a few months behind.
Whether it's Mistral, Llama, DeepSeek R1,
R2 coming, Qwen. These models are all exceptional.
Every single one of them is exceptional. And
so we've dedicated ourselves over the
last several years to apply some of the
world's best AI researchers to make those AI
models even better and we call that Nemotron.
Basically, what we do is we take the models
that are open-sourced, and of course they're
all built on NVIDIA anyhow, and we take those
open-sourced models, we then post-train it.
We might do neural architecture search.
We might do neural architecture search,
provided with even better data.
Use reinforcement learning techniques, enhance
those models, give it reasoning capabilities.
Extend the context so that it could learn and
read more before it interacts with you.
Most of these models have relatively
short context, and we wanted to have enormous
context capability because we want to use it in
enterprise applications where the conversation
we want to have with it is not available on the
internet. It's available in our company, and so
we have to load it up with an enormous amount
of context. All of that capability is then
packaged together into a downloadable NIM.
You could come to NVIDIA's website and literally
download an API, a state of the art AI model.
Put it anywhere you like and
we improve it tremendously.
This is an example of Nemotron improvement over
Llama. So Llama 8B, 70B, 405B improved by our
post-training capability, extension of reasoning
capability, all the data that we provide. We
enhanced it tremendously. We're going to do this
generation after generation after generation. And
so for all of you who use Nemotron, you will
know that there's a whole slew of other models
in the future, and they're open anyway, so if
you would like to start from the open model,
that's terrific. If you'd like to start with
the Nemotron model, that's terrific. And the
Nemotron models, the performance is excellent.
In benchmarks after benchmarks after benchmarks,
Nemotron performance has top of the leaderboard
all over the place. And so now you know that you
have access to an enhanced open model that is
still open, that is top of the leader charts.
And you know that NVIDIA is dedicated
to this. And so I will do this for
as long as I shall live. Okay.
This strategy is so good. This
strategy is so good that the regional model
makers, the model builders across Europe,
have now recognized how wonderful the strategy is
and we're partnering together to adapt, enhance
each one of those models for regional languages.
Your data belongs to you. Your data belongs
to you. It is the history of your
people, the knowledge of your people,
the culture of your people. It belongs to you.
And for many companies, in the case of NVIDIA,
our data is largely inside. 33 years of data.
I was looking up this morning Siemens 180 years of
data. Some of it written down on papyrus. Roland
Bush is here. I thought I'd pick on Roland Bush,
my good friend. And so you'll have to
digitize that before the AI can learn.
And so the data belongs to you. You should use
that data. Use an open model like Nemotron and
all the tool suites that we provide so
that you can enhance it for your own use.
We're also announcing that we have a great
partnership with Perplexity. Perplexity as a
reasoning search engine.
Yep.
The three models I use, I use ChatGPT, Gemini Pro,
and Perplexity, and these three models are used
interchangeably. And Perplexity is fantastic.
We're announcing today that Perplexity will
take these regional models and connect it
right into Perplexity so that you could now
ask and get questions in the language, in the
culture and the sensibility of your country.
Okay, so Perplexity regional models.
Agentic AI. Agentic AI. Agents is a very big deal.
You know, in the beginning with pre-trained
models, people said, but it hallucinates. It
makes things up. You're absolutely right.
It doesn't have access to the latest news
and data information.
Absolutely right.
It gives up without reasoning through problems.
It's as if every single answer has to be memorized
from the past. You're absolutely right.
All of those things, you know, why is it
trying to figure out how to add or count the
count numbers and add numbers? Why doesn't
it use a calculator? You're absolutely right.
And so all of those capabilities associated with
intelligence, everybody was able to criticize,
but it was absolutely right because everybody
largely understands how intelligence works.
But those technologies were being built all around
the world and they were all coming together, from
retrieval augmented generation, to web search,
to multimodal understanding so that you can read
PDFs, go to a website, look at the images and the
words, listen to the videos, watch the videos.
And then take all of that understanding into
your context. You could also now understand,
of course, a prompt from almost anything.
You could even say, I'm going to ask you
a question, but start from this image. I
could say start from this text.
Before you answer the question
or do what I ask you to do.
It then goes off and reasons
and plans and evaluates itself.
All of those capabilities are now
integrated and you can see it coming out into
the marketplace all over the place. Agentic AI
is real. Agentic AI is a giant step function
from one-shot AI. One-shot AI was necessary to
lay the foundation so that we can teach the
agents how to be agents. You need some basic
understanding of knowledge and basic understanding
of reasoning to even be able to be teachable. And
so pre-training is about the teachability of AI.
Post-training reinforcement learning, supervised
learning, human demonstration,
context provision, generative AI.
All of that is coming together to
formulate what is now Agentic AI.
Let's take a look at one example. Let me show
you something. It's built on Perplexity and
it's super cool.
[video]
AI agents are digital assistants. Based on
a prompt, they reason through and break down
problems into multi-step plans. They use
the proper tools, work with other agents,
and use context from memory to properly execute
the job on NVIDIA accelerated systems. It starts
with a simple prompt. Let's ask Perplexity
to help start a food truck in Paris. First,
the perplexity agent reasons through the
prompt and forms a plan. Then calls other
agents to help tackle each step using
many tools. The market researcher reads
reviews and reports to uncover trends
and analyze the competitive market.
Based on this research, a concept designer
explores local ingredients and proposes a menu
complete with prep time estimates. It researches
palettes and generates a brand identity. Then the
financial planner uses Monte Carlo simulations to
forecast profitability and growth trajectory. An
operations planner builds a launch timeline with
every detail, from buying equipment to acquiring
the right permits. The marketing specialist
builds a launch plan with a social media
campaign,and even codes an interactive website
including a map, menu, and online ordering. Each
agent's work comes together in a final package
proposal, and it all started from a single prompt.
One prompt, one prompt like that in the original
chatbot would have generated a few hundred tokens,
but now with that one single prompt
into an agent to solve a problem,
it must have generated 10,000 times more tokens.
This is the reason why Grace Blackwell is
necessary. This is the reason why we need
performance and the systems to be so much
more performant generationally. Well, this is how
Perplexity builds their agents. Every company will
have to build their own agents. It's is terrific,
you're going to be hiring agents from OpenAI and
Gemini and Microsoft CoPilot and
Perplexity and Mistral and they'll
be agents that are built for you.
And they might help you plan a vacation or
you know, go do some research, so on, so forth.
However, if you want to build a company, you're
going to need specialized agents with specialized
tools and using specialized tools and specialized
skills. And so the question is, how do you build
those agents? And so we created a platform for
you. We created a framework, and a set of tools
that you can use and a whole bunch of partners
to help you do it. It starts with on the very
bottom, on the very bottom, the ability to have
reasoning models that I spoke about.
NVIDIAs Nemotron reasoning Large
Language models are world-class.
We have NeMo Retriever, which is
a multimodal search engine, semantic search
engine, incredible. And we built a blueprint,
a demonstration that is operational, that is
essentially a general agent. We call it AI-Q.
And on top we have a suite of tools
that allows you to onboard an agent,
a general agent, curate data to teach it,
evaluate it, guardrail it, supervised train it,
use reinforcement learning all the way to
deployment, keep it secure, keep it safe.
That suite of toolkits is integrated, those
libraries are integrated, into the AI ops
ecosystem. You can come and download it from
our website yourself as well, but it's largely
integrated into AI ops ecosystem. From that,
you could create your own special agents.
Many companies are doing this. This is Cisco.
They announced it yesterday. We're building AI
platforms together for security.
Now look at this.
AI agents, and not one model that does all of
these amazing things, are a collection or a
system of models. It's a system of large
language models. Some of them are optimized
for certain types of things. Retrieval, as I
mentioned, performing skills, using a computer.
You don't want to bundle all of that stuff up
into one giant mass of AI, but you break it up
into small things that you could then deploy CICD
over time. This is an example of Cisco's. Now the
question is how do you now deploy this?
Because as I mentioned earlier,
there are public clouds where NVIDIAs
compute is, there are regional clouds,
we call them NCPs here, for example Mistral.
You might have something that is a private cloud,
because of your security requirements
and your data privacy requirements.
You might even decide that you
have something on your desk.
And so the question is, how do you run all of
these, and sometimes theyre running in different
places because these are all microservices.
These are AIs that could talk to each other,
they could obviously talk to each other over
networking. And so how do you deploy all of
these microservices? Well, we now have a great
system. I'm so happy to announce this for you.
This is called our DGX Lepton. DGX
Lepton. What you're looking at here
is a whole bunch of different clouds. Here's
the Lambda cloud, the AWS cloud, you know,
here's your own developers machine. Your
own system could be a DGX station. Nebius,
Yoda, and Scale. It could be AWS, it could
be GCP. NVIDIAs architecture is everywhere.
And so you could decide where you would like
to run your models. You deploy it using one
super cloud. So it's a cloud of clouds. Once
you get it to work, once you get these NIMs
deployed into Lepton, it will go and be hosted
and run on the various clouds that you decide.
One model architecture, one deployment and you
can run it everywhere. You can even run it on
this little tiny machine here.
You know, this DGX Spark.
Is it cafe time?
Look at this
This lift is 2000 horsepower.
This is my favorite little machine.
DGX Spark, the firstthe AI supercomputer.
We built an AI supercomputer in 2016. It's
called the DGX-1. It was the first version.
everything that I've been talking about.
Eight Volta GPUs connected with NVLink.
It took us billions of dollars to build.
And on the day we announced it, DGX-1, there
were customers, no interest, no applause.
100% confusion
Why would somebody build a computer like that?
Does it run windows? Nope.
And so we built it anyway. Well,
thankfully, a young company, a startup, a
nonprofit startup in San Francisco was so
delighted to see the computer. They said, can we
have one? And I thought, oh my gosh, we sold one,
but then I discovered it was a non-profit.
But I put a computer, put a DGX-1 in my car,
and I drove it up to San Francisco.
And the name of that company is OpenAI.
I don't know what the life lesson is there.
There are a lot of non profits. You know,
so next time, next time maybe the lesson is this:
if a developer reaches out to you a needs a GPU,
the answer is yes. And so that's right.
So imagine, you have Lepton. It's in your
browser and you have this this helm chart. An AI
agent that you've developed and you want to run
it here and parts of it you want to run in AWS and
parts of it you want to run you know in a regional
cloud somewhere. You use Lepton, you deploy
your helm chart, and it magically shows up here.
Okay, and so if you would like to run it
here until you're done with it and ready
to deploy it and then deploy it into the
cloud, terrific. But the beautiful thing is
this architecture is based on Grace Blackwell
GB10 versus GB200, vs GB300, and all of these
different versions of this architecture is
exactly Grace Blackwell. Now this is amazing.
So we're doing this for Lepton,
but, next, HuggingFace and NVIDIA
have connected Lepton together.
And so whenever you're training
a model on HuggingFace, if you would like to
deploy it into Lepton and directly into Spark,
no problem. It's just one click. So
whether you're training or inferencing,
we are now connected to HuggingFace and Lepton
will help you decide where you want to deploy
it. Let's take a look at it.
[video]
Developers need easy and reliable access
to compute that keeps up with their work.
Wherever they are, whatever they're
building. DGX Cloud Lepton provides on
demand access to a global network of GPUs
across clouds, regions, and partners like
Yoda and Nebius. Multi cloud GPU clusters are
managed through a single unified interface.
Provisioning is fast. Developers can
scale up the number of nodes quickly
without complex setups and start training
right away with pre-integrated tools and
training-ready infrastructure. Progress is
monitored in real time. GPU performance,
convergence and throughput are at your fingertips.
You can test your fine-tuned models right within
the console. DGX cloud Lepton can deploy NIM
endpoints or your models in multiple clouds
or regions for fast distributed inference. Just
like ride-sharing apps connect riders to drivers,
DGX Cloud Lepton connects developers to GPU
compute, powering a virtual global AI factory.
DGX cloud Lepton.
Okay, so that's Cisco. This is the way SAP,
they're building an AI platform onNVIDIA. Sana
is building an AI business application automation
on NVIDIA. DeepL is building their language
framework and platform on NVIDIA. AI Photo Room,
a video editing and AI editing platform is
building their platform on NVIDIA. And this
is Qodo, I think used to be Qodium, incredible
coding agent built on NVIDIA. And this is Aiola,
a voice platform built on NVIDIA. And this one
is a clinical trial platform, the world's largest
automation platform for clinical trials,
built on NVIDIA. And so all of these,
all of these basically build on the same idea.
NIMs that encapsulate and package up in a virtual
container that you could deploy anywhere,
the Nemotron large language model or other
large language models like Mistral or others.
We then integrate libraries that basically covers
the entire lifecycle of an AI agent. The way you
treat an AI agent is a little bit like a digital
employee. So your IT department would have to
onboard them, fine-tune them, train them, evaluate
them, keep them guardrailed, keep them secure
and continuously improve them. And that entire
framework platform is called NeMo. And all of
that is now being integrated into one application
framework after another all over the world.
This is just an example of a few of them. And then
now we make it possible for you to deploy them
anywhere. If you want to deploy it in the cloud,
you got DGX, you got GB200s in the cloud. If you
want to deploy it on-prem, because you've got
VMware or Red Hat Linux or Neutonix and you want
to deploy it in your virtual machines on-prem,
you can do that. If you wanted
to deploy it as a private cloud,
you could do that. You can deploy it all the way
on your DGX Spark or DGX Station. No problem.
And so Lepton will help you do all of that.
Let's talk about industrial AI.
This is one of my favorite moments. This is Roland
Bush. He just, this is a really fun moment. He
wanted to remind me that neural computers, neural
network computers, were invented in Europe.
That's this whole slide.
I just, it was such a great moment. This is
Synapse1. This is incredible, you guys. Synapse1.
This is Synapse1, 1992. It runs neural networks
8000 times faster than CPUs at that time.
Isn't it incredible? So this
is the world's AI computer.
And Roland just wants to just. Never forget that,
Jensen. Never, ever forget that. I said, okay,
all right, I'll tell and I'll even tell everybody.
Siemens, 1992.
Siemens, 1992. We have a great partnership with
Siemens and Roland Bush, the CEO, is supercharging
the company so that they could leap, completely
leap the last, IT industrial revolution and
fuse the industrial capabilities of Europe, the
industrial capabilities and might of Siemens with
artificial intelligence and create what is called
the industrial AI revolution. We're partnering
with Siemens on so many different fronts.
Everything from design to simulation
to digital twins of factories,
to operations of the AI in the factories.
Everything from end to end. And it just
reminds us, it reminds me how incredible
the industrial capabilities of Europe are.
And what an extraordinary opportunity this is
for you. What an extraordinary opportunity.
Because AI is unlike software. AI is really smart
software. And this smart software can finally
do something that can revolutionize
the very industries that you serve.
And so we made a love letter
video if you will. Let's play it.
[video]
It began here.
The first industrial revolution.
Watts steam engine and the mechanized loom.
Introduced automation
And the advent of factories.
And industry was born
The age of electricity.
Ampere unraveled electromagnetism.
Faraday built the first electric generator.
And Maxwell laid the foundations
For modern electrical engineering.
Siemens and Wheatstones Dynamo.
The engine of electricity.
Bringing machine, trains,
factories and cities to life.
Electrifying the planet.
Igniting modern manufacturing
And today, born out of the
computing and information age.
The 4th Industrial Revolution.
The age of AI.
Reimagining every part of industry.
Across the continent,
industrial AI is taking hold.
From design to engineering.
You're blazing new trails toward
understanding and reinvention.
You brought the physical world into the virtual.
To plan and optimize
The world's modern factories.
You're building the next frontier.
Where everything that moves is robotic.
Every car an intelligent autonomous agent.
And a new collaborative workforce to
help close the global labor shortage gap.
Developers across the continent
are building every type of robot.
Teaching them new skills.
In digital twin worlds and robot gyms.
Preparing them to work alongside us.
In our factories.
Warehouses
The operating room.
And at home.
The 4th industrial revolution is here.
Right where the first began.
What do you think?
I love that video.
You made it. That's so great. You made it.
Well, we're working on industrial AI with one
company after another. This is a BMW. Building
their next generation factory in Omniverse.
This isI don't know how to say it. Can somebody
teach me? Sounds good. Exactly. That's exactly
right. Good job. Good job. That's exactly right.
They're building, of course, their plants,
digital twins and Omniverse is key. Their
digital twin for warehouse logistics.
This is Mercedes Benz and their digital twins
of their factories built in Omniverse. This is
Schaeffer and their digital twin of
their warehouse built in Omniverse.
This is your train station here in France
building a digital twin of their train stations in
Omniverse. And this is Toyota building a digital
twin of their warehouse in Omniverse. And when,
when you build these warehouses and these
factories and Omniverse, then you could design it,
you can plan it, you can change it.
In green-field its wonderful,
in brown-field it's wonderful. You could simulate
its effectiveness before you go and physically
lift and move things around to discover it wasn't
optimal. And so the ability to do everything
digitally in a digital twin is incredible.
But the question is why does the digital twin have
to be photoreal and why does it have to obey the
laws of physics? The reason for that is because
we wanted ultimately to be a digital twin where a
robot could learn how to operate as a robot. And
robots rely on photons for their perception system
and those photons are generated through Omniverse.
And a robot needs to interact with the physical
world so that it could know whether it's doing
the right things and learn how to do it
properly. And so these digital twins have
to look real and behave realistically. Okay,
so that's the reason why Omniverse was built.
This is fantastic. This is a fusion reactor
digital twin. Incredibly complicated piece of
instrument as you know, and without AI the next
generation fusion reactor would not be possible.
We're announcing today that we are going to
build the world's first industrial AI cloud
here in Europe.
Yep.
These industrial AI clouds are, yes, a whole
lot of computers in the cloud. However,
the requirements, its performance, its safety
requirements fundamentally different. And so I'm
going to tell you a lot more about it on Friday.
I'm only teasing you part of the story today,
but this industrial cloud will be
used for design and simulation.
Virtual wind tunnels that you just walk
into. Virtual wind tunnels you just move
a car into and you see it behave. Open
doors, open windows, change the design.
All completely in real time. Design in real
time, simulate in a digital wind tunnel,
digital twin of a wind tunnel in real time,
build it in a factory of a digital factory
twin in real time. All of this, and the robots
learn how to be great robots and build the
robots of our future, self driving cars and such.
We already have a tremendous ecosystem here. We've
been here, as you know, for a very long time.
NVIDIA is 33 years old. The first time we came
to Europe was during the time when workstations
and the digitalization of products, CAD, the CAD
revolution started. We were here during the CAE
revolution and now the digital twin revolution.
Some $2 trillion of ecosystem here in
Europe that we partner with and that have
the privilege of supporting. What comes out
of that is a new revolution that's happening.
As you know, everything that moves will be
robotics, everything that moves will be AI-driven,
and the car is the most obvious next one.
NVIDIA builds the AI supercomputers
to train the model.
The AI supercomputer
for Omniverse digital twins. We also build
the AI supercomputers for the robot itself.
In every single case, whether it's in the cloud
for Omniverse or in the car, we offer the entire
stack, the computer itself, the operating
system that runs on top of this computer,
which is different in every single case.
This computer, high-speed, sensor-rich,
must be functionally safe. In no
circumstance could it fail completely.
And so the safety requirements are incredibly
high. And now we have an incredible model that
sits on top of it. This model that sits on top
of it is a transformer model. It's a reasoning
model. It takes sensor in, you tell it what
you want it to do, and it will drive you there.
Takes pixels in and it generates path
plans output. So it's a generative AI
model based on transformers.
Incredible technology
NVIDIAs AI team, AV team, is incredible.
This is the only team that I know that has
won end-to-end self-driving car
challenge at CVPR two years in a
row. And so they're the winner again this year.
Let's take a look at the video. Yep, thank you.
[video]
Let's take a look at what they do.
Like any driver, autonomous vehicles
operate in a world full of unpredictable
and potentially safety critical scenarios.
NVIDIA Drive built on the Halos safety system.
Lets developers build safe autonomous
vehicles with diverse software stacks
and sensors and redundant computers.
It starts with training. Safe AVs need massive
amounts of diverse data to be able to address
edge cases, but real world data is limited.
Developers use NVIDIA Omniverse and
Cosmos to reconstruct the real world
and generate realistic synthetic training
data to bring diversity to the AV model.
The model can perceive and
reason about its environment.
Predict future outcomes.
And generate a motion plan.
And for decision making diversity, an independent
classical stack runs in parallel.
Guardrails monitors safe performance.
And in cases of anomalies, calls the
arbitrator to make an emergency stop.
Further diversity and redundancy are built
into the sensor and compute architecture.
Each sensor connects to redundant computers,
so even if a sensor or computer fails,
the vehicle stays safe and operational.
And in the event of a critical failure,
the system can execute a minimum risk
maneuver like pulling over to the shoulder.
Safety is foundational to autonomous driving.
NVIDIA Drive lets developers worldwide
integrate Halos into their own products
to build the next generation of safe AVs.
A billion cars on the road, 10,000 miles
a year on average, ten trillion miles.
The future of autonomous
driving is obviously gigantic.
And it's going to be driven,
it's going to be powered by AI.
This is, this is the next gigantic opportunity.
And we're working with enormous companies and
really fantastic companies around
the world to make this possible.
At the core of everything we do here
with AV is safety and we're really,
really proud of our Halos system.
It starts with the architecture of the
chip and then the chip design and the systems
design, the operating systems, the AI models,
the methodology of developing the software, the
way we test it, everything from the way we train
the models, the data we provide for the models,
all the way to the way we evaluate the models.
NVIDIAs Halos system and our AV safety team
and capabilities are absolutely world-renowned.
This computer was the first one to be
software-defined, the world's first software
defined completely 100% software-defined, AI
driven stack for AVs. We've been at this now for
coming up on ten years. And so this capability
is world-renowned and I'm really proud of it.
The same thing that's happening for
cars is happening for a new industry.
As I mentioned earlier, if you
can generate video from prompts.
AI can perceive, it can reason.
It can generate videos and words
and images. And just now with cars,
the path, the steering wheel path.
Why can't it also generate locomotion
abilities and articulation abilities?
So that fundamental ability for AI to
revolutionize one of the hardest robotics
problems is around the corner. Humanoid
robots are going to be a thing. We now
know how to build these things, train
these things, and operate these things.
Human robotics is going to potentially
be one of the largest industries ever.
And it requires companies who
know how to manufacture things.
Manufacture things of extraordinary capabilities.
This speaks of the European countries. So much
of the world's industries are based here. I
think this is going to be a giant opportunity.
Well, let's say it's a billion
robots around the world.
The idea that there'd be a billion robots is a
very sensible thing. Now why hasn't it happened?
Well, the reason for that is simple.
Today's robots are too hard to program.
Only the largest companies can afford
to install a robot, get it, to teach it,
program it to do exactly the right things.
Keep it sufficiently surrounded so that it's safe.
That's the reason why the world's largest car
companies have robots. They're large enough.
The work is sufficiently repetitive.
The industry is at a sufficient scale
that you could deploy robots into those
factories. Almost everybody who is a middle,
or small and medium companies, or mom and
pop restaurants or stores or warehouses, it's
impossible to have that programming capability.
Until now. We're going to give you essentially
robots where you could teach them. They'll
learn from you. Just as we were talking about
Agentic AI, we now have humanoid AI that can learn
from your teaching using toolkits that are very
consistent with the NeMo tools I spoke about.
NVIDIA here as well is built in a three-layer
stack. We build the computer, the Thor
computer dev kit. looks a little bit like this.
This is a robotic computer, completely
self-contained dev kit sits on your desk.
These are all the sensors and inside
is a little supercomputer Thor chip.
Really, really incredible. And these. Yep.
Yeah. I could imagine getting one of these
inserted like that. Okay, thank you.
So that's the Thor processor. On top
is an operating system designed for robotics.
And on top of that, transformer models that
take sensor data and instructions and transforms
them and generates paths and motor controls for
arm articulation, finger articulation,
and of course your legs articulation.
Now the big challenge of human robotics is the
amount of data necessary to train. It is very,
very hard to get. And so the question is, how do
you do that? Well, the way you solve that problem
is to back in Omniverse, a digital twin world
that obeys the laws of physics. And this is an
incredible piece of work that we're doing.
Don't do it. Don't
Oh. My fault
Okay, these are robots.
We developed computers to train them, computers
to simulate them and the computer that goes
inside them. There's a whole bunch of human
robotics companies being built around the world.
They all see the great opportunity to
revolutionize this new device if you will. And
the progress is going incredibly fast. And the
way that they all learn is they learn in a virtual
world. and this virtual world has to obey the
laws of physics. And recently we announced a big
partnership with Disney Research and DeepMind,
and we're going to work together to create the
world's most sophisticated physics simulation.
And I'm just trying to figure out at this point
how to go to that slide. Teach me who's with me.
This is what happens when you only rehearse once.
Okay, so this this. Incredible system This
incredible system is where an AI learns how to
be an AI. Let me show it to you.
[video]
We have a special guest.
Your name is Grek.
Are you a petite garson or petite belle?
Okay. Grek is a little girl.
Now look at this. Grek learned
how to walk inside Omniverse.
Obeying the laws of physics.
But inside Omniverse, we created
hundreds of thousands of scenarios.
Then finally, when Grek learned how
to operate and walk and manipulate in those
environments on sand and on, you know, on gravel,
slippery floors, on concrete, on carpet.
Then when it comes, when Greg comes into
the physical world, the physical world
is just the 100,001 version of the world.
And so you learn how to walk the
virtual world and look at you now.
Can you, can you jump? Wow.
Can you dance?
Well, I think, I think, um. I just want to
let you know I am the keynote presenter.
So I need you, I need you to behave. I
need you to behave for a few seconds.
I need you to behave for a few
seconds. Could you sit? Sit.
Hey, you know what we should do?
Let's take a picture of everybody.
Yeah. Bam. bam.
Would you like to come home with me?
Would you like to come home with me? I got.
Yeah, I know. Yeah, I have pets. They
would like to have you as a pet.
No? No. You're so smart. You're
so smart. Well. Incredible right?
Grek you are the world's best robot
and someday we'll all have one like
you and they'll follow us around.
But if I need a glass of whiskey, you're going
to have to go tell somebody else to go get me a
glass of whiskey because you have no arms.
Yeah. You're so cute. Okay little girl,
you stay here for a second. Let's wrap up.
Alright. It's very clear. It's very clear an
industrial revolution has started. The
next, the next wave of AI has started.
Grek is a perfect example of what's possible
now with robotics. The technology necessary to
teach a robot. To manipulate to simulate.
And of course the manifestation of an
incredible robot is now right in front of us.
We have physical robots and we have information
robots. We call them agents. So the next wave of
AI has started. It's going to require inference
workloads to explode, it's basically going to
go exponential. The number of people that are
using inference has gone from 8 million to 800
million, a hundred times in just a couple of
years. The number, the amount of prompts that,
the tokens generated, as I mentioned earlier,
from a few hundred tokens to thousands of tokens.
And of course we use AI even more than ever today.
So we need a special computer designed
for thinking, designed for reasoning. And
that's what Blackwell is, a thinking machine.
These Blackwells will go into new types of
datacenters, essentially AI factories designed
for one thing and one thing only. And these AI
factories are going to generate tokens, and these
tokens are going to become your food, little Grek.
I know, I know.
And what's really incredible,
I'm so happy to see that Europe is going all
in on AI. The amount of AI infrastructure
being built here will increase by an order
of magnitude in the next couple of years.
I want to thank all of you for your
partnership. Have a great VivaTech. Thank you.
Say bye bye. Say bye bye. Take a bunch of
pictures. Take a bunch of pictures. Take a
bunch of pictures.
Yeah.

Ladies and gentlemen,
I have a very special guest.
But could I ask everybody to sit down?
We're about to get started.
My next,
my next guest.
I am so impressed by this person.
Three reasons.
First reason
is there are only a handful
of entrepreneurs, founders
that started a company
that literally touched the lives
of billions of people around the world
as part of the social fabric,
invented services,
and a state-of-the-art computing company.
Two. Very few entrepreneurs, founders,
founded the company and led it to over
$1 trillion of value.
And three, a college dropout.
All three things simultaneously true.
Ladies and gentlemen,
please help me welcome Mark Zuckerberg.
How's it going?
Welcome.
Mark, welcome to your first Siggraph.
All right.
Can you believe this?
One of the pioneers of computing.
A driver of modern computing.
And I had to invite him to Siggraph.
So, anyways, Mark, sit down.
It's great to have you here. Welcome.
Thanks for flying down.
Yeah. No, this will be fun.
I hear youve been going for, like,
five hours already or something.
Well, yeah, sure.
This is Siggraph.
You know, there's 90% PhDs. And so
the thing that's really great
about Siggraph, as you know,
this is
this is the show of computer graphics,
image processing,
artificial intelligence
and robotics combined.
And some of the some of the companies
that over the years have demonstrated
and revealed
amazing things here from Disney, Pixar,
Adobe, Epic Games.
And of course, you know, NVIDIA.
We've done a lot of work here.
This year
we introduced 20 papers
at the intersection
of artificial intelligence and simulation.
So we're using artificial
intelligence to do,
help simulation,
be way larger scale, way faster.
For example, differentiable physics.
we're using simulation to create,
simulation environments for synthetic data
generation, for artificial intelligence.
And so these two areas are really coming
together.
Were really proud of the work
that we've done here. At Meta,
you guys have done amazing AI work.
I mean, one of the things that,
that I find amusing is, when the press
writes about how Meta has jumped into AI
this last couple of years, as if,
you know, the
work that the FAIR has done.
remember,
we all use PyTorch, that comes out of Meta,
the work that you do in computer
vision
the work in language models, real-time
translation.
groundbreaking work.
I guess my first question for you is,
how do you see how the,
the advances
of generative AI at Meta today?
And how do you apply it to either
enhance your operations
or introduce new capabilities
that you're offering?
Yeah.
So a lot to unpack there.
First of all, really happy to be here.
you know, Meta has done a lot of work
and, has been at Siggraph for,
you know, eight years.
So, I mean, it's a, you know, we're noobs
compared to you guys.
But, I know, I think it was back in in 2018.
You're dressed right, but this is my hood.
I just, you know, it's I mean, well,
thank you for welcoming me to your hood.
I think it was back in 2018.
We showed the some of the early 
hand-tracking work
for our VR and mixed reality headsets.
You know, I think we've talked a bunch
about the progress that we're making on
codec avatars, the photorealistic avatars
that we want to be able
to drive from consumer headsets,
which we're getting closer and closer to,
so pretty excited about that.
And also, a lot of the display systems
work that we've done.
So, some of the future prototypes
and research for getting
the mixed reality headsets to be able
to be really thin with, like with just,
pretty advanced optical stacks
and display
systems, the integrated system - I mean
that's stuff that we've typically
shown here first.
So, excited to be here.
You know, this year not just talking
about the metaverse stuff, but also,
all the AI pieces, which, as you said,
I mean, we started FAIR,
the AI research center.
you know, back then it was Facebook.
Now, Meta. Before we started Reality Labs.
I mean, we've been at this for
for a while.
All the stuff around
gen AI,
it's an interesting revolution.
And I think that it's 
going to end up making,
I think all of the different products
that we do,
you know, different 
in interesting ways.
I mean, I kind of go through -
you can look at the big product lines
that we have already,
so things like the feed
and recommendation systems and Instagram
and Facebook
and we've kind of been on this journey
where that's gone from just being about
connecting with your friends
and, the ranking was always important
because even when you were just,
you know, following friends, you know,
if someone did something really important,
like your cousin had a baby or something,
it's like, you want that at the top.
You'd be pretty angry at us
if we, you know, it was buried
somewhere down in your feed.
So the ranking was important.
But now, over the last few years, it's
gotten to a point where more of that stuff
Is just different public content
that's out there.
The recommendation systems
are super important because now instead of
just a few hundred or thousand
potential candidate posts from friends,
there's
millions of pieces of content
and that turns into
a really interesting
recommendation problem.
And with generative AI,
I think we're going to quickly
move into the zone
where not only is
is the majority of the content
that you see today on Instagram,
just recommended to you
from the kind of stuff
that's out there in the world
that matches your interests,
whether or not you follow the people.
I think in the future, a lot of this stuff
is going to be created with these tools, too.
Some of that is going to be creators
using the tools to create new content.
Some of it,
I think, eventually is going to be content
that's either created on the fly for you,
or kind of pulled together
and synthesized
through different things
that are out there.
So that that's just one example of how
kind of the core part of what we're doing
is just going to evolve.
And it's been evolving for
for 20 years already.
Well very few people realize that
one of the largest computing systems
the world has ever conceived of
is a recommender system.
I mean, it's this whole yeah, it's
this whole different path. Right?
It's not quite the kind of gen AI hotness
that people talk about,
but I think it's
all the transformer architectures.
And it's a similar thing of just 
building up more and more general models
Embedding, embedding
unstructured data into features.
Yeah. I mean, one of the big things
that just drives quality improvements
is, you know, it used to be
that you'd have a different model
for each type of content, right?
So a recent example is, you know,
we had, you know, one model for ranking
and recommending reels
and another model for ranking
and recommending more long form videos.
And then, you know, take some product work
to basically make it
so that the system can display,
you know, anything in line.
But, you know, the more you kind of
just create more general recommendation
models that can span everything,
it just gets better and better.
I mean, part of it,
I think, is just like economics
and liquidity of content
and the
broader of a pool that you can pull from,
you're just not having these weird
inefficiencies
of pulling from different pools.
But yeah, I mean, as the models get bigger
and more general,
that gets better and better.
So I kind of dream of one day,
you can almost imagine
all of Facebook or Instagram being,
you know, like a single AI model
that is unified,
all these different content types
and systems together
that actually have different objectives
over different time frames. Right.
Because some of it is just showing you,
you know, what's the interesting content
that you're going to be that,
that you want to see today,
but some of it is helping you build out
your network over the long term.
Right? People you may know or accounts
you might want to follow.
And these
these multi-modal models tend to be,
tend to be much better at recognizing
patterns, weak signals and such.
And so one of the things that people
people, you know, it's so interesting
that AI has been so deep in your company,
you've been building GPU infrastructure,
running these large recommender systems
for a long time.
Were a little slow on it
actually, getting to GPUs.
Yeah, I was trying to be nice.
I know.
Well, you know too nice.
I was trying to be nice.
You know, youre my guest. 
When I was backstage
before I came on here,
you were talking about, like, owning
your mistakes or something, right? So
You don't have to volunteer it
out of the blue.
I think this one has been well tried.
Yeah, it's like I got raked over the coals for it.
As soon as you got into it,
you got into it strong.
Let's just put there you go, there you go.
Now, the thing
that's really cool
about, about generative
AI is these days when I use WhatsApp,
I feel like I'm collaborating
with WhatsApp.
I love Imagine. I'm sitting here typing
and it's generating the images
as I'm going.
I go back and I change my words.
It's generating other images.
Yeah.
You know, and so the one that 
old Chinese guy,
enjoying a glass of whiskey at sundown
with three dogs: a Golden Retriever,
a Goldendoodle and a Bernese Mountain dog.
And it generates,
you know, a pretty good-looking picture.
Yeah. Yeah, we're getting there.
And then now you could actually load
my picture in there
and itll actually be me.
Yeah. That's as of last week.
Yeah. Yeah. Super excited about that.
Now imagine me. Yeah.
Now I'm spending a lot of time
with my daughters
imagining them as mermaids and things
over the last, over the last week, it's
been it's been a lot of fun.
But yeah, I mean, that's
that's the other half of it.
I mean, a lot of the gen
AI stuff is going to, on the one hand
its I think going to just be this big upgrade
for all of the workflows and products
that we've had for a long time.
But on the other hand,
there's going to be all these
completely new things that can now
get created. So Meta AI
you know, the idea of having, you know,
just an AI assistant
that can help you with different tasks
and, in our world is going to be,
you know, very creatively oriented,
like you're saying. But um.
I mean, they're very general,
so you don't need
to just constrain it to that.
It'll be able to answer any question.
Over time,
I think, you know, when we move from
like the Llama 3 class of models to Llama 4
and beyond, it's,
it's going to,
I think, feel less like a chat bot
where it's like you,
you give it a prompt and it just responds.
Then you give it a prompt and it responds,
and it's just like back and forth.
I think it's going to pretty quickly
evolve to, you give it an intent
and it actually can go away
on multiple time frames.
And I mean, it probably should acknowledge
that you gave it an intent up front.
But I mean, some of the stuff
I think will end up, you know itll spin up,
you know, compute jobs that take,
you know, weeks or months or something
and then just come back to you and like,
something happens in the world.
And I think that that's
going to be really powerful.
Today's AI,
as you know, is kind of turn-based,
you say something,
it says something back to you.
But obviously when we think,
when we're given a mission
or we're giving a problem, you know,
we'll contemplate multiple options.
Or maybe we come up with a, you know, a
tree of options, a decision tree,
and we walk down the decision tree
simulating in our mind,
you know, what are the different outcomes
of each decision
that we could potentially make.
And so we're doing planning.
And so in the future AI's
will kind of do the same.
One of the things that
that I was super excited about
when you talked
about your vision of creator AI,
I just think that's a home
run idea, frankly.
Tell everybody about the creator AI and
AI studio
that's going to enable you to do that.
Yeah, so we actually
I mean, this is something that we're
we've talked about it a bit, but
we're rolling it out a lot wider today.
You know, a lot of our vision is that
I don't think that there's just going
be like one AI model, right?
I mean, this is something
that some of the other companies
in the industry, they're like,
you know, it's like they're building
like one central agent
and yeah, we'll have the Meta AI
assistant that you can use.
But a lot of our vision
is that we want to empower
all the people who use our products
to basically create agents for themselves.
So whether that's, you know,
all the many, many millions of creators
that are on the platform or, you know,
hundreds of millions of small businesses,
we eventually want to just be able
to pull in all your content
and very quickly stand up a business
agent and be able to interact
with your customers and, you know, do
sales and customer support and all that.
So, the one that we're that we're just
starting to roll out more now is,
we call it AI Studio.
And it basically is, a set of tools
that eventually is going to make it
so that every creator can build
sort of an AI version of themselves,
as sort of an agent or an assistant
that their community can interact with.
There's kind of a fundamental issue here
where there's
just not enough hours in the day.
Right?
Its like if you're a creator, you want to engage
more with your community,
but you're constrained on time.
And similarly,
your community wants to engage with you,
but it's tough.
I mean, there's
just limited time to do that.
So the next best thing
is allowing people to basically create
these artifacts.
Right?
It's an agent,
but it's you train it
on your material
to represent you in the way that you want.
I think it's a very kind
of creative endeavor, almost like a,
like a piece of art or content
that you're putting out there. 
No, it's to be very clear that it's
not engaging with the creator themselves.
But I think it'll be
another interesting way,
just like how creators put out content
on, on these, social systems,
to be able to have agents that do that.
Similarly,
I think that there's going to be a thing
where people basically create their own
agents for all different kinds of uses.
Some will be sort of customized utility,
things that they're trying to get done
that they want to fine tune and
and train an agent for, and
some of them will be entertainment.
And some of the things
that people create are just funny,
you know,
and just kind of silly in different ways.
Or kind of have a funny attitude
about things that,
you know, we probably couldn't
we probably wouldn't build into Meta AI
as an assistant, but I think people
people are kind of pretty interested
to see, and interact with.
And then one of the interesting use cases
that we're seeing is people
kind of using these agents for support.
This was one thing
that was a little bit
surprising to me is one of the top
use cases for Meta AI already is
people basically using it to role play
difficult social situations
that they're going to be in.
So whether it's a professional situation,
it's like, all right,
I want to ask my manager, like,
how do I get a promotion or raise?
Or I'm having this fight with my friend,
or I'm having this difficult situation
with my girlfriend.
Like,
how can this conversation go?
And basically having
a like a completely judgment-free
zone where you can
basically role play that
and see how the conversation will go
and get feedback on it.
But a lot of people, they don't just want
to interact with the same
agent, whether it's Meta
AI or ChatGPT
or whatever
it is that everyone else is using,
they want to kind of
create their own thing.
So that's 
roughly where we're going with AI studio.
But it's all part of this bigger,
I guess, view that we have,
that there shouldn't just be one
big AI that people interact with.
We just think that the world
will be better and more interesting
if there's a diversity
of these different things.
I just think it's so cool
that if you're an artist
and you have a style,
you could take your style,
all of your body of work,
you could fine tune
one of your models.
And now this becomes an AI model
that you can come and you could prompt it.
You could ask me to
create something along
the lines of the art style that I have,
and you might even give me
a piece of art as 
a drawing, a sketch, as an inspiration.
And I can generate something for you.
And you come to my
bot for that,
come to my AI for that.
It could be,
every single restaurant,
every single website will
probably in the future have these AIs.
Yeah I mean, I kind of think that in
the future, just like every business has,
you know, an email address and a website
and a social media account or several.
I think in the future,
every business is going to have an AI
agent that interfaces with their customers.
And some of these things, I think
have been pretty hard to do historically.
Like, if you think about any company,
it's like you probably have customer
support as just a separate organization from sales,
and that's not really
how you'd want it to work as CEO.
It's just that, okay,
they're kind of different skills.
You're building up these- 
I'm your customer support just so you know.
Yeah.
Well, apparently I am. 
Whenever Mark needs something. 
I can't tell whether its his chat bot
or it's just Mark, but
It just was my chat bot here, 
just asking here.
Well, I guess that's kind of,
when you're CEO,
you have to do all this stuff.
But, I mean,
then when you build the abstraction
in your organization,
a lot of times, like the,
you know, in general
the organizations are separate
because they're kind of optimized
for different things. 
But I think,
like the platonic ideal of this would be
that it's kind of one thing, right?
As a, you know, as a customer,
you don't really care.
You know, you don't want to
have a different route
when you're trying to buy something versus
if you're having an issue
with something that you bought,
you just want to have a place
that you can go
and get your questions answered
and be able to
engage with the business in different ways.
And I think that that applies
for creators, too.
I think thats the kind of personal consumer
side of this-
And all that engagement
with your customers,
especially their complaints,
is going to make your company better.
Yeah. Totally.
Right?
The fact that is all engaging 
with this AI is going to capture
the institutional knowledge
and all of that
can go into analytics
which improves the AI and so on, so forth.
Yeah, yeah.
So the business version of this is-
that I think
has a little more integration and we're
still in a pretty early alpha with that.
But the AI Studio making it so that people
can kind of create their UGC agents
and different things,
and getting started on this flywheel
of having creators create them.
I'm pretty excited about that.
So can I, can I use AI Studio to fine tune
with my images, my collection of images?
Yeah, yeah,
we're going to get there.
And then I could, could I give it, load it
with all the things that I've written,
use it as my RAG?
Yeah. Basically. 
Okay.
And then every time I come back to it,
it loads up its memory again,
so it remembers where it left off
last time. 
And we carry on our conversation as,
though never nothing ever happened.
Yeah and look, I mean, like any product,
it'll get better over time.
The tools for training,
it will get better.
It's not just about
what you want it to say.
I mean, I think generally creators
and businesses
have topics
that they want to stay away from too.
So just getting better at all this stuff,
I think the platonic version of
this is not just text, right?
You almost want to just be able to,
and this is a sort of an intersection
with some of the codec avatar work
that we're doing over time.
You want to basically be able
to have almost like
a, a video chat 
with the agent.
And I think we'll get there over time.
I don't think that this stuff
is that far off, but the flywheel is
spinning really quickly, so it's exciting.
There is a lot of new stuff to build.
And I think
even if the progress on the foundation
models kind of stopped now,
which I don't think it will,
I think we'd have like five years
of product innovation
for the industry to basically figure out
how to most effectively use
all the stuff that's gotten built so far.
But I actually just think
the kind of foundation models
and the progress on the fundamental
research is accelerating.
So, that it's, a pretty wild time.
Your vision-
It's all you know,
you kind of made this happen.
Why thank you.
In the last conversation, I -
Thank you.
Yeah.
You know, you know, you know,
CEOs, we're delicate flowers.
We need a lot of back-
Yeah.
We're pretty grizzled at this point.
I think we're
we're the two kind of longest
standing founders in the industry, right?
It's true. 
It's true.
I just-
And your hair has gotten gray.
Mine has just gotten longer.
Mine's gotten gray.
Yours has gone curly, what's up?
It was always curly.
That's why I kept it short.
Okay.
You know, I just.
If I'd known
it was going to take so long to succeed,
you would never would have started.
No, I would have dropped out of college,
just like you.
Get a head start.
Well, that's a there's a good difference
between our personalities.
You got a 12 year head start.
That's pretty good.
You know, you're doing pretty well.
I'm gonna-
I'm going to be able to carry on.
Let me just put it that way.
Yeah. So, so,
the thing that I love about
your vision that,
everybody can have an AI
that every business can have an AI
In our company,
I want every engineer and every software
developer to have an AI.
And,
or many AIs.
The thing
that I love about your vision
is you also believe
that everybody and every company
should be able to make their own AI.
So you actually open-sourced,
when you open-sourced Llama
I thought that was great.
Llama 2.1, by the way,
I thought Llama 2 was
probably the biggest event
in AI last year.
And the reason for that-
I mean, I thought it was the H100, but,
you know, it's,
it's a chicken or the egg question.
That's a chicken or the egg question.
Yeah. Which came first?
The H100.
Well, Llama 2, it was,
it was actually not the H100.
Yeah, it was A100 yeah. Thank you.
And so,
but the reason why I said it was
the biggest event was because when that
came out,
it activated every company,
every enterprise and every industry.
All of a sudden,
every health care company was building AI.
Every company was building AI,
every large company, small companies,
startups were building AIs.
It made it possible for every researcher
to be able to reengage AI again,
because they have a starting
point to do something with,
and then now,
3.1 is out and the excitement,
just so you know,
you know, we work together to, to deploy,
3.1, we're taking it out to the world's
enterprise.
And the excitement is just off the charts.
And, and I think it's going to enable
all kinds of applications.
But tell me about your
your open-source philosophy.
Where did that come from?
And, you know,
you open-sourced PyTorch.
And that it is now the framework
by which AI is done.
And, now you've open-sourced Llama
3.1 or Llama
there's a whole ecosystem built around it.
And so I think it's terrific.
But where did that all come from?
Yeah.
So there's a bunch of history
on a lot of this.
I mean, we've done
a lot of open-source work over time.
I think part of it,
you know, just bluntly is, you know,
we got started
after some of the other tech
companies, right, in building out
stuff like the distributed computing
infrastructure and the data centers.
And, you know, because of that,
by the time that we built that stuff,
it wasn't a competitive advantage.
We're like,
all right, we might as well make this open
and then we'll benefit 
from the ecosystem around that.
So we
had a bunch of projects like that.
I think the biggest one
was probably Open Compute where
we took our server
designs, the network designs,
and eventually the data center designs
and published all of that.
And by having that
become somewhat of an industry standard,
all the supply chains basically got
organized around it, which had this
benefit of saving money for everyone.
So by making it public, and open,
we basically have saved billions
of dollars from doing that.
Well, Open Compute was also
what made it possible for NVIDIA
HGXs, that we designed for one data center,
all of a sudden, works
in every data center.
Awesome.
So that was an awesome experience.
And then, you know,
we've done it with a bunch of our 
infrastructure
tools, things like React, PyTorch.
So I'd say
by the time that Llama came around,
we were sort of positively
predisposed towards doing this.
For, for AI models specifically.
I guess there's a few ways
that I look at this.
I mean, one is,
you know it's been really fun building stuff
over the last 20 years at the company.
One of the things that that has been sort
of the most difficult
has been kind
of having to navigate the fact
that we ship our apps
through our competitors mobile platforms.
So in the one hand, the mobile platforms
have been this huge boon to the industry.
That's been awesome.
On the other hand, having to deliver
your products through your competitors,
is challenging, right?
And I also,
you know,
I grew up in a time where, you know,
the first version of Facebook
was on the web and that was open.
And then,
as a transition to mobile,
you know, the plus side of that was,
you know, now
everyone has a computer in their pocket.
So that's great.
The downside is, okay, we're a lot more
restricted in what we can do.
So, when you look at these generations
of computing there's this big recency bias
where everyone just looks
at mobile and thinks, okay,
because the closed ecosystem,
because Apple basically won and set the
the terms of that.
And like yeah,
I know that there's more Android phones
out there technically, but like Apple
basically has the whole market.
and like all the profits.
And basically Android is kind of following
Apple in terms of the development of it.
So I think Apple
pretty clearly won this generation.
But it's not always like that
where if you go back a generation,
you know, Apple was doing
their kind of closed thing.
But Microsoft, which as you know, it
obviously wasn't like this perfectly open
company, but, you know, compared
to Apple with Windows running
on all the different OEMs and different
software, different hardware
it was a much more open ecosystem
and Windows
was the leading ecosystem.
It, basically in the kind of PC
generation of things,
the open ecosystem won.
And I am kind of hopeful
that in the next generation of computing,
we're going to return
to a zone where the open ecosystem wins
and is the leading one again.
There will always be a closed one
and an open one.
I think that there's reasons to do both.
There are benefits to both.
I'm not like a zealot on this.
I mean, we do closed source stuff
and not everything that
we that we publish is open.
But I think in
general for the computing platforms
that the whole industry is building
on, there's a lot of value for that
if the software especially is open.
So that's really shaped
my philosophy on this. And,
for both AI with Llama
and with the work that we're doing in
AR and VR, where we are
basically making the Horizon OS
that we're building for mixed reality,
an open operating system in the sense of,
what Android or Windows was
and basically making it so that
we're going to be able to work
with lots of different hardware companies
to make all different kinds
of devices.
We basically just want to return
the ecosystem to that level where
that's going to be the open one.
And I'm pretty optimistic
that in the next generation,
the open ones are going to win.
For us specifically
I just want to make
sure that we have access to-
I mean, this is sort of selfish, but,
you know, 
after building this company for a while,
one of my things
for the next 10 or 15 years is like,
I just want to make sure that we can build
the fundamental technology
that we're going to be building
social experiences on,
because there have just been too many 
things that I've tried to build
and then have just been told,
nah, you can't really build that
by the platform provider, that
like, we're going to go build
all the way down and,
and make sure that that- 
There goes our broadcast opportunity.
Yeah. No, sorry.
Sorry.
There's a beep.
Yeah.
You know, Ive been
doing okay for, like, 20 minutes, but...
get me talking about closed
platforms and I get angry.
Hey, look, it is great.
I think it's a great world.
Where there are
people who are dedicated
to build the best possible AIs,
however they build it,
and they offer it to the world,
you know, as a service. And then.
But if you want to build your own AI,
you could still also build your own AI.
So the ability to use an AI.
You know, there's a lot of stuff,
I prefer not to make this jacket myself.
I prefer to have this jacket made for me.
You know what I'm saying?
Yeah.
But so the fact that.
So the fact that leather could be open
source is not a useful concept for me,
but I think the,
the idea that you could,
you could have great services, incredible
services as well as open service.
Open ability.
Then we basically have the entire spectrum.
But the thing that's,
that you did with 3.1 that was really
great was you have 4 or 5B,
you have 70B, you have 8B
you could, you could use it for synthetic
data generation,
use the larger models
to essentially teach the smaller models.
And although the larger models
will be more general, it's less brittle,
you could still build a smaller
model that fits in, you know, whatever
operating domain or operating costs
that you would like to have.
Meta guard, I think?
Yeah Llama Guard.
Yeah Llama Guard, Llama
Guard for guard railing.
Fantastic.
And so now and the way that you built the
model, it's built in a transparent way.
You dedicated-
You've got a world class safety team.
World class ethics team.
You could build it in such a way
that everybody knows it's built properly.
And so I really love that part of it.
Yeah and I mean, just to finish the thought
from before,
before I got,
I got sidetracked there for a detour.
I do think there's this alignment where
we're building it
because we want the thing to exist,
and we want to not get cut off
from some closed model.
Right? And,
but this isn't just like
a piece of software that you can build.
It's, you know, you need
an ecosystem around it.
And so it's almost like it
kind of almost wouldn't even work that
well if we didn't open source it. Right?
It's not we're not doing this
because we're kind of altruistic people.
Even though I think that
this is going to be helpful
for the ecosystem, and we're doing it
because we think that this is going
to make the thing that we're building
the best by having a robust ecosystem.
Well, look how many people contributed
to PyTorch ecosystem.
Yeah, totally.
Mountains of engineering.
Yeah. Right. Yeah. Yeah.
I mean, NVIDIA alone, we probably have
a couple of hundred people just dedicated
to making PyTorch better and scalable and,
you know, more performant and so on
and so forth.
Yeah and it's also just when something becomes
something of an industry
standard,
other folks do work around it, right?
So like all of the silicon in the systems
will end up being optimized
to run this thing really well,
which will benefit everyone,
but it will also work
well with the system that we're building.
And that's, I think, just one
example of how this ends up being,
just being really effective.
So, yeah, I mean, I think that
the open-source strategy is going to be,
yeah, it's just going to be a good one
as a business strategy.
I think people still don't quite get it.
We love it so much.
We built an ecosystem around it.
We build this thing
Called AI Foundry.
Yeah. Yeah, yeah.
I mean, you guys have been awesome. Yeah.
I mean, every time we're shipping
something, you
you guys are the first to release this
and optimize it and make it work.
And so I mean, I, I appreciate that.
What can I say?
We have good engineers you know and so.
Well you always just jump on this stuff
quickly too.
You know, I'm a
senior citizen, but I'm agile.
You know, that's what CEOs have to do.
And I recognize an important thing,
I recognize an important thing.
And I think that Llama is 
genuinely important.
We built this concept called an AI factory, uh,
AI Foundry around it
so that we can help everybody build, take-
you know, a lot of people,
they have a desire to
build AI.
And it's very important
for them to own the AI because once
they put that into
their flywheel, their data flywheel,
that's how their company's institutional
knowledge
is encoded and embedded into an AI.
So they can't afford to have the
AI flywheel, the data flywheel
that experience flywheel somewhere else.
So and so open source
allows them to do that.
But they don't really know
how to turn this whole thing into an AI
and so we created this thing
called AI Foundry.
We provide the tooling,
we provide the expertise,
Llama technology,
we have the ability to help them
turn this whole thing, into an AI service.
And, and then when we're done with that,
they take it, they own it.
The output of it is what we
call a NIM.
And this NIM, this this neuro micro NVIDIA
Inference Microservice,
they just download it,
they take it, they run it anywhere
they like, including on-prem.
And we have a whole ecosystem of partners,
from OEMs that can run the NIMs to,
GSIs like Accenture
that that we train and work with to create
Llama-based NIMs and pipelines and
and now
we're off helping enterprises
all over the world do this.
I mean, it's really quite an exciting thing.
It's really all triggered off
of, the Llama open-sourcing.
Yeah, I think especially the ability
to help people distil their own models
from the big model
is going to be a really valuable new thing
because there's this, just like
we talked about on the product side,
how at least I don't
think that there's going to like one major
AI agent that everyone talks to.
At the same level,
I don't think that
there's going to necessarily
be one model that everyone uses.
We have a chip AI, chip design AI,
we have a software coding AI,
and our software coding
AI understands USD
because we code
in USD for Omniverse stuff.
We have
software AI that understands
Verilog, our Verilog.
we have we have software AI
that understands our bugs database
and knows how to help us triage
bugs and sends it to the right engineers.
And so each one of these AIs are fine
tuned off of Llama and,
and so we fine tune them, we guardrail
them, you know 
if we have an AI design, for,
chip design,
we're not interested in asking it
about politics, you know, and religion
and things like that.
So we guardrail it.
And so,
so I think, I think every company will
essentially have for every single function
that they have, they will likely have AIs
that are built for that.
And they need help to do that.
Yeah.
I mean, I think it's one of the big
questions is going to be in the future,
to what extent are
people just using the kind of the bigger,
more sophisticated models
versus just training their own models
for the uses that they have?
And at least I would bet
that they're going to be
just a vast
proliferation of different models.
We use the largest ones.
And the reason for that
is because our engineers, their
time is so valuable.
and so we get, right now we're getting 4
or 5B, optimized for performance.
And as you know, 405B doesn't
fit in any GPU, no matter how big.
And so that's why
the NVLink performance is so important.
We have every one of our GPUs
connected by this,
non-blocking switch called NVLink switch.
And in the HGX for example,
there are two of those switches
and we make it possible for all these,
all these GPUs to work and,
and run the 405Bs really performant.
The reason why we do it is because
the engineers times are so valuable to us.
You know,
we want to use the best possible model,
the fact that it's cost effective
by a few pennies,
who cares?
And so we just want to make sure
that the best quality of results
is presented to them.
Yeah.
Well, I mean, the 405
I think is about half the cost
to inference of the GPT 4o model.
So I mean, at that level, it's already
I mean, it's pretty good.
But yeah, I mean I think people are doing
stuff on devices or want smaller models.
They're just going to distil it down.
So that's like a whole different set
of services.
That AI is running,
and let's pretend for a second
that we're hiring that AI,
that AI for chip
design is probably $10 an hour.
You're using,
if you're using
it constantly and you're sharing that
AI across a whole bunch of engineers.
So each engineer probably has an
AI that's sitting with them.
And, you know, it doesn't cost very much.
And we pay the engineers a lot of money.
And so to us, a few dollars an hour,
amplifies the capabilities of somebody
that's really valuable.
Yeah, yeah.
I mean, you don't need to convince me.
If you haven't,
if you haven't hired
an AI, do it right away.
That's all we're saying.
And so, 
let's talk about,
the next, the next wave.
you know, one of the things
that I really love about the work
that you guys do, computer vision,
one of the models that we use
a lot internally,
is Segment Everything, and,
you know, that
that we're now training AI models on video
so that we can understand the world model.
Our use case is for robotics
and industrial digitalization and,
connecting these AI models into Omniverse
so that we can,
we can, model and represent
the physical world better,
have robots that operate
in these Omniverse worlds better.
Your application,
the Ray-Ban Meta glass,
your vision for bringing AI into
the virtual world, is really interesting.
Tell us about that.
Yeah. Well,
okay, a lot to unpack in there.
the Segment Anything model that you're
talking about, we're actually presenting,
I think the next version of that here
at Siggraph.
Segment Anything 2.
And it now works,
it's faster,
it works with, oh, here we go.
It works in video now as well.
I think these are actually cattle
from my ranch in Kauai.
By the way,
these are called Marks Cows
Delicious Marks Cows. 
There you go.
Next time we do-
So, Mark,
Mark came over to my house
and we made Philly cheesesteak together.
Next time you're bringing the cow.
Id say you did.
I was more of a sous-chef.
But, boy, that was really good.
It was really good.
That sous-chef comment.
Okay, listen,
And then at the end of the night though, you were
like, hey, so you ate enough, right?
And I was like,
I don't know, I could eat another one.
You're like, really?
You know, usually when you say something
to your guest.
I was definitely like, yeah,
we're making more, we're making more.
Did you get enough to eat?
Usually your guest says,
oh yeah, I'm fine.
Make me another cheesesteak Jensen.
So just to let you know how OCD he is.
So I turn around, I'm prepping the,
the cheesesteak
and I said, Mark,
cut the tomatoes. And so Mark,
I handed him a knife.
Yeah, I'm a precision cutter.
And so he cuts. He cuts the tomatoes.
Every single one of them are perfectly
to the exact millimeter.
But the really interesting thing
is, I was expecting all the tomatoes
to be sliced and kind of stacked up,
kind of like a deck of cards.
And, but when I turned around,
he said he needed another plate.
And the reason for
that was because all of the tomatoes
he cut, none of them touched each other.
Once he separates one
slice of tomato from the other tomato,
they shall not touch again.
Yeah.
Look, man, if you wanted them to touch,
you needed to tell me that.
Thats why Im just a sous-chef. Okay?
That's why he needs an AI that doesn't judge.
Yeah, it's like.
So this is super cool.
Okay, so it's recognizing the cows track.
It's recognizing tracking the cows.
Yeah, yeah.
So it's, a lot of fun
effects will be able to be made with this.
And because it'll be open
a lot of more serious applications
across the industry, too.
So, yeah, I mean, scientists
use this stuff to, you know, study,
like coral reefs and natural habitats and,
and kind of evolution of landscapes
and things like that.
But, I mean, it's, being able to do this
and video and having it be a zero shot
and be able to kind of interact with it
and tell it what you want to track
is, it's pretty cool research.
So, for example, the reason why we use it,
for example, you have a warehouse
and they've got a whole bunch of
cameras and the warehouse AI
is watching everything that's going on.
And let's say
you know, a stack of boxes fell,
or somebody spilled
water on the ground, or, you know,
whatever accident is about to happen,
the AI recognizes it, generates the text,
sends it to somebody, and, you know,
you know, help will come along the way.
And so that's one way of using it,
instead of recording everything.
If there's an accident, instead of
recording every nanosecond of video
and then going back and retrieve
that moment,
it just records the important stuff
because it knows what it's looking at.
And so having a video understanding
model, a video language model
is really, really powerful
for all of these interesting applications.
Now what else what else
are you guys going to work on beyond-
talk to me about-
Yes. There's all the smart glasses.
Yeah. Right.
So I think when we think
about the next computing platform,
you know, we kind of break it down
into mixed reality, the headsets
and the smart glasses.
I think it's easier for people
to wrap their head
around that and wearing it
because, you know,
pretty much everyone
who's wearing a pair of glasses
today will end up
that'll get upgraded to smart glasses.
And that's like more than a billion people
in the world.
So that's going to be a pretty big thing.
the VR MR headsets,
I think some people find it
interesting for gaming or different uses.
Some don't yet.
Yet my view is that
they're going to be both in the world.
I think the smart glasses
are going to be sort of the mobile phone,
kind of always on version
of the next computing platform,
and the mixed reality headsets
are going to be more like your workstation
or your game console,
where, when you're sitting down
for a more immersive session
and you want access to more compute,
I mean, look, I mean, the glasses are
just very small form factor.
There are
going to be a lot of constraints on that.
Just like you can't do
the same level of computing on a phone.
It came at exactly the time
when all of these breakthroughs
in generative AI happened.
Yeah.
So we basically for smart glasses,
we've been we've been going at the problem
from two different directions
on the one hand, we've been building
what we think is sort of the technology
that you need for the kind of ideal
holographic AR glasses and
we're doing all the custom silicon work,
all the custom display stack work,
like all the stuff that you need to do
to make that work
in their glasses.
Right? It's not a headset.
It's not like a VR or MR headset.
They look like glasses. But,
they're still quite a bit
far off from the glasses
that you're wearing now.
I mean, those are very thin, but,
but even the Ray-Bans
that we that we make, you couldn't
quite fit all the tech that you need to
into that yet for kind of full holographic
AR, we're getting close.
And over the next few years
I think we'll basically get closer.
It'll still be pretty expensive, but 
I think that will start to be a product.
The other angle that we've come at
this is let's start with good looking
glasses.
By partnering with the best glasses maker
in the world, Essilor Luxottica.
They basically make
they have all the big brands that you use.
You know, it's
Ray-Ban or Oakley or Oliver Peoples
or just like a handful of others.
Yeah, it's kind of all Essilor Luxottica.
The NVIDIA of glasses.
I think that, you
know, I think they would probably like that
analogy, but, I mean, who wouldn't
at this point?
So we've been working with them
on the Ray-Bans.
We're on the second generation.
And the goal there has been,
okay, let's constrain the form factor
to just something that looks great.
And within that,
let's put in as much technology as we can,
understanding
that we're not going to get to
the kind of ideal of what
we want to fit into a technically, but
it'll, but at the end,
it'll be like great looking glasses.
And at this point we have
we have camera sensors,
so you can take photos and videos.
You can actually livestream to Instagram.
You can take video calls on WhatsApp
and stream to the other person
what you're seeing.
You can, I mean, it has
it has a microphone and speakers.
I mean, the speaker is actually really,
really good.
Its open ear
so really a lot of people find it
more comfortable than, than earbuds.
you can listen to music and it's just like
this private experience.
That's pretty neat,
people love that.
You take phone calls on it.
but then it just turned out
that that sensor
package was exactly what you needed
to be able to talk to AI too.
So that was sort of an accident.
If you'd asked me five years ago,
Were we going to get holographic
AR before AI,
I would have said, yeah, probably.
Right I mean, it's just seems like
kind of the graphics progression
and the display progression
on all the virtual and mixed reality stuff
and building up the new display stack.
We're just making continual progress
towards that.
That's right.
And then this breakthrough
happened with LLMs.
And it turned out that
we have sort of really high-quality AI now
and getting better at a really fast rate
before you have holographic AR.
So it's sort of this inversion
that, that I didn't really expect.
I mean, we're
we're fortunately well positioned
because we were working
on all these different products.
But I think what you're
going to end up with is,
just a whole
series of different potential glasses
products
at different price points with different
levels of technology in them.
So I kind of think, based on what
we're seeing now with the Ray-Ban Metas,
I would guess that display
less AI glasses
at like a $300 price
point are going to be a really big product
that, like tens of millions of people
or hundreds of millions of people
eventually are going to have.
So you're going to have super
interactive AI that you're talking to.
Yeah, visual.
You have visual language
understanding that you just showed
you have real time translation.
You could talk to me in one language,
I hear in another language.
Then then the display
is obviously going to be great too,
but it's going to add
a little bit of weight to the glasses
and it's
going to make them more expensive.
So I think for
there will be a lot of people
who want the kind of full
holographic display.
But there are also going to be
a lot of people for whom,
you know,
they want something that eventually
is going to be
like really thin glasses and-
Well for industrial applications
and for some work applications,
we need that.
I think, for consumer stuff too.
You think so?
Yeah.
I mean, I think, you know, it's
I was thinking about this
a lot during the,
you know, during Covid when everyone
kind of went remote for a bit.
It's like you're spending all this time
on Zoom that's like, okay, this is
like it's great that we have this, but,
but in the future we're like,
not that many years away
from being able to have a virtual meeting
where, like,
you know, it's like,
I'm not here physically.
It's just my hologram.
Yeah.
And like, it just feels like we're there
and we're physically present.
We can work on something
and collaborate on something together.
But I think this is going to be
especially important with AI.
With that application I could live with, with a, a device
that, that I'm not wearing all the time.
Oh yeah. But I think we're going to get
to the point where it actually is.
Yeah
Itll be, I mean, within glasses
there's like thinner frames
and there's thicker frames
and there's like all these styles.
But so I don't,
I think we're, we're a while away
from having full holographic glasses
in the form factor of your glasses, but
I think having it in a pair of stylish,
kind of chunkier framed glasses
is not that far off.
Sunglasses are face size these days.
I could see that.
Yeah.
And you know, that's
that's a very helpful style.
Yeah, sure. that's very helpful.
You know, it's like I'm trying
to, you know,
I'm trying to make my way into
becoming a style influencer.
So I can, like, influence this before,
you know, before the glasses
come to the market, but, you know?
Well I can see you attempting it. How's your
style influencing working out for you?
You know, it's early.
 Yeah?
It's early.
It's early.
But, I don't know,
I feel like if a big part
of the future of the business
is going to be building,
kind of stylish glasses that people wear,
this is something I should probably start
paying a little more attention to.
Thats right.
So, yeah,
we're going to have to retire the version
of me that wore the same thing every day.
But I mean, that's
the thing about glasses, too.
I think it's,
you know, it's unlike,
you know, even the watch
or phones,
like, people really do not want to all
look the same.
Right? And it's like,
so I do think that it's, you know, it's
a, it's a platform
that I think is going to lend itself,
going back to the theme
that we talked about before
towards being an open ecosystem,
because I think the diversity
of form factors that people and styles
that people are going to
demand is going to be immense.
It's not like
everyone is not going to want to put
like the one kind of pair of glasses that,
you know, whoever else designs like,
that's not
I don't think
that's going to fly for this.
Yeah, I think that's right.
Well, Mark, it's sort of incredible
that we're living through a time 
where the entire computing stack is being
reinvented, how we think about software.
You know, what Andrej calls software
one and software two.
And now we're basically in software three now.
The way we compute,
from general purpose computing to these
generative neural network
processing way of doing computing.
The capabilities, the applications
we could develop
now are unthinkable in the past.
And, and this technology, generative AI,
I don't remember another technology
that that in such a fast
rate, influenced consumers
enterprise, industries and science.
And to be able to, to cut across,
cut across,
all these different fields of science
from, from climate tech
to, biotech,
to physical sciences,
in every single field
that we're encountering,
generative AI is right in the middle of that,
fundamental transition.
And in addition to that,
the things that you're talking about,
generative AI is going to make
a profound impact in society.
You know, the products that we're making.
And one of the things that I'm
super excited about, and somebody asked me
earlier, is there going
to be a, you know, Jensen AI?
Well, that's exactly the creative
AI you were talking about.
You know, where we just build our own AIs
and I, I load it up
with all of the things that I've written
and I fine tune it with
the way I answer questions and
hopefully, over time,
the accumulation of use and,
you know, it becomes a really,
really great assistant and companion,
For a whole lot of
people who just want
to, you know, ask questions or, bounce
ideas off of and,
and it'll be the version of Jensen
that as, as you were saying earlier,
that's not judgmental.
You're not afraid of being judged.
And so you could come and interact with it
all the time.
But I just think, I think that
those are really incredible things.
And, you know, we write
we write a lot of things all the time.
And how incredible is it
just to give it, you know, 3 or 4 topics.
Now, these are the basic themes of what
I want
to write about and write in my voice
and just use that as a starting point.
So there's just 
so many things that we can do now.
it's really
terrific working with you. And,
I know that,
I know that,
it's not easy building a company,
and you pivoted yours
from desktop to mobile to VR to AI,
all these devices, it's really, really,
really extraordinary to watch.
And NVIDIA's pivoted many times ourselves,
and I know exactly how hard it is
doing that.
And, you know, both of us have gotten
kicked in our teeth
a lot, plenty over the years.
But that's what it takes to,
to want to be a pioneer and, innovate.
So it's really great watching you.
Well.
And likewise,
I mean, it's like, it's I'm
not sure if it's a pivot
if you keep doing the thing
you were doing before, but as well.
But it's but you add to it.
I mean there's more chapters
to all, to all of this.
And I think the same thing for, it's
been fun watching...
I mean, the journey that you guys
have been on, I mean, just and you,
we went through this period
where everyone was like,
nah, everything is going to kind of move
to these devices and,
you know, it's just going to get
super kind of cheap compute.
And you
guys just kept on plugging away at this
and it's like, no, like actually
you're going to want these big systems
that can parallelize.
You went the other way.
Yeah. No.
We went and instead of building smaller
and smaller devices, we made computers
the size of warehouses.
A little unfashionable.
Super unfashionable.
Yeah, yeah.
But now, now it's cool.
And instead of, you know, we started
building a graphics chip, a GPU,
and now when you, when,
when you're deploying a GPU,
you still call it Hopper H100,
but so you guys know when,
Zuck calls
it H100 his data center of 
H100s,
I think you're coming up on 600,000.
Were good customers.
That's how you get the
Jensen Q&A at Siggraph.
Wow. Hang on.
I was getting the Mark Zuckerberg Q&A.
You were my guest.
And I wanted to make sure
that-
You just called me one day you're like, hey,
you know, in like a couple of weeks,
we're doing this thing at Siggraph.
I'm like, yeah,
I don't think I'm doing anything
that day. I'll fly to Denver.
It sounds fun.
Exactly.
I'm not doing anything
that afternoon, you just showed up.
But the thing
thats just incredible.
These systems that you guys build,
they're giant systems.
Incredibly hard to orchestrate,
incredibly hard to run.
And, you know,
you said that, you got into the GPU,
journey later than the most.
but you're operating larger
than just about anybody,
and it's incredible to watch.
And congratulations
on everything that you've done.
And, you are quite the style icon now.
Check out this guy.
Early stage, working on it.
It's uh-
Ladies and gentlemen,
Mark Zuckerberg.
Thank you.
Hang on, hang on.
Well,
you know, you know,
so it turns out the last time
that we got together, after dinner,
Mark and I were-
Jersey swap.
Jersey swap, and
we took a picture and
it turned into something viral,
and, and.
Now, I thought that he
he has no trouble wearing my jacket.
I don't know, is that my look?
It should be.
Is that right?
Yeah, I actually, I,
I made one for you.
You did?
Yeah.
That one's Mark's.
I mean, here, let's see.
We got a box back here.
It's black and leather and shearling.
Oh!
I didn't make this.
I just ordered it online.
Hang on a second.
It's a little chilly in here.
I think I'll try this on.
I think this is-
My goodness.
I mean, it's a vibe
you just need.
Is this me?
Get this guy a chain.
Next time I see you Im bringing you
a gold chain.
So fair is fair.
So I let you know.
I was telling everybody
that Lori bought me a new jacket
to celebrate this year's Siggraph.
Siggraph is a big thing in our company.
As you could imagine.
RTX was launched here.
amazing things were launched here.
And this is a brand-new jacket.
It's literally two hours old. Wow.
And so I think we oughta jersey swap again.
All right. Well-
This ones yours.
I mean, this is worth more
because it's used.
Let's see.
I don't know. 
I think I think Mark is pretty buff.
He's like, the guy is pretty jacked.
I'm in.
You too, man.
All right, all right,
all right, everybody, thank you.
Mark Zuckerberg have a great Siggraph.

this is how intelligence is
made a new kind of
factory generator of
tokens the building blocks of
AI tokens have opened a new frontier the
first step into an extraordinary world
where endless possibilities are born
[Music]
tokens transform words into knowledge
and breathe life into
images they turn ideas into
videos and help us safely navigate any
environment tokens teach robots to move
like the Masters
[Music]
Inspire new ways to celebrate our
victories a martini pleas call light
up thank you
Adam and give us peace of mind when we
need it most hi moroka hi Anna it's good
to see you again hi Emma we're going to
take your blood sample today okay don't
worry I'm going to be here the whole
time they bring meaning to numbers
to help us better understand the world
around
[Music]
us predict the dangers that surround
[Music]
us and find cures for the threats within
us
[Music]
tokens can bring our Visions to
[Music]
life and restore what we've
[Music]
[Applause]
lost
Zachary I got my voice back
buddy they help us move
forward one small step at a time
[Music]
and one giant
leap
[Music]
together and
here is where it all begins
welcome to the stage Nvidia founder and
CEO Jensen
[Music]
[Applause]
[Music]
[Applause]
Wong welcome to
CES are you excited to be in Las
Vegas do you like my Jack
it I thought I'd go the other way from
Gary
Shapiro I'm in Las Vegas after all if
does if this doesn't work out if all of
you
object well just get used to it I think
I really think you have to let this sink
in in another hour or so you're going to
feel good about
it well uh welcome to
Nvidia in fact you're inside nvidia's
digital
twin and we're going to take you to
Nvidia ladies and gentlemen welcome to
Nvidia your
inside our digital
twin everything here is generated by
AI it has been an extraordinary Journey
extraordinary year here and uh it
started in 1993 ready go with
mv1 we wanted to build computers that
can do things that normal computers
couldn't and mv1 made it possible to
have a game console in your
PC our programming architecture was
called
UD missing the letter c until a little
while later but UDA UniFi Unified device
architecture and the first developer for
UDA and the first application that ever
worked on UDA was sega's Virtual
Fighter six years later we invented in
1999 the programmable
GPU and it
started 20 years 20 plus years of
incredible advance in this incredible
processor called the GPU it made modern
computer Graphics
possible and now 30 years later sega's
Virtual Fighter is completely
cinematic this is the new Virtual
Fighter project that's coming I just
can't wait absolutely
incredible six years after that six year
six years after
1999 we invented Cuda so that we could
explain or or expressed the
programmability of our gpus to a rich
set of algorithms that could benefit
from it Cuda
initially was difficult to explain and
it took years in fact it took
approximately six years somehow six
years later six years later or
so
2012 Alex kvki ilas sus and Jeff Hinton
discovered Cuda used it to process
alexnet and the rest of it is history AI
has been advancing at an incredible Pace
since started with perception AI we now
can understand images and words and
sounds to generative AI we can generate
images and text and
sounds and now agentic ai AIS that can
perceive reason plan and act and then
the next phase some of which we'll talk
about tonight physical AI 2012 now
magically
2018 something happened that was pretty
incredible Google's Transformer was
released as Bert and the world of AI
really took off Transformers as you know
completely changed the land landcape for
artificial intelligence in fact it
completely changed the landscape for
computing
altogether we recognized properly that
AI was not just a new application with a
new business opportunity but AI more
importantly machine learning enabled by
Transformers was going to fundamentally
change how Computing works and
today Computing is revolutionized in
every single layer from hand coding
instructions that run on CPUs to create
software tools that humans use we now
have machine learning that creates and
optimizes new networks that processes on
gpus and creates artificial
intelligence every single layer of the
technology stack has been completely
changed an incredible transformation in
just 12 years
well we can Now understand information
of just about any modality surely you've
seen text and images and sounds and
things like that but not only can we
understand those we can understand amino
acids we can understand physics we
understand them we can translate them
and generate them the applications are
just completely endless in fact almost
any AI application that you you see out
there what modality is the input that it
learned from what modality of
information did it translate to and what
modality of information is it generating
if you ask these three fundamental
questions just about every single
application could be inferred and so
when you see application after
applications that are Aid driven AI
native at the core of it this
fundamental concept is there machine
learning has changed how every
application is going to be built how
computing will be done and the
possibilities Beyond
well
gpus gForce in a lot of
ways all of this with AI is the house
that GeForce built GeForce enabled AI to
reach the masses and now ai is coming
home to
GeForce there are so many things that
you can't do without AI let me show you
some of it
now
[Music]
[Applause]
[Music]
[Applause]
[Music]
that was realtime computer
Graphics no computer Graphics researcher
no computer scientist would have told
you that it is possible for us to rate
trce every single Pixel at this point we
Ray tracing is a simulation of light the
amount of geometry that you saw was
absolutely insane it would have been
impossible without artificial
intelligence there are two fundamental
things that we did we used of course
programmable shading and Ray traced
acceleration to produce incredibly
beautiful pixels but then we have
artificial
intelligence be
conditioned be controlled by that pixel
to generate a whole bunch of other
pixels not only is it able to generate
pixels spatially because it's aware of
what the colors should be it has been
trained on a supercomputer back in
Nvidia and so the neuron Network that's
running on the GPU can infer and predict
the pixels that we did not render not
only can can we do that it's called
dlss the latest generation of dlss also
generates Beyond frames it can predict
the future generating three additional
frames for every frame that we calculate
what you saw if we just said four frames
of what you saw because we're going to
render one frame and generate three if I
said four frames at full HD 4K that's 33
million pixels or so out of that 33
million
pixels we computed only
two it is an absolute miracle that we
can computationally comput tionally
using programmable shaders and our R
traced engine R tracing engine to
compute 2 million pixels and have ai
predict all of the other 33 and as a
result we're able to render at
incredibly high performance because AI
does a lot less computation it takes of
course an enormous amount of training to
produce that but once you train it the
generation is extremely efficient so
this is one of the incredible cap
abilities of artificial intelligence and
that's why there's so many amazing
things that are happening we used gForce
to enable artificial intelligence and
now artificial intelligence is
revolutionizing
GeForce everyone today we're announcing
our next
Generation the RTX Blackwell family
let's take a look
[Music]
is
[Music]
[Music]
here it
is our brand new
gForce
RTX 50 Series Blackwell architect
the GPU is just a beast 92 billion
transistors
4,000 tops four pedop flops of AI three
times higher than the last generation
Ada and we need all of it to generate
those pixels that I showed you 380 Ray
tracing Tera flops so that we could for
the pixels that we have to compute
compute the most beautiful image you
possibly can and of course 125 Shader
teraflops there is actually a concurrent
Shader teraflops as well as an Inger
unit of equal performance so two dual
shaders one is for floating point one is
for integer G7 memory from Micron 1.8
terabytes Per Second Twice the
performance of our last generation and
we now have the ability to intermix AI
workloads with computer graphics
workloads and one of the amazing things
about this gener eration is the
programmable Shader is also able to now
process neuron networks so the Shader is
able to carry these neuron networks and
as a result we invented neurot texture
compression and neurom material shading
as a result of that you get these
amazingly beautiful images that are only
possible because we use AIS to learn the
texture learn a compression algorithm
and as a result get extraordinary
results okay so this is this is uh the
brand
new
RTX Blackwell
9
now even even the even the mechanical
design is a miracle look at this it's
got two
fans this whole graphics card is just
one giant fan you know so the question
is where's the graphics card is it
literally this
big the voltage regul to design is
state-of-the-art incredible design the
engineering team did a great job so here
it is thank
you okay so those are the speeds and
fees so how does it
compare
well this is RTX
490 I know I know many of you have
one I I know it look it's
$1,599 it is one of the best investments
you could possibly
make you for
$15.99 you bring it home to your
$10,000 PC
entertainment Command Center isn't that
right don't tell me that's not true
don't be
ashamed it's liquid
cooled fancy lights all over it
you lock it when you
leave it's it's the modern home theater
it makes perfect sense and now for
$1,500 and99
$15.99 you get to upgrade that and
turbocharged the living Daya lights out
of it well now with the Blackwell family
RTX 570 490 performance at 549
[Applause]
impossible without artificial
intelligence impossible without the Four
Tops four ter Ops of AI tensor cores
impossible without the G7 memories okay
so 5070 490 performance $549 and here's
the whole family starting from 5070 all
the way up to 5090 5090 twice the
performance of a 4090
starting of course we're producing at
very large scale availability starting
January well it is incredible but we
managed to put these in in gigantic
performance gpus into a laptop this is a
570 laptop for
$12.99 this 570 laptop has a 4090
performance I think there's one here
somewhere
let me show you
this this is a look at this thing here
let me
here there's only so many
pockets ladies and gentlemen Janine
[Applause]
Paul so can you imagine you get this
incredible graphics card here Blackwell
we're going to shrink it and put it in
put it in there does that make any
sense well you can't do that without
artificial intelligence and the reason
for that is because we're generating
most of the pixels using pixels using
our tensor cores so we retrace only the
pixels we need and we generate using
artificial intelligence all the other
pixels we have as a result the amount of
the Energy Efficiency is just off the
charts the future of computer Graphics
is neural rendering the fusion of
artificial intelligence and computer
graphics and what's really
amazing is oh here we go thank
you this is a surprisingly kinetic
keynote and and uh what's really amazing
is the family of gpus we're going to put
in here and so the 1590 the 1590 will
fit into a laptop a thin laptop that
last laptop was 14 14.9 mm you got a
5080 5070 TI and
5070 okay so ladies and gentlemen the
RTX Blackwell family
[Applause]
well GeForce uh brought AI to to the
world democratized AI now ai has come
back and revolutionized GeForce let's
talk about artificial intelligence let's
go to somewhere else at
Nvidia this this is literally our office
this is literally nvidia's
headquarters okay so let's talk about
let's talk about AI the
industry is chasing and racing to scale
artificial intelligence int artificial
intelligence and the scaling law is a
powerful model it's an empirical law
that has been observed and demonstrated
by researchers and Industry over several
Generations ations and this the the
scale the scaling law says that the more
data you have the training data that you
have the larger model that you have and
the more compute that you apply to it
therefore the more effective or the more
capable your model will become and so
the scaling law continues what's really
amazing is that now we're moving towards
of course and the internet is producing
about twice twice the amount of data
every single year as it did last year I
think the in the next couple of years we
produce uh Humanity will produce more
data than all of humanity has ever
produced uh since the beginning and so
we're still producing a gigantic amount
of data and it's becoming more
multimodal video and images and sound
all of that data could be used to train
the fundamental knowledge the
foundational knowledge of an AI but
there are in fact two other scaling laws
that has now emerged and it's somewhat
intuitive the second scaling law is post
trining scaling law posttraining scaling
law uses Technologies techniques like
reinforcement learning human feedback
basically the AI produces and generates
answers the hum based on a human query
the human then of course gives a
feedback um it's much more complicated
than that but the reinforcement learning
system uh with a fair number of very
high quality prompts causes the AI to
refine its skills it could find tune its
skills for particular domains it could
be better at solving math problems
better at reasoning so on so forth and
so it's essentially like having a mentor
or having a coach give you feedback um
after you're done going to school and so
you you get test you get feedback you
improve yourself we also have
reinforcement learning AI feedback
and we have synthetic data generation uh
these techniques are rather uh uh Ain to
if you will uh self-practice uh you know
you know the answer to a particular
problem and uh you continue to try it
until you get it right and so an AI
could be presented with a very
complicated and difficult problem that
has that is verifiable U functionally
and has a has an answer that we
understand maybe proving a theorem maybe
solving a solving a uh geometry problem
and so these problems uh would cause the
AI to produce answers and using
reinforcement learning uh it would learn
how to improve itself that's called post
training post training requires an
enormous amount of computation but the
end result produces incredible models we
now have a third scaling law and this
third scaling law has to do with uh
what's called test time scaling test
time scaling is basically when you're
being used when you're using the AI uh
the AI has the ability to now apply a
different resource allocation instead of
improving its parameters now it's
focused on deciding how much computation
to use to produce the answers uh it
wants to
produce reasoning is a way of thinking
about this uh long thinking is a way to
think about this instead of a direct
inference or One-Shot answer you might
reason about you might break down the
problem into multiple steps you might uh
generate multiple ideas and uh evaluate
you know your AI system would evaluate
which one of the ideas that you
generated was the best one maybe it
solves the problem step by step so on so
forth and so now test time scaling has
proven to be incredibly effective you're
watching this sequence of technology and
this all of these scaling laws emerge as
we see incredible achievements from chat
GPT to 01 to 03 and now Gemini Pro all
of these systems are going through this
journey step by step by step of
pre-training to posttraining to test
time scaling well the amount of
computation that we need of course is
incredible and we would like in fact we
would like in fact that Society has the
ability to scale the amount of
computation to produce more and more
novel and better intelligence
intelligence of course is the most
valuable asset that we have and it can
be applied to solve a lot of very
challenging problems and so scaling law
it's driving enormous demand for NVIDIA
Computing it's driving an enormous
demand for this incredible chip we call
Blackwell let's take a look at Blackwell
well Blackwell is in full
production it is incredible what it
looks like so first of all there's some
uh every every single cloud service
provider now have systems up and running
uh we have systems here from about 15 uh
15 15 U uh excuse me 15 computer makers
it's being made uh about 200 different
SKS 200 different configurations they're
liquid cooled air cooled x86 Nvidia gray
CPU versions mvlink 36 by 2 MV links 72
by1 whole bunch of different types of
systems so that we can accommodate just
about every single data center in the
world well this these systems are being
currently manufactured in some 45
factories it tells you how pervasive
artificial intelligence is and how much
the industry is jumping onto artificial
intelligence in this new Computing
model well the reason why we're driving
it so hard is because we need a lot more
computation and it's very clear it's
very clear that that um
Janine you know
I it's hard to tell you don't ever want
to reach your hands into a dark
place hang a second is this a good
idea all right
[Applause]
[Music]
wait for
it wait for
it I thought I was
worthy apparently yor didn't think I was
worthy all right
this is my show and tell this is a show
and tell so uh this mvlink system this
right here this mvlink system this is
gb200 MV link 72 it is 1 and 12
tons 600,000
Parts approximately equal to 20
cars 12 12 120 kilow
it has um a spine behind it that
connects all of these GPU
together two miles of copper
cable 5,000
cables this is being manufactured in 45
factories around the world we build them
we liquid cool them we test them we
disassemble them shiping parts to the
data centers because it's 1 and A2 tons
we reassemble it outside the data
centers and install them the
manufacturing is insane but the goal of
all of this is because the scaling laws
are driving Computing so hard that this
level of computation Blackwell over our
last generation improves the performance
per watt by a factor of four performance
per watt by a factor of four perform
performance per dollar by a factor of
three that's basically says that in one
generation we reduce the
cost of training these models by a
factor of three or if you want to
increase um the size of your model by a
factor of three it's about the same cost
but the important thing is this these
are generating tokens that are being
used by all of us when we use Chad GPT
or when we use Gemini use our phones in
the future just about all of these
applications are going to be consuming
these AI tokens and these AI tokens are
being generated by these
systems and every single data center is
limited by power
and so if the perf per watt of Blackwell
is four
times our last
generation then the revenue that could
be generated the amount of business that
can be generated in the data center is
increased by a factor of four and so
these AI Factory systems really are
factories today now the goal of all of
this is to so that we can create one
giant chip the amount of computation we
need is really quite incredible and this
is basically one giant chip if we would
have had to build a chip one here we go
sorry
guys you see that that's
cool look at that disco lights in
here right if we had to build this as
one chip obviously this would be the
size of the wafer but this doesn't
include the impact of yield it would
have to be probably three or four times
the size but what we basically have here
is 72 Blackwell gpus or 144 dieses this
one chip here is 1.4 exop flops the
world's largest supercomputer fastest
supercomputer only recently this entire
room supercomputer only recently
achieved an exf flop plus this is 1.4
exf flops of AI floating Point
performance it has 14 terabytes of
memory but here's the amazing thing the
memory bandwidth is 1.2 petabytes per
second that's basically basically the
entire internet traffic that's happening
right
now the entire world's internet traffic
is being processed across these chips
okay and we have um 103 130 trillion
transistors in total
2592 CPU
cores whole bunch of networking and so
these I wish I could do this I don't
think I will so these are the black
Wells these are our
connectx networking chips these are the
mvy link and we're trying to pretend
about the Envy the the Envy Ling spine
but that's not possible okay and these
are all of the hbm memories 12 ter 14
terabytes of hbm memory this is what
we're trying to do and this is the
miracle this is the miracle of the
Blackwell system the blackwall dies
right here it is the largest single chip
the world's ever made but yet the
miracle is really in addition to that
this is uh the grace black wall system
well the goal of all of this of course
is so that we can thank you
thanks boy is there a chair I could sit
down for a
second can I have a m AO
Ultra how is it possible that we're in
the mobe ultra
Stadium it's like coming to Nvidia and
we don't have a GPU for
you so so we need an enormous the
computation because we want to train
larger and larger models and these
inferences these inferences used to be
one inference but in the future the AI
is going to be talking to itself it's
going to be thinking it's going to be
internally reflecting processing so
today when the tokens are being
generated at you so long as it's coming
out at 20 or 30 tokens per second it's
basically as fast as anybody can read
however in the future and right now with
uh gp1 you know with the new the pre
Gemini Pro and the new GP the the 0103
models they're talking to themselves we
reflecting they thinking and so as you
can imagine the rate at which the tokens
could be ingested is incredibly high and
so we need the token rates the token
generation rates to go way up and we
also have to drive the cost way down
simultaneously so that the C the quality
of service can be extraordinary the cost
to customers can continue to be low and
uh will continue to scale and so that's
the fundamental purpose the reason why
we created MV link well one of the most
important things that's happening in the
world of Enterprise is a Genentech AI a
Genentech AI basically is a perfect
example of test time scaling it's a AI
is a system of models some of it is
understanding interacting with the
customer interacting with the user some
of it is maybe retrieving information
retrieving information from Storage a
semantic AI system like a rag uh maybe
it's going on to to the internet uh
maybe it's uh studying a PDF file and so
it might be using tools it might be
using a calculator and it might be using
a generative AI to uh generate uh charts
and such and it's iter it's taking the
the problem you gave it breaking it down
step by step and it's iterating through
all these different models well in order
to respond to a customer in the future
in order for AI to respond it used to be
ask a question answer start spewing out
in the future you ask a question a whole
bunch bu of models are going to be
working in the background and so test
time scaling the amount of computation
used for inferencing is going to go
through the roof it's going to go
through the roof because we want better
and better answers well to help the the
industry build agentic AI our our go to
market is not direct to Enterprise
customers our go to market is is we work
with software developers in the it
ecosystem to integrate our technology to
make possible new capabilities just like
we did did with Cuda libraries we now
want to do that with AI libraries and
just as the Computing model of the past
has apis that are uh doing computer
Graphics or doing linear algebra or
doing fluid dynamics in the future on
top of those acceleration libraries C
acceleration libraries will have ai
libraries we've created three things for
helping the ecosystem build agentic AI
Nvidia Nims which are essentially AI
microservices all packaged up it takes
all of this really complicated Cuda
software Cuda
DNN cutless or tensor rtlm or Triton or
all of these different really
complicated software and the model
itself we package it up we optimize it
we put it into a container and you could
take it wherever you like and so we have
models for vision for understanding
languages for speech for animation for
digital biology and we have some new new
exciting models coming for physical Ai
and these AI models run in every single
Cloud because nvidia's gpus are now
available in every single Cloud it's
available in every single OEM so you
could literally take these models
integrate it into your software packages
create AI agents that run on Cadence or
they might be S uh service now agents or
they might be sap agents and they could
deploy it to their customers and run it
wherever the customers want to run the
software the next layer is what we call
Nvidia Nemo Nemo is
essentially a digital employee
onboarding and training evaluation
system in the future these AI agents are
essentially digital Workforce that are
working alongside your employees um
working Al doing things for you on your
behalf and so the way that you would
bring these specialized agents into your
these special agents into your company
is to onboard them just like you onboard
an employee and so we have different
libraries that helps uh these AI agents
be uh trained for the type of you know
language in your company maybe the
vocabulary is unique to your company the
business process is different the way
you work is different so you would give
them examples of what the work product
should look like and they would try to
generate and you would give a feedback
and then you would evaluate them so on
so forth and so that uh and you would
guardrail them you say these are the
things that you're not allowed to do
these are things you're not allowed to
say this and and we even give them
access to certain information okay so
that entire pipeline a digital employee
pipeline is called Nemo in a lot of ways
the IT department of every company is
going to be the HR department of AI
agents in the
future today they manage and maintain a
bunch of software from uh from the IT
industry in the future they will Main
maintain you know nurture onboard and
improve a whole bunch of digital agents
and provision them to the companies to
use okay and so your H your it
department is going to become kind of
like AI agent HR and on top of that we
provide a whole bunch of blueprints that
our ecosystem could could uh take
advantage of all of this is completely
open source and so you could take take
it and uh modify the blueprints we have
blueprints for all kinds of different
different types of Agents well today
we're also announcing that we're doing
something that's really cool and I think
really clever we're announcing a whole
family of models that are based off of
llama the Nvidia llama neotron language
Foundation models llama 3.1 is a
complete
phenomenon the download of llama 3.1
from meta 350 650,000 times something
like that it has
been der red and turned into other
models uh about 60,000 other different
models it it is singularly the reason
why just about every single Enterprise
and every single industry has been
activated to start working on AI well
the thing that we did was we realized
that the Llama models really could be
better fine-tuned for Enterprise use and
so we fine-tune them using our expertise
and our capabilities and we turn them
into the Llama neotron Suite of open
models there are small ones that
interact in uh very very fast response
time extremely small uh they're uh sup
what we call Super llama neotron supers
they're basically your mainstream
versions of your models or your Ultra
model the ultra model could be used uh
to be a teacher model for a whole bunch
of other models it could be a reward
model evaluator uh a judge for other
models to create answers and decide
whether it's a good answer or not
give basically give feedback to other
models it could be distilled in a lot of
different ways basically a teacher model
a knowledge distillation uh uh model
very large very capable and so all of
this is now available online well these
models are incredible it's a a number
one in leaderboards for chat leaderboard
for instruction uh lead leaderboard for
retrieval um so the different types of
functionalities necessary that are used
in AI agents around the world uh these
are going to be incredible models for
you we're also working with uh the
ecosystem these Tech all of our Nvidia
AI Technologies are integrated into uh
uh the it in Industry uh we have great
partners and really great work being
done at service now at sap at Seaman uh
for industrial AI uh Cadence is during
great work synopsis doing great work I'm
really proud of the work that we do with
perplexity as you know they
revolutionize search yeah really
fantastic stuff uh codium uh every every
software engineer in the world this is
going to be the next giant AI
application next giant AI service period
is software coding 30 million software
Engineers around the world everybody is
going to have a software assistant uh
helping them code uh if if um if not
obviously you're just you're going to be
way less productive and create lesser
good code and so this is 30 million
there's a billion knowledge workers in
the world it is very very clear AI
agents is probably the next robotics
industry and likely to be a
multi-trillion dollar opportunity well
let me show you some of the uh
blueprints that we've created and some
of the work that we've done with our
partners uh with these AI
agents AI agents are the new digital
Workforce working for and with
us AI agents are a system of models that
reason about a mission break it down
into tasks and retrieve data or use
tools to generate a quality
response nvidia's agentic AI building
blocks Nim pre-trained models and Nemo
framework let organizations easily
develop AI agents and deploy them
anywhere we will onboard and train our
agentic workforces on our company's
methods like we do for
employees AI agents are domain specific
task experts let me show you four
examples for the billions of knowledge
workers and students AI research
assistant agents ingest complex
documents like lectures journals
Financial results and generate
interactive podcasts for easy learning
by combining a unet regression model
with a diffusion model cordi can
downscale global weather forecasts down
from 25 km to 2
km developers like at Nvidia manage
software security AI agents that
continuously scan software for
vulnerabilities alerting developers to
what action is
needed Virtual Lab AI agents help
researchers design and Screen billions
of compounds to find promising drug
candidates faster than
ever Nvidia analytics AI agents built on
an Nvidia metr blueprint including
Nvidia Cosmos nimron Vision language
models llama neaton llms and Nemo
retriever Metropolis agents analyze
content from the billions of cameras
generating 100,000 pedes of video per
day they enable interactive search
summarization and automated
reporting and help monitor traffic flows
flagging congestion or danger
in industrial facilities they monitor
processes and generate recommendations
or
Improvement Metropolis agents centralize
data from hundreds of cameras and can
reroute workers or robots when incidents
occur the age of agentic AI is here for
every
organization okay
that was the first pitch at a baseball
that was not generated I just felt that
none of you were
impressed okay so ai ai was was created
in the cloud and for the cloud AI is
creating the cloud for the cloud and for
uh enjoying AI on on phones of course
it's perfect um very very soon we're
going to have a continuous AI that's
going to be with you and when you use
those metag glasses you could of course
uh point at something look at something
and and ask it you know whatever
information you want and so AI is is
perfect in the CL was creating the cloud
is perfect in the cloud however we would
love to be able to take that AI
everywhere I've mentioned already that
you could take Nvidia AI to any Cloud
but you could also put it inside your
company but the thing that we want to do
more than anything is put it on our PC
as well and so as you know Windows 95
revolutionized the computer industry it
made possible this new Suite of
multimedia services and it change the
way that applications was created
forever um Windows 95 this this model of
computing of course is not perfect for
AI and so the thing that we would like
to do is we would like to have in the
future your AI basically become your AI
assistant and instead of instead of just
the the 3D apis and the sound apis and
the video API you would have generative
apis generative apis for 3D and
generative apis for language and
generative AI for sound and so on so
forth and we need a system that makes
that possible while leveraging the
massive investment that's in the cloud
there's no way that we could the world
can create yet another way of
programming AI models it's just not
going to happen and so if we could
figure out a way to make Windows
PC a worldclass
aipc um it would be completely awesome
and it turns out the answer is Windows
it's Windows wsl2 Windows wsl2 Windows
wsl2 basically it's two operating
systems within one it works perfectly
it's developed for developers and it's
developed uh uh so that you can have
access to Bare Metal it's been wsl2 has
been
optimized optimized for cloud native
applications it is optimized for and
very importantly it's been optimized for
Cuda and so wsl2 supports Cuda perfectly
out of the box as a
result everything that I showed you with
Nvidia Nims Nvidia Nemo the blueprints
that we develop that are going to be up
in ai. nvidia.com so long as the
computer fits it so long as you can fit
that model and we're going to have many
models that that fit whether it's Vision
models or language models or speech
models or these animation human digital
human models all kinds of different
different types of models are going to
be perfect for your PC and it would you
download it and it should just run and
so our focus is to turn Windows wsl2
Windows PC into a Target first class
platform that we will support and
maintain for as long as we shall live
and so this is an incredible thing for
engineers and developers everywhere let
let me show you something that we can do
with that this is one of the examples of
a blueprint we just made for
you generative AI synthesizes amazing
images from Simple Text prompts yet
image composition can be challenging to
control using only words with Nvidia Nim
microservices creators can use Simple 3D
objects to guide AI image generation
let's see how a concept artist can use
this technology to develop the look of a
scene they start by laying out 3D assets
created by hand or generated with AI
then use an image generation Nim such as
flux to create a visual that adheres to
the 3D
scene add or move objects to refine the
composition change camera angles to
frame the perfect
shot or reimagine the whole scene with a
new
prompt assisted by generative AI and
Nvidia Nim and artists can quickly
realize their
[Music]
Vision Nvidia AI for your
PCS hundreds of millions of PCS in the
world with Windows and so we could get
them ready for AI uh oems all the PC
oems we work with just basically all of
the world's leading PC oems are going to
get their PCS ready for this stack and
so aips are coming to a home near you
Linux is
good okay let's talk about physical
AI speaking of Linux let's talk about
physical
AI So Physical AI imagine
imagine whereas your large language
model you give it your your context your
prompt on the left and it generates
tokens one at a time to produce the
output that's basically how it works the
amazing thing is this model in the
middle is quite large has billions of
parameters the context length is
incredibly large because you might
decide to load in a PDF in my case I
might load in several PDFs before I ask
it a question those PDFs are turned into
tokens the attention the basic attention
characteristic of a transformer has
every single token find its relationship
and relevance against every other token
so you could have hundreds of thousands
of tokens and the computational load
increases quadratically and it does this
that all of the parameters all of the
input sequence process it through every
single layer of the Transformer and it
produces one token that's the reason why
we needed blackw
and then the next token is produced when
the current token is done it puts the
current token into the input sequence
and takes that whole thing and generates
the next token it does it one at a time
this is the Transformer model it's the
reason why it is so so incredibly
effective computationally demanding What
If instead of PDFs it's your surrounding
and what if instead of the prompt a
question it's a request go over there
and pick up that that you know that box
and bring it back and instead of what is
produced in tokens its text it produces
action
tokens well that I just described is a
very sensible thing for the future of
Robotics and the technology is right
around the corner but what we need to do
is we need to create the effective
effectively the world
model of you know as opposed to GPT
which is a language model and this World
model has to understand the language of
the world it has to understand physical
Dynamics things like gravity and
friction and inertia it has to
understand geometric and spatial
relationships it has to understand cause
and effect if you drop something a fall
to the ground if you you know poke at it
it tips over it has to understand object
permanence if you roll a ball over the
kitchen counter when it goes off the
other side the ball didn't leave into
another quantum universe that that's
still there and so all of these types of
understanding is intuitive understanding
that we know that most models today have
a very hard time with and so we would
like to create a world we need a world
Foundation model today we're announcing
a very big thing we're announcing Nvidia
Cosmos a world Foundation model that is
designed that was created to understand
the physical world and the only way for
you to really understand this is to see
it let's
[Music]
flip the next Frontier of AI is physical
AI model performance is directly related
to data availability but physical world
data is costly to capture curate and
label Nvidia Cosmos is a world
Foundation model development platform to
Advance Physical AI it includes Auto
regressive world found Foundation models
diffusion-based World Foundation models
Advanced
tokenizers and an Nvidia Cuda an AI
accelerated data
pipeline Cosmos models ingest text image
or video prompts and generate virtual
world States as
videos Cosmos Generations prioritize the
unique requirements of Av and Robotics
use cases like real world environments
lighting and object permanence
developers use Nvidia Omniverse to build
physics-based
geospatially accurate scenarios then
output Omniverse renders into Cosmos
which generates photoreal physically
based synthetic
[Music]
data whether diverse
objects or environments
conditions like weather or time of day
or Edge case
scenarios developers use Cosmos to
generate worlds for reinforcement
learning AI feedback to improve policy
models or to test and validate model
performance even across multisensor
views Cosmos can generate tokens in real
time bringing the power of foresight and
Multiverse simulation to AI models
generating every possible future to help
the model select the right
path working with the world's developer
ecosystem Nvidia is helping Advance the
next wave of physical
[Music]
AI Nvidia
Cosmos Nvidia
Cosmos Nvidia Cosmos the world's first
world Foundation model it is trained on
20 million hours of video the 20 million
hours of video focuses on physical
Dynamic things so n n Dynamic nature
nature themes themes uh humans uh
walking uh hands moving uh manipulating
things uh you know things that are uh
fast camera movements it's really about
teaching the AI not about generating
creative content but teaching the AI to
understand the physical world and from
this with this physical AI there are
many Downstream things that we could uh
do as a result we could do synthetic
data generation to train uh models we
could distill it and turn it into
effectively the seed the beginnings of a
robotics model you could have it
generate multiple physically based
physically plausible uh scenarios that
the future basically do a doctor strange
um you could uh because because this
model understands the physical world of
course you saw a whole bunch of images
generated this model understanding the
physical world it also uh could do of
course captioning and so it could take
videos caption it incredibly well and
that captioning and the video could be
used to train large language models
multimodality large language models and
uh so you could use this technology to
use this Foundation model to train
robotics robots as well as larger
language models and so this is the
Nvidia Cosmos the platform has an auto
regressive model for real-time
applications has diffusion model for a
very high quality image generation it's
incredible tokenizer basically learning
the vocabulary of uh real world and a
data pipeline so that if you would like
to take all of this and then train it on
your own data this data pipeline because
there's so much data involved we've
accelerated everything end to endend for
you and so this is the world's first
data processing pipeline that's Cuda
accelerated as well as AI accelerated
all of this is part of the cosmos
platform and today we're announcing that
Cosmos is open licensed it's open
available on
GitHub we hope we hope that this moment
and there's a there's a small medium
large for uh uh very fast models um you
know mainstream models and also teacher
models basically not knowledge transfer
models Cosmo Cosmos World Foundation
model being open we really hope will do
for the world of Robotics and Industrial
AI what llama 3 has done for Enterprise
AI the magic happens when you connect
Cosmos to Omniverse and the reason
fundamentally is this Omniverse is a
physics grounded not physically grounded
but physics grounded it's algorithmic
physics principled physics simulation
grounded system it's a simulator when
you connect that to
Cosmos it provides the grounding the
ground truth that can control and to
condition the Osmos generation as a
result what comes out of Osmos is
grounded on Truth this is exactly the
same idea as connecting a large language
model model to a rag to a retrieval
augmented generation system you want to
ground the AI generation on ground truth
and so the combination of the two gives
you a
physically simulated a physically
grounded Multiverse generator and the
application the use cases are really
quite exciting and of course uh for
robotics uh for industrial applications
uh it is very very clear this Cosmos
plus
o Omniverse plus Cosmos represents the
Third computer that's necessary for
building robotic systems every robotics
company will ultimately have to build
three computers a robotics the robotics
system could be a factory the robotics
system could be a car it could be a
robot you need three fundamental
computers one computer of course to
train the AI we call the dgx computer to
train the AI another of course when
you're done to deploy the AI we call
that agx that's inside the car in the
robot or in an AMR or you know at the uh
in a in a stadium or whatever it is
these computers are at the edge and
they're autonomous but to connect the
two you need a digital twin and this is
all the simulations that you were seeing
the digital twin is where the AI that
has been trained goes to practice to be
refined to do its synthetic data
generation reinforcement learning AI
feedback such and such and so it's the
digital twin of the AI these three
computers are going to be working
interactively nvidia's strategy for uh
the industrial world and we've been
talking about this for some time is this
three computer
system you know instead of a three three
body problem we have a three Computer
Solution and so it's the Nvidia
robotics so let me give you three
examples
all right so the first example is uh uh
how we apply apply all of this to
Industrial digitalization there millions
of factories hundreds of thousands of
warehouses that's basically it's the
backbone of A50 trillion doll
manufacturing industry all of that has
to become software defined all of that
has has to have Automation in the future
and all of it will be infused with
robotics well we're partnering with Keon
the world's leading Warehouse automation
Solutions provider and Accenture the
world's largest professional services
provider and they have a big focus in
digital manufacturing and we're working
together to create something that's
really special and I'll show you that in
the second but our go to market is
essentially the same as all of the other
software uh platforms and all the
technology platforms that we have
through the uh developers and ecosystem
Partners uh and we have just just a
growing number of ecosystem Partners
connecting to Omniverse and the reason
for that is very clear everybody wants
to digitalize the future of Industries
there's so much waste so much
opportunity for Automation in that $50
trillion doar of the world's GDP so
let's take a look at that this one one p
one example that we're doing with Keon
and
Accenture Keon the supply chain solution
company Accenture a global leader in
Professional Services and Nvidia are
bringing physical AI to the $1 trillion
warehouse and Distribution Center Market
managing high- Performance Warehouse
Logistics involves navigating a complex
web of decisions influenced by
constantly shifting variables these
include daily and seasonal demand
changes space constraints Workforce
availability and the integration of of
diverse robotic and automated systems
and predicting operational kpis of a
physical Warehouse is nearly impossible
today to tackle these challenges Keon is
adopting Mega an Nvidia Omniverse
blueprint for building industrial
digital twins to test and optimize
robotic fleets first Keon's warehouse
management solution assigns tasks to the
industrial AI brains in the digital twin
such as moving a load from from a buffer
location to a shuttle storage
solution the robot's brains are in a
simulation of a physical Warehouse
digitalized into Omniverse using open
USD connectors to aggregate CAD video
and image to 3D Light Art to point cloud
and AI generated data the fleet of
robots execute tasks by perceiving and
reasoning about their Omniverse digital
twin environment planning their next
motion and acting
the robot brains can see the resulting
State through sensor simulations and
decide their next action the loop
continues while Mega precisely tracks
the state of everything in the digital
twin now Keon can simulate infinite
scenarios at scale while measuring
operational kpis such as throughput
efficiency and utilization all before
deploying changes to the physical
Warehouse together with Nvidia
Keon and Accenture are Reinventing
industrial
autonomy in the future is that that's
incredible everything is in
simulation in the future in the future
every Factory will have a digital twin
and that digital twin operates exactly
like the real factory and in fact you
could use Omniverse with Cosmos to
generate a whole bunch of future
scenarios and you pick then an AI
decides which which one of the scenarios
are the most optimal for whatever kpis
and that becomes the programming
constraints the program if you will the
AI that will be uh deployed into the
real factories the next example
autonomous vehicles the AV revolution
has arrived after so many years with weo
success and Tesla's success it is very
very clear autonomous vehicles has
finally arrived well our offering to
this industry is the three computers the
training systems the training the AIS
the simulation systemss and and the and
the synthetic data generation systems
Omniverse and now Cosmos and also the
computer that's inside the car each car
company might might work with us in a
different way use one or two or three of
the computers we're working with just
about every major car company around the
world whmo and zuk and Tesla of course
in their data center byd the largest uh
EV company in the world jlr has got a
really cool car coming Mercedes because
a fleet of cars coming with Nvidia
starting with this starting this year
going to production and I'm super super
pleased to announce that today Toyota
and Nvidia are going to partner together
to create their next Generation
AVS just so many so many cool companies
uh lucid and rivan and Shi and of course
uh Volvo just so many different
companies Wabi is uh building uh
self-driving trucks Aurora we announced
this week also that Aurora is going to
use Nvidia to build self-driving trucks
autonomous 100 million cars build each
year a billion cars vehicles on a road
all over the world a trillion miles that
are driven around the world each year
that's all going to be either highly
autonomous or you know fully autonomous
coming up and so this is going to be a
very L very large industry I predict
that this will likely be the first
multi-trillion dollar
robotics industry this IND this business
for us um notice in just just a few of
these cars that are starting to ramp
into the world uh our business is
already $4 billion and this year
probably on a run rate of about $5
billion so really significant business
already this is going to be very large
well today we're announcing that our
next generation processor for the car
our next generation computer for the car
is called Thor I have one right here
hang on a second
okay this is
Thor this is
Thor this is this is a robotics
computer this is a robotics computer
takes sensors and just a Madness amount
of sensor information process it you
know een teed cameras high resolution
Radars Liars they're all coming into
this chip and this chip has to process
all that sensor turn them into tokens
put them into a Transformer and predict
the next PATH and this AV computer is
now in full production Thor is 20 times
the processing capability of our last
generation Orin which is really the
standard of autonomous vehicles today
and so this is just really quite quite
incredible Thor is in full production
this robotics processor by the way also
goes into a full robot and so it could
be an AMR it could be a human or robot
could be the brain it could be the
manipulator this Rob this processor
basically is a universal robotics
computer the second part of our drive
system that I'm incredibly proud of is
the dedication to safety Drive OS I'm
pleased to announce is now the first
softwar defined programmable AI computer
that has been certified up to asold D
which is the highest standard of
functional safety for automobiles the
only and the highest and so I'm really
really proud of this asold ISO
26262 it is um the work of some 15,000
engineering years this is just
extraordinary work and as a result of
that Cuda is now a functional safe
computer and so if you're building a
robot Nvidia Cuda y
okay so so now I wanted to I told you I
was going to show you what would we use
Omniverse and Cosmos to do in the
context of self-driving cars and you
know today instead of showing you a
whole bunch of uh uh videos of of cars
driving on the road I'll show you some
of that too um but I want to show you
how we use the car to reconstruct
digital twins automatically using Ai and
use that capability to train future am
models okay let's play
it the autonomous vehicle Revolution is
here building autonomous vehicles like
all robots requires three computers
Nvidia dgx to train AI models Omniverse
to test drive and generate synthetic
data and drive agx a supercomputer in
the car
building safe autonomous vehicles means
addressing Edge scenarios but real world
data is limited so synthetic data is
essential for
training the autonomous vehicle data
Factory powered by Nvidia Omniverse AI
models and Cosmos generates synthetic
driving scenarios that enhance training
data by orders of
magnitude first omnimap fuses map and
geospatial data to construct drivable 3D
environments driving scenario variations
can be generated from replay Drive logs
or AI traffic
generators next a neural reconstruction
engine uses autonomous vehicle sensor
logs to create High Fidelity 4D
simulation
environments it replays previous drives
in 3D and generates scenario Vari ations
to amplify training
data finally edify 3DS automatically
searches through existing asset
libraries or generates new assets to
create Sim ready
scenes the Omniverse scenarios are used
to condition Cosmos to generate massive
amounts of photo realistic data reducing
the Sim toore
Gap and with text prompts generate near
infinite variations of the driving
scenario with Cosmos neotron video
search the massively scaled synthetic
data set combined with recorded drives
can be curated to train
models nvidia's AI data Factory scales
hundreds of drives into billions of
effective miles setting the standard for
safe and advanced autonomous driving
[Music]
is that incredible
we take take thousands of drives and
turn them into billions of miles we are
going to have mountains of training data
for autonomous vehicles of course we
still need actual cars on the road of
course we will continuously collect data
for as long as we shall live however
synthetic data generation using this
Multiverse physically based physically
grounded capability so that we generate
data for training AIS that are
physically grounded and accurate and or
plausible so that we could have an
enormous amount of data to train with
the AV industry is here uh this is an
incredibly exciting time super super
super uh uh excited about the next
several years I think you're going to
see just as computer Graphics was
revolutionized such incredible pace
you're going to see the pace of Av
development increasing tremendously over
the next several
years I I think I think
um I I think the next part is is
robotics so um
human
robots my
[Applause]
friends the chat GPT moment for General
robotics is just around the corner and
in fact all of the enabling technologies
that I've been talking about is going to
make it possible for us in the next
several years to see very rapid break
breakthroughs surprising breakthroughs
in in general robotics now the reason
why General robotics is so important is
whereas robots with tracks and wheels
require special environments to
accommodate them there are three
robots three robots in the world that we
can make that require no green
fields Brown field adaptation is perfect
if we if we could possibly build these
amazing robots we could deploy them in
exactly the world that we've built for
ourselves these three robots are one
agentic robots agentic AI because you
know they're information workers so long
as they could accommodate uh the
computers that we have in our offices is
going to be great number two
self-driving cars and the reason for
that is we spent 100 plus years building
roads and cities and then number three
human or robots if we have the
technology to solve these three this
will be the largest technology industry
IND the world's ever seen and so we
think that robotics era is just around
the corner the critical capability is
how to train these robots in the case of
human or
robots the imitation information is
rather hard to collect and the reason
for that is uh in the case of car you
just drive it we're driving cars all the
time in the case of these human robots
the imitation information the the human
demonstration is rather laborious is to
do and so we need to come up with a
clever way to take hundreds of
demonstrations thousands of human
demonstrations and somehow use
artificial intelligence and
Omniverse to synthetically
generate
millions
of
synthetically generated motions and from
those motions the AI can learn uh how to
perform a task let me show you how
that's
done developers around the world are
building the next wave of physical AI
embodied robots
humanoids developing general purpose
robot models requires massive amounts of
real world data which is costly to
capture and
curate Nvidia Isaac Groot helps tackle
these challenges providing humanoid
robot developers with four things robot
Foundation
models data
pipelines simulation
Frameworks and a Thor robotics
computer the Nvidia Isaac Groot
blueprint for synthetic motion
generation is a simulation workflow for
imitation learning enabling developers
to generate exponentially large data
sets from a small number of
demonstrations first Groot teleop
enables skilled human workers to portal
into a digital twin of their robot using
the Apple Vision
Pro this means operators can capture
data even without a physical robot and
they can operate the robot in a
risk-free environment eliminating the
chance of physical damage or wear and
tear to teach a robot a single task
operators capture motion trajectories
through a handful of teleoperated
demonstrations then use Groot mimic to
multiply these trajectories into a much
larger data
set next they use Gro gen built on
Omniverse and Cosmos for domain
randomization and 3D to real
upscaling generating an exponentially
larger data
set the Omniverse and Cosmos Multiverse
simulation engine provides a massively
scaled data set to train the robot
policy once the policy is trained
developers can perform software in the
loop testing and validation in Isaac Sim
before deploying to the real
robot the age of General robotics is
arriving powered by Nvidia Isaac
Groot we're going to have mountains of
data to train robots with
Nvidia Isaac group Nvidia Isaac group
this is our platform to provide
technology platform technology elements
to the robotics industry to accelerate
the development of General
Robotics and um well I have one more
thing that I want to show you none of
none of this none of this would be
possible if not for uh this incredible
project that we started uh about a
decade ago inside the company what
called project project digits deep
learning GPU intelligence training
system
digits well before we launched it uh I
shrunk it to
dgx and to harmonize it with
RTX agx ovx and all of the other X's
that we have in the company and and um I
and and it really revolutionized uh djx1
really
revolutionized where where's djx1
dgx-1 revolutionized artificial
intelligence the reason why we built it
was because we wanted to uh make it
possible for researchers and startups to
have an out-of-the-box AI supercomputer
imagine the way supercomputers were
built in the past you really have to uh
build your own facility and you have to
go build your own infrastructure and
really engineer it into existence and so
we created a supercomputer for AI for AI
development for researchers and and
startups that comes literally one out of
the box I delivered the first one to a
startup company in 2016 called open Ai
and Elon was there and and Ilia sus was
there and many of Nvidia Engineers were
there and and um uh we we celebrated the
arrival of djx1 and obviously uh it
revolutionized uh artificial
intelligence and Computing um but now
artificial intelligence is everywhere
it's not just in researchers and and and
startup Labs you know we want artificial
intelligence as I mentioned in the
beginning of our
this is now the new way of doing
Computing this is the new way of doing
software every software engineer every
engineer every creative artist everybody
who uses computers today as a tool will
need a AI
supercomputer and so I just wished I
just wish that djx1 was smaller and
um you know so so um you know imagine
ladies and gentlemen
our this is nvidia's latest AI
supercomputer and and it's finally
called project digits right now and if
you have a good name for it uh reach out
to us um uh this here's the amazing
thing this is an AI supercomputer it
runs the entire Nvidia AI
stack all of nvidia's software runs on
this dgx Cloud runs on
this this
sits well somewhere and it's wireless or
you know connect it to your computer
it's even a workstation if you like it
to be and you could access it you could
you could reach it like a like a cloud
supercomputer and nvidia's AI works on
it and um it's based on a a super secret
chip that we've been working on called
GB 110 the smallest Grace Blackwell that
we make and I have well you know what
let's show let's show everybody insight
isn't it just isn't just it's just so
cute and this is the chip that's
inside it is in it is in
production this top secret chip uh we
did in collaboration the CPU the gray
CPU was a uh is built for NVIDIA in
collaboration with mediatech
uh they're the world's leading s so
company and they worked with us to build
this CPU this CPU s so and connect it
with chipto chip mvy link to the
Blackwell GPU and uh this little this
little thing here is in full production
uh we're expecting this computer to uh
be available uh around May time frame
and so it's coming at you uh it's just
incredible what we could do and it's
just I think it's you
really I was trying to figure out do I
need more hands or more
pockets all right so so uh imagine this
is what it looks
like you know who doesn't want one of
those and if you if you use
PC Mac you know anything because because
uh you know it's it's a cloud platform
it's a cloud computing platform that
sits on your desk you could also use it
as a l Linux workstation if you like uh
if you would like to have double
digits this is what it looks like you
know and you you connect it you connect
it together uh uh with connectx and it
has
nickel GPU direct all of that out of the
box it's like a supercomputer our entire
supercomputing stack uh is available and
so Nvidia Project digits
[Applause]
okay well let me let me let me tell you
what I told you I told you that we are
in production with three new Blackwells
not only is the grace Blackwell
supercomputers mvlink 72s in production
all over the world we now have three new
Blackwell systems in production one
amazing AI foundational M World
Foundation model the world's first
physical AI Foundation model is open
available to activate the world's
industries of Robotics and such and
three and three robotics three robots
working on uh agentic AI uh human or
robots and self-driving
cars uh it's been an incredible year I
want to thank all of you for your
partnership uh thank all of you for
coming I made you a short video to
reflect on last year and look forward to
the next year play please w
[Music]
[Applause]
[Music]
[Music]
[Music]
[Music]
[Applause]
[Music]
[Music]
have a great C us
everybody happy New
Year thank you

